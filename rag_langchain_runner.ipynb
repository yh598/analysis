{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be73a32b-2982-414b-9730-96089223657c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq databricks-agents mlflow mlflow-skinny databricks-vectorsearch langchain==0.2.1 langchain_core==0.2.5 langchain_community==0.2.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c61ed9d2-3592-4d8f-8d81-63ac68813659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37afb747-1aa4-4f6f-aac6-48fe0c5969f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_community.vectorstores import DatabricksVectorSearch\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableBranch\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "## Enable MLflow Tracing\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "\n",
    "############\n",
    "# Helper functions\n",
    "############\n",
    "# Return the string contents of the most recent message from the user\n",
    "def extract_user_query_string(chat_messages_array):\n",
    "    return chat_messages_array[-1][\"content\"]\n",
    "\n",
    "\n",
    "# Return the chat history, which is is everything before the last question\n",
    "def extract_chat_history(chat_messages_array):\n",
    "    return chat_messages_array[:-1]\n",
    "\n",
    "\n",
    "# Load the chain's configuration\n",
    "model_config = mlflow.models.ModelConfig(development_config=\"rag_chain_config.yaml\")\n",
    "\n",
    "# Here, we define an input example in the schema required by Agent Framework\n",
    "input_example = {\"messages\": [ {\"role\": \"user\", \"content\": \"What is Retrieval-augmented Generation?\"}]}\n",
    "\n",
    "############\n",
    "# Connect to the Vector Search Index\n",
    "############\n",
    "vs_client = VectorSearchClient(disable_notice=True)\n",
    "vs_index = vs_client.get_index(\n",
    "    endpoint_name=model_config.get(\"vector_search_endpoint_name\"),\n",
    "    index_name=model_config.get(\"vector_search_index\"),\n",
    ")\n",
    "\n",
    "############\n",
    "# Turn the Vector Search index into a LangChain retriever\n",
    "############\n",
    "vector_search_as_retriever = DatabricksVectorSearch(\n",
    "    vs_index,\n",
    "    text_column=\"chunked_text\",\n",
    "    columns=[\n",
    "        \"chunk_id\",\n",
    "        \"chunked_text\",\n",
    "    ],\n",
    ").as_retriever(search_kwargs={\"k\":3})\n",
    "\n",
    "############\n",
    "# Required to:\n",
    "# 1. Enable the RAG Studio Review App to properly display retrieved chunks\n",
    "# 2. Enable evaluation suite to measure the retriever\n",
    "############\n",
    "\n",
    "mlflow.models.set_retriever_schema(\n",
    "    primary_key=\"chunk_id\",\n",
    "    text_column=\"chunked_text\",\n",
    "  # Review App uses `doc_uri` to display chunks from the same document in a single view\n",
    ")\n",
    "\n",
    "\n",
    "############\n",
    "# Method to format the docs returned by the retriever into the prompt\n",
    "############\n",
    "def format_context(docs):\n",
    "    chunk_template = \"Passage: {chunk_text}\\n\"\n",
    "    chunk_contents = [\n",
    "        chunk_template.format(\n",
    "            chunk_text=d.page_content,\n",
    "        )\n",
    "        for d in docs\n",
    "    ]\n",
    "    return \"\".join(chunk_contents)\n",
    "\n",
    "\n",
    "############\n",
    "# Prompt Template for generation\n",
    "############\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (  # System prompt contains the instructions\n",
    "            \"system\",\n",
    "            model_config.get(\"llm_prompt_template\"),\n",
    "        ),\n",
    "        # If there is history, provide it.\n",
    "        # Note: This chain does not compress the history, so very long converastions can overflow the context window.\n",
    "        MessagesPlaceholder(variable_name=\"formatted_chat_history\"),\n",
    "        # User's most current question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Format the converastion history to fit into the prompt template above.\n",
    "def format_chat_history_for_prompt(chat_messages_array):\n",
    "    history = extract_chat_history(chat_messages_array)\n",
    "    formatted_chat_history = []\n",
    "    if len(history) > 0:\n",
    "        for chat_message in history:\n",
    "            if chat_message[\"role\"] == \"user\":\n",
    "                formatted_chat_history.append(\n",
    "                    HumanMessage(content=chat_message[\"content\"])\n",
    "                )\n",
    "            elif chat_message[\"role\"] == \"assistant\":\n",
    "                formatted_chat_history.append(\n",
    "                    AIMessage(content=chat_message[\"content\"])\n",
    "                )\n",
    "    return formatted_chat_history\n",
    "\n",
    "\n",
    "############\n",
    "# Prompt Template for query rewriting to allow converastion history to work - this will translate a query such as \"how does it work?\" after a question such as \"what is spark?\" to \"how does spark work?\".\n",
    "############\n",
    "query_rewrite_template = \"\"\"Based on the chat history below, we want you to generate a query for an external data source to retrieve relevant documents so that we can better answer the question. The query should be in natural language. The external data source uses similarity search to search for relevant documents in a vector space. So the query should be similar to the relevant documents semantically. Answer with only the query. Do not add explanation.\n",
    "\n",
    "Chat history: {chat_history}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "query_rewrite_prompt = PromptTemplate(\n",
    "    template=query_rewrite_template,\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    ")\n",
    "\n",
    "\n",
    "############\n",
    "# FM for generation\n",
    "############\n",
    "model = ChatDatabricks(\n",
    "    endpoint=model_config.get(\"llm_model_serving_endpoint_name\"),\n",
    "    extra_params={\"temperature\": 0.01},\n",
    ")\n",
    "\n",
    "############\n",
    "# RAG Chain\n",
    "############\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_user_query_string),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_chat_history),\n",
    "        \"formatted_chat_history\": itemgetter(\"messages\")\n",
    "        | RunnableLambda(format_chat_history_for_prompt),\n",
    "    }\n",
    "    | RunnablePassthrough()\n",
    "    | {\n",
    "        \"context\": RunnableBranch(  # Only re-write the question if there is a chat history\n",
    "            (\n",
    "                lambda x: len(x[\"chat_history\"]) > 0,\n",
    "                query_rewrite_prompt | model | StrOutputParser(),\n",
    "            ),\n",
    "            itemgetter(\"question\"),\n",
    "        )\n",
    "        | vector_search_as_retriever\n",
    "        | RunnableLambda(format_context),\n",
    "        \"formatted_chat_history\": itemgetter(\"formatted_chat_history\"),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "## Tell MLflow logging where to find your chain.\n",
    "# `mlflow.models.set_model(model=...)` function specifies the LangChain chain to use for evaluation and deployment.  This is required to log this chain to MLflow with `mlflow.langchain.log_model(...)`.\n",
    "\n",
    "mlflow.models.set_model(model=chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2eb80e54-254c-4548-b2f6-aa91b93d79bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_example = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"User's first question\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Assistant's reply\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"User's next question\",\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# chain.invoke(input_example)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "rag_langchain_runner",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}