{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d386c83-abd9-4034-b008-a4df19b0e3d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Document Ingestion and Preparation\n",
    "\n",
    "<img style=\"float: right\" width=\"800px\" src=\"https://raw.githubusercontent.com/databricks-industry-solutions/hls-llm-doc-qa/basic-qa-LLM-HLS/images/data-prep.jpeg?token=GHSAT0AAAAAACBNXSB4IK2XJS37QU6HCJCEZEBL3TA\">\n",
    "\n",
    "\n",
    "#\n",
    "1. Organize your documents into a Unity Catalog Volume\n",
    "    * In this demo we have preuploaded a set of PDFs from PubMed on S3, but your own documents will work the same way\n",
    "2. Use LangChain to ingest those documents and split them into manageable chunks using a text splitter\n",
    "3. Use a sentence transformer NLP model to create embeddings of those text chunks and store them in a vectorstore\n",
    "    * Embeddings are basically creating a high-dimension vector encoding the semantic meaning of a chunk of text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4819ddb4-8abe-4076-a8b3-2f4c967fdae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Start with required Python libraries for data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04477c7-fd40-4e36-8423-1852892fc661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d3a2f1b-cb47-4645-ab29-a1ab2e4d7797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /databricks/python3/lib/python3.12/site-packages (4.51.3)\nCollecting transformers\n  Using cached transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from transformers) (3.13.1)\nCollecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n  Using cached huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.12/site-packages (from transformers) (2.1.3)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.12/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.12/site-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from transformers) (2.32.3)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /databricks/python3/lib/python3.12/site-packages (from transformers) (0.6.2)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.12/site-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\nCollecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n  Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\nUsing cached transformers-4.57.0-py3-none-any.whl (12.0 MB)\nUsing cached huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\nUsing cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\nUsing cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\nInstalling collected packages: hf-xet, huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.30.2\n    Not uninstalling huggingface-hub at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'huggingface-hub'. No files were found to uninstall.\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Not uninstalling tokenizers at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'tokenizers'. No files were found to uninstall.\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.3\n    Not uninstalling transformers at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'transformers'. No files were found to uninstall.\nSuccessfully installed hf-xet-1.1.10 huggingface-hub-0.35.3 tokenizers-0.22.1 transformers-4.57.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: sentence-transformers in /databricks/python3/lib/python3.12/site-packages (4.0.1)\nCollecting sentence-transformers\n  Using cached sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from sentence-transformers) (4.57.0)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (2.7.0+cpu)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (1.15.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from sentence-transformers) (0.35.3)\nRequirement already satisfied: Pillow in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /databricks/python3/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.5.0)\nRequirement already satisfied: packaging>=20.9 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (74.0.0)\nRequirement already satisfied: sympy>=1.13.3 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /databricks/python3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /databricks/python3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\nUsing cached sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\nInstalling collected packages: sentence-transformers\n  Attempting uninstall: sentence-transformers\n    Found existing installation: sentence-transformers 4.0.1\n    Not uninstalling sentence-transformers at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'sentence-transformers'. No files were found to uninstall.\nSuccessfully installed sentence-transformers-5.1.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting mlflow\n  Using cached mlflow-3.4.0-py3-none-any.whl.metadata (30 kB)\nCollecting mlflow-skinny==3.4.0 (from mlflow)\n  Using cached mlflow_skinny-3.4.0-py3-none-any.whl.metadata (31 kB)\nCollecting mlflow-tracing==3.4.0 (from mlflow)\n  Using cached mlflow_tracing-3.4.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: Flask<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (2.2.5)\nRequirement already satisfied: alembic!=1.10.0,<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (1.16.4)\nRequirement already satisfied: cryptography<46,>=43.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (43.0.3)\nCollecting docker<8,>=4.0.0 (from mlflow)\n  Using cached docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting fastmcp<3,>=2.0.0 (from mlflow)\n  Using cached fastmcp-2.12.4-py3-none-any.whl.metadata (19 kB)\nCollecting graphene<4 (from mlflow)\n  Using cached graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: gunicorn<24 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (20.1.0)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (3.10.0)\nRequirement already satisfied: numpy<3 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (2.1.3)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (2.2.3)\nRequirement already satisfied: pyarrow<22,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (19.0.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (1.6.1)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (1.15.1)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (2.0.37)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (5.5.1)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (3.0.0)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (0.49.0)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (0.116.1)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (6.6.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (1.36.0)\nCollecting opentelemetry-proto<3,>=1.9.0 (from mlflow-skinny==3.4.0->mlflow)\n  Using cached opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (1.36.0)\nRequirement already satisfied: packaging<26 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (24.2)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (5.29.4)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (2.10.6)\nCollecting python-dotenv<2,>=0.19.0 (from mlflow-skinny==3.4.0->mlflow)\n  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (6.0.2)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (2.32.3)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (0.4.2)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (4.12.2)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow) (0.35.0)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.12/site-packages (from alembic!=1.10.0,<2->mlflow) (1.2.0)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.12/site-packages (from cryptography<46,>=43.0.0->mlflow) (1.17.1)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.12/site-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\nCollecting authlib>=1.5.2 (from fastmcp<3,>=2.0.0->mlflow)\n  Using cached authlib-1.6.5-py2.py3-none-any.whl.metadata (9.8 kB)\nCollecting cyclopts>=3.0.0 (from fastmcp<3,>=2.0.0->mlflow)\n  Using cached cyclopts-3.24.0-py3-none-any.whl.metadata (11 kB)\nCollecting exceptiongroup>=1.2.2 (from fastmcp<3,>=2.0.0->mlflow)\n  Using cached exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\nCollecting httpx>=0.28.1 (from fastmcp<3,>=2.0.0->mlflow)\n  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting mcp<2.0.0,>=1.12.4 (from fastmcp<3,>=2.0.0->mlflow)\n  Using cached mcp-1.16.0-py3-none-any.whl.metadata (80 kB)\nCollecting openapi-core>=0.19.5 (from fastmcp<3,>=2.0.0->mlflow)\n  Using cached openapi_core-0.19.5-py3-none-any.whl.metadata (6.6 kB)\nCollecting openapi-pydantic>=0.5.1 (from fastmcp<3,>=2.0.0->mlflow)\n  Using cached openapi_pydantic-0.5.1-py3-none-any.whl.metadata (10 kB)\nCollecting pydantic<3,>=1.10.8 (from mlflow-skinny==3.4.0->mlflow)\n  Using cached pydantic-2.12.0-py3-none-any.whl.metadata (83 kB)\nCollecting pyperclip>=1.9.0 (from fastmcp<3,>=2.0.0->mlflow)\n  Using cached pyperclip-1.11.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: rich>=13.9.4 in /databricks/python3/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow) (13.9.4)\nRequirement already satisfied: Werkzeug>=2.2.2 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow) (3.1.3)\nRequirement already satisfied: Jinja2>=3.0 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow) (3.1.5)\nRequirement already satisfied: itsdangerous>=2.0 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow) (2.2.0)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /databricks/python3/lib/python3.12/site-packages (from graphene<4->mlflow) (3.2.4)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n  Using cached graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.12/site-packages (from graphene<4->mlflow) (2.9.0.post0)\nRequirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.12/dist-packages (from gunicorn<24->mlflow) (74.0.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (1.4.8)\nRequirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (3.2.0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas<3->mlflow) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas<3->mlflow) (2024.1)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (3.5.0)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.12/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.12->cryptography<46,>=43.0.0->mlflow) (2.21)\nRequirement already satisfied: attrs>=23.1.0 in /databricks/python3/lib/python3.12/site-packages (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow) (24.3.0)\nCollecting docstring-parser>=0.15 (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow)\n  Using cached docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\nCollecting rich-rst<2.0.0,>=1.3.1 (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow)\n  Using cached rich_rst-1.3.1-py3-none-any.whl.metadata (6.0 kB)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.4.0->mlflow) (2.40.3)\nRequirement already satisfied: starlette<0.48.0,>=0.40.0 in /databricks/python3/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny==3.4.0->mlflow) (0.47.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.4.0->mlflow) (4.0.11)\nRequirement already satisfied: anyio in /databricks/python3/lib/python3.12/site-packages (from httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (4.6.2)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.12/site-packages (from httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (1.0.2)\nRequirement already satisfied: idna in /databricks/python3/lib/python3.12/site-packages (from httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (3.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (0.14.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.4.0->mlflow) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from Jinja2>=3.0->Flask<4->mlflow) (3.0.2)\nCollecting httpx-sse>=0.4 (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow)\n  Using cached httpx_sse-0.4.2-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: jsonschema>=4.20.0 in /databricks/python3/lib/python3.12/site-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (4.23.0)\nCollecting pydantic-settings>=2.5.2 (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow)\n  Using cached pydantic_settings-2.11.0-py3-none-any.whl.metadata (3.4 kB)\nCollecting python-multipart>=0.0.9 (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow)\n  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nCollecting sse-starlette>=1.6.1 (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow)\n  Using cached sse_starlette-3.0.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: isodate in /databricks/python3/lib/python3.12/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.6.1)\nCollecting jsonschema-path<0.4.0,>=0.3.1 (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow)\n  Using cached jsonschema_path-0.3.4-py3-none-any.whl.metadata (4.3 kB)\nCollecting more-itertools (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow)\n  Using cached more_itertools-10.8.0-py3-none-any.whl.metadata (39 kB)\nCollecting openapi-schema-validator<0.7.0,>=0.6.0 (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow)\n  Using cached openapi_schema_validator-0.6.3-py3-none-any.whl.metadata (5.4 kB)\nCollecting openapi-spec-validator<0.8.0,>=0.7.1 (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow)\n  Using cached openapi_spec_validator-0.7.2-py3-none-any.whl.metadata (5.7 kB)\nCollecting parse (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow)\n  Using cached parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting Werkzeug>=2.2.2 (from Flask<4->mlflow)\n  Using cached werkzeug-3.1.1-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.4.0->mlflow) (0.57b0)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow) (0.7.0)\nCollecting pydantic-core==2.41.1 (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow)\n  Using cached pydantic_core-2.41.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\nCollecting typing-extensions<5,>=4.0.0 (from mlflow-skinny==3.4.0->mlflow)\n  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting typing-inspection>=0.4.2 (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow)\n  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\nCollecting email-validator>=2.0.0 (from pydantic[email]>=2.11.7->fastmcp<3,>=2.0.0->mlflow)\n  Using cached email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.4.0->mlflow) (3.3.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /databricks/python3/lib/python3.12/site-packages (from rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /databricks/python3/lib/python3.12/site-packages (from rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow) (2.15.1)\nRequirement already satisfied: sniffio>=1.1 in /databricks/python3/lib/python3.12/site-packages (from anyio->httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (1.3.0)\nCollecting dnspython>=2.0.0 (from email-validator>=2.0.0->pydantic[email]>=2.11.7->fastmcp<3,>=2.0.0->mlflow)\n  Using cached dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.4.0->mlflow) (5.0.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.4.0->mlflow) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.4.0->mlflow) (4.9.1)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /databricks/python3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (2023.7.1)\nRequirement already satisfied: referencing>=0.28.4 in /databricks/python3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (0.30.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (0.22.3)\nCollecting pathable<0.5.0,>=0.4.1 (from jsonschema-path<0.4.0,>=0.3.1->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow)\n  Using cached pathable-0.4.4-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: mdurl~=0.1 in /databricks/python3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow) (0.1.0)\nRequirement already satisfied: rfc3339-validator in /databricks/python3/lib/python3.12/site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.1.4)\nCollecting lazy-object-proxy<2.0.0,>=1.7.1 (from openapi-spec-validator<0.8.0,>=0.7.1->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow)\n  Using cached lazy_object_proxy-1.12.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (5.1 kB)\nCollecting docutils (from rich-rst<2.0.0,>=1.3.1->cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow)\n  Using cached docutils-0.22.2-py3-none-any.whl.metadata (15 kB)\nCollecting anyio (from httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow)\n  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.4.0->mlflow) (0.4.8)\nUsing cached mlflow-3.4.0-py3-none-any.whl (26.7 MB)\nUsing cached mlflow_skinny-3.4.0-py3-none-any.whl (2.2 MB)\nUsing cached mlflow_tracing-3.4.0-py3-none-any.whl (1.2 MB)\nUsing cached docker-7.1.0-py3-none-any.whl (147 kB)\nUsing cached fastmcp-2.12.4-py3-none-any.whl (329 kB)\nUsing cached graphene-3.4.3-py2.py3-none-any.whl (114 kB)\nUsing cached authlib-1.6.5-py2.py3-none-any.whl (243 kB)\nUsing cached cyclopts-3.24.0-py3-none-any.whl (86 kB)\nUsing cached exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\nUsing cached graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nUsing cached httpx-0.28.1-py3-none-any.whl (73 kB)\nUsing cached mcp-1.16.0-py3-none-any.whl (167 kB)\nUsing cached openapi_core-0.19.5-py3-none-any.whl (106 kB)\nUsing cached openapi_pydantic-0.5.1-py3-none-any.whl (96 kB)\nUsing cached opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\nUsing cached pydantic-2.12.0-py3-none-any.whl (459 kB)\nUsing cached pydantic_core-2.41.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\nUsing cached pyperclip-1.11.0-py3-none-any.whl (11 kB)\nUsing cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nUsing cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\nUsing cached werkzeug-3.1.1-py3-none-any.whl (224 kB)\nUsing cached docstring_parser-0.17.0-py3-none-any.whl (36 kB)\nUsing cached email_validator-2.3.0-py3-none-any.whl (35 kB)\nUsing cached httpx_sse-0.4.2-py3-none-any.whl (9.0 kB)\nUsing cached jsonschema_path-0.3.4-py3-none-any.whl (14 kB)\nUsing cached openapi_schema_validator-0.6.3-py3-none-any.whl (8.8 kB)\nUsing cached openapi_spec_validator-0.7.2-py3-none-any.whl (39 kB)\nUsing cached pydantic_settings-2.11.0-py3-none-any.whl (48 kB)\nUsing cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\nUsing cached rich_rst-1.3.1-py3-none-any.whl (11 kB)\nUsing cached sse_starlette-3.0.2-py3-none-any.whl (11 kB)\nUsing cached anyio-4.11.0-py3-none-any.whl (109 kB)\nUsing cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\nUsing cached more_itertools-10.8.0-py3-none-any.whl (69 kB)\nUsing cached parse-1.20.2-py2.py3-none-any.whl (20 kB)\nUsing cached dnspython-2.8.0-py3-none-any.whl (331 kB)\nUsing cached lazy_object_proxy-1.12.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (71 kB)\nUsing cached pathable-0.4.4-py3-none-any.whl (9.6 kB)\nUsing cached docutils-0.22.2-py3-none-any.whl (632 kB)\nInstalling collected packages: pyperclip, parse, Werkzeug, typing-extensions, python-multipart, python-dotenv, pathable, opentelemetry-proto, more-itertools, lazy-object-proxy, httpx-sse, graphql-relay, docutils, docstring-parser, dnspython, typing-inspection, pydantic-core, jsonschema-path, graphene, exceptiongroup, email-validator, docker, anyio, sse-starlette, rich-rst, pydantic, httpx, authlib, pydantic-settings, openapi-schema-validator, openapi-pydantic, cyclopts, openapi-spec-validator, mlflow-tracing, mlflow-skinny, mcp, openapi-core, fastmcp, mlflow\n  Attempting uninstall: Werkzeug\n    Found existing installation: Werkzeug 3.1.3\n    Not uninstalling werkzeug at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'Werkzeug'. No files were found to uninstall.\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.12.2\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.27.2\n    Not uninstalling pydantic-core at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'pydantic_core'. No files were found to uninstall.\n  Attempting uninstall: anyio\n    Found existing installation: anyio 4.6.2\n    Not uninstalling anyio at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'anyio'. No files were found to uninstall.\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.10.6\n    Not uninstalling pydantic at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'pydantic'. No files were found to uninstall.\n  Attempting uninstall: httpx\n    Found existing installation: httpx 0.27.0\n    Not uninstalling httpx at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'httpx'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 3.0.1\n    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\nSuccessfully installed Werkzeug-3.1.1 anyio-4.11.0 authlib-1.6.5 cyclopts-3.24.0 dnspython-2.8.0 docker-7.1.0 docstring-parser-0.17.0 docutils-0.22.2 email-validator-2.3.0 exceptiongroup-1.3.0 fastmcp-2.12.4 graphene-3.4.3 graphql-relay-3.2.0 httpx-0.28.1 httpx-sse-0.4.2 jsonschema-path-0.3.4 lazy-object-proxy-1.12.0 mcp-1.16.0 mlflow-3.4.0 mlflow-skinny-3.4.0 mlflow-tracing-3.4.0 more-itertools-10.8.0 openapi-core-0.19.5 openapi-pydantic-0.5.1 openapi-schema-validator-0.6.3 openapi-spec-validator-0.7.2 opentelemetry-proto-1.37.0 parse-1.20.2 pathable-0.4.4 pydantic-2.12.0 pydantic-core-2.41.1 pydantic-settings-2.11.0 pyperclip-1.11.0 python-dotenv-1.1.1 python-multipart-0.0.20 rich-rst-1.3.1 sse-starlette-3.0.2 typing-extensions-4.15.0 typing-inspection-0.4.2\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: langchain in /databricks/python3/lib/python3.12/site-packages (0.3.21)\nCollecting langchain\n  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\nCollecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n  Using cached langchain_core-0.3.78-py3-none-any.whl.metadata (3.2 kB)\nCollecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n  Using cached langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: langsmith>=0.1.17 in /databricks/python3/lib/python3.12/site-packages (from langchain) (0.1.133)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langchain) (2.12.0)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /databricks/python3/lib/python3.12/site-packages (from langchain) (2.0.37)\nRequirement already satisfied: requests<3,>=2 in /databricks/python3/lib/python3.12/site-packages (from langchain) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /databricks/python3/lib/python3.12/site-packages (from langchain) (6.0.2)\nCollecting langsmith>=0.1.17 (from langchain)\n  Using cached langsmith-0.4.33-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.0.0)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\nRequirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\nRequirement already satisfied: packaging<26.0.0,>=23.2.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\nRequirement already satisfied: orjson>=3.9.14 in /databricks/python3/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (3.11.2)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\nCollecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n  Using cached zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.3 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.1)\nRequirement already satisfied: typing-inspection>=0.4.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\nRequirement already satisfied: anyio in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.2)\nRequirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\nRequirement already satisfied: sniffio>=1.1 in /databricks/python3/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\nUsing cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\nUsing cached langchain_core-0.3.78-py3-none-any.whl (449 kB)\nUsing cached langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\nUsing cached langsmith-0.4.33-py3-none-any.whl (387 kB)\nUsing cached zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.5 MB)\nInstalling collected packages: zstandard, langsmith, langchain-core, langchain-text-splitters, langchain\n  Attempting uninstall: langsmith\n    Found existing installation: langsmith 0.1.133\n    Not uninstalling langsmith at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'langsmith'. No files were found to uninstall.\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.63\n    Not uninstalling langchain-core at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'langchain-core'. No files were found to uninstall.\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.8\n    Not uninstalling langchain-text-splitters at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'langchain-text-splitters'. No files were found to uninstall.\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.21\n    Not uninstalling langchain at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'langchain'. No files were found to uninstall.\nSuccessfully installed langchain-0.3.27 langchain-core-0.3.78 langchain-text-splitters-0.3.11 langsmith-0.4.33 zstandard-0.25.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting databricks-vectorsearch==0.45\n  Using cached databricks_vectorsearch-0.45-py3-none-any.whl.metadata (2.8 kB)\nCollecting mlflow-skinny<3,>=2.11.3 (from databricks-vectorsearch==0.45)\n  Using cached mlflow_skinny-2.22.2-py3-none-any.whl.metadata (31 kB)\nCollecting protobuf<5,>=3.12.0 (from databricks-vectorsearch==0.45)\n  Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nRequirement already satisfied: requests>=2 in /databricks/python3/lib/python3.12/site-packages (from databricks-vectorsearch==0.45) (2.32.3)\nCollecting deprecation>=2 (from databricks-vectorsearch==0.45)\n  Using cached deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from deprecation>=2->databricks-vectorsearch==0.45) (24.2)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (5.5.1)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (3.0.0)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (0.49.0)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (0.116.1)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (6.6.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (1.36.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (1.36.0)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (2.12.0)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (6.0.2)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (0.4.2)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (4.15.0)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (0.35.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch==0.45) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch==0.45) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch==0.45) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests>=2->databricks-vectorsearch==0.45) (2025.1.31)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (2.40.3)\nRequirement already satisfied: starlette<0.48.0,>=0.40.0 in /databricks/python3/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (0.47.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (3.21.0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (0.57b0)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (2.41.1)\nRequirement already satisfied: typing-inspection>=0.4.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (0.4.2)\nRequirement already satisfied: h11>=0.8 in /databricks/python3/lib/python3.12/site-packages (from uvicorn<1->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (0.14.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (5.0.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (4.9.1)\nRequirement already satisfied: anyio<5,>=3.6.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (4.11.0)\nRequirement already satisfied: sniffio>=1.1 in /databricks/python3/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (1.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.45) (0.4.8)\nUsing cached databricks_vectorsearch-0.45-py3-none-any.whl (13 kB)\nUsing cached deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nUsing cached mlflow_skinny-2.22.2-py3-none-any.whl (6.3 MB)\nUsing cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\nInstalling collected packages: protobuf, deprecation, mlflow-skinny, databricks-vectorsearch\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 5.29.4\n    Not uninstalling protobuf at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 3.4.0\n    Uninstalling mlflow-skinny-3.4.0:\n      Successfully uninstalled mlflow-skinny-3.4.0\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmlflow 3.4.0 requires mlflow-skinny==3.4.0, but you have mlflow-skinny 2.22.2 which is incompatible.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\ngrpcio-status 1.67.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed databricks-vectorsearch-0.45 deprecation-2.1.0 mlflow-skinny-2.22.2 protobuf-4.25.8\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting pycryptodome\n  Using cached pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\nUsing cached pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\nInstalling collected packages: pycryptodome\nSuccessfully installed pycryptodome-3.23.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: accelerate in /databricks/python3/lib/python3.12/site-packages (1.5.2)\nCollecting accelerate\n  Using cached accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /databricks/python3/lib/python3.12/site-packages (from accelerate) (2.1.3)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.12/site-packages (from accelerate) (5.9.0)\nRequirement already satisfied: pyyaml in /databricks/python3/lib/python3.12/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from accelerate) (2.7.0+cpu)\nRequirement already satisfied: huggingface_hub>=0.21.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from accelerate) (0.35.3)\nRequirement already satisfied: safetensors>=0.4.3 in /databricks/python3/lib/python3.12/site-packages (from accelerate) (0.6.2)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2023.5.0)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /databricks/python3/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (74.0.0)\nRequirement already satisfied: sympy>=1.13.3 in /databricks/python3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /databricks/python3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.1.31)\nUsing cached accelerate-1.10.1-py3-none-any.whl (374 kB)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.5.2\n    Not uninstalling accelerate at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'accelerate'. No files were found to uninstall.\nSuccessfully installed accelerate-1.10.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting sacremoses\n  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: regex in /databricks/python3/lib/python3.12/site-packages (from sacremoses) (2024.11.6)\nRequirement already satisfied: click in /databricks/python3/lib/python3.12/site-packages (from sacremoses) (8.1.7)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.12/site-packages (from sacremoses) (1.4.2)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.12/site-packages (from sacremoses) (4.67.1)\nUsing cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\nInstalling collected packages: sacremoses\nSuccessfully installed sacremoses-0.1.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: ninja in /databricks/python3/lib/python3.12/site-packages (1.11.1.4)\nCollecting ninja\n  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\nUsing cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\nInstalling collected packages: ninja\n  Attempting uninstall: ninja\n    Found existing installation: ninja 1.11.1.4\n    Not uninstalling ninja at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'ninja'. No files were found to uninstall.\nSuccessfully installed ninja-1.13.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: tiktoken in /databricks/python3/lib/python3.12/site-packages (0.9.0)\nCollecting tiktoken\n  Using cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: regex>=2022.1.18 in /databricks/python3/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\nRequirement already satisfied: requests>=2.26.0 in /databricks/python3/lib/python3.12/site-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\nUsing cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\nInstalling collected packages: tiktoken\n  Attempting uninstall: tiktoken\n    Found existing installation: tiktoken 0.9.0\n    Not uninstalling tiktoken at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'tiktoken'. No files were found to uninstall.\nSuccessfully installed tiktoken-0.12.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: nltk in /databricks/python3/lib/python3.12/site-packages (3.9.1)\nCollecting nltk\n  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: click in /databricks/python3/lib/python3.12/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.12/site-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /databricks/python3/lib/python3.12/site-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.12/site-packages (from nltk) (4.67.1)\nUsing cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\nInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.9.1\n    Not uninstalling nltk at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'nltk'. No files were found to uninstall.\nSuccessfully installed nltk-3.9.2\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting databricks\n  Using cached databricks-0.2-py2.py3-none-any.whl.metadata (172 bytes)\nUsing cached databricks-0.2-py2.py3-none-any.whl (1.2 kB)\nInstalling collected packages: databricks\nSuccessfully installed databricks-0.2\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting databricks-agents\n  Using cached databricks_agents-1.7.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting databricks-connect (from databricks-agents)\n  Using cached databricks_connect-17.2.2-py2.py3-none-any.whl.metadata (2.9 kB)\nCollecting dataclasses-json (from databricks-agents)\n  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\nCollecting databricks-sdk>=0.58.0 (from databricks-sdk[openai]>=0.58.0->databricks-agents)\n  Using cached databricks_sdk-0.67.0-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: jinja2>=3.0.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-agents) (3.1.5)\nCollecting mlflow-skinny<4.0.0,>=3.4.0 (from databricks-agents)\n  Using cached mlflow_skinny-3.4.0-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: tenacity>=8.5 in /databricks/python3/lib/python3.12/site-packages (from databricks-agents) (9.0.0)\nRequirement already satisfied: tiktoken>=0.8.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from databricks-agents) (0.12.0)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.12/site-packages (from databricks-agents) (4.67.1)\nRequirement already satisfied: urllib3>=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-agents) (2.3.0)\nRequirement already satisfied: pydantic>=2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from databricks-agents) (2.12.0)\nCollecting whenever==0.7.3 (from databricks-agents)\n  Using cached whenever-0.7.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: boto3>1 in /databricks/python3/lib/python3.12/site-packages (from databricks-agents) (1.36.2)\nRequirement already satisfied: botocore in /databricks/python3/lib/python3.12/site-packages (from databricks-agents) (1.36.3)\nCollecting litellm==1.75.9 (from databricks-agents)\n  Using cached litellm-1.75.9-py3-none-any.whl.metadata (41 kB)\nRequirement already satisfied: aiohttp>=3.10 in /databricks/python3/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (3.11.10)\nRequirement already satisfied: click in /databricks/python3/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (8.1.7)\nRequirement already satisfied: httpx>=0.23.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (0.28.1)\nCollecting importlib-metadata>=6.8.0 (from litellm==1.75.9->databricks-agents)\n  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\nRequirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (4.23.0)\nCollecting openai>=1.99.5 (from litellm==1.75.9->databricks-agents)\n  Using cached openai-2.2.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: python-dotenv>=0.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (1.1.1)\nRequirement already satisfied: tokenizers in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents) (0.22.1)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from boto3>1->databricks-agents) (1.0.1)\nRequirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /databricks/python3/lib/python3.12/site-packages (from boto3>1->databricks-agents) (0.11.3)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.12/site-packages (from botocore->databricks-agents) (2.9.0.post0)\nRequirement already satisfied: requests<3,>=2.28.1 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk>=0.58.0->databricks-sdk[openai]>=0.58.0->databricks-agents) (2.32.3)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk>=0.58.0->databricks-sdk[openai]>=0.58.0->databricks-agents) (2.40.3)\nCollecting langchain-openai (from databricks-sdk[openai]>=0.58.0->databricks-agents)\n  Using cached langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from jinja2>=3.0.0->databricks-agents) (3.0.2)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (5.5.1)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (3.0.0)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (0.116.1)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (3.1.43)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (1.36.0)\nRequirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (1.37.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (1.36.0)\nRequirement already satisfied: packaging<26 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (24.2)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (4.25.8)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (6.0.2)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (0.4.2)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (4.15.0)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (0.35.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic>=2->databricks-agents) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from pydantic>=2->databricks-agents) (2.41.1)\nRequirement already satisfied: typing-inspection>=0.4.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from pydantic>=2->databricks-agents) (0.4.2)\nRequirement already satisfied: regex>=2022.1.18 in /databricks/python3/lib/python3.12/site-packages (from tiktoken>=0.8.0->databricks-agents) (2024.11.6)\nRequirement already satisfied: googleapis-common-protos>=1.65.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (1.65.0)\nRequirement already satisfied: grpcio-status>=1.67.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (1.67.0)\nRequirement already satisfied: grpcio>=1.67.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (1.67.0)\nRequirement already satisfied: numpy>=1.15 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (2.1.3)\nRequirement already satisfied: pandas>=1.0.5 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (2.2.3)\nCollecting py4j==0.10.9.9 (from databricks-connect->databricks-agents)\n  Using cached py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: pyarrow>=10.0.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect->databricks-agents) (19.0.1)\nRequirement already satisfied: setuptools>=68.0.0 in /usr/local/lib/python3.12/dist-packages (from databricks-connect->databricks-agents) (74.0.0)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from databricks-connect->databricks-agents) (1.16.0)\nCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->databricks-agents)\n  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json->databricks-agents)\n  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (0.2.0)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents) (1.18.0)\nRequirement already satisfied: starlette<0.48.0,>=0.40.0 in /databricks/python3/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (0.47.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (4.0.11)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk>=0.58.0->databricks-sdk[openai]>=0.58.0->databricks-agents) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk>=0.58.0->databricks-sdk[openai]>=0.58.0->databricks-agents) (4.9.1)\nCollecting protobuf<7,>=3.12.0 (from mlflow-skinny<4.0.0,>=3.4.0->databricks-agents)\n  Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\nRequirement already satisfied: anyio in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from httpx>=0.23.0->litellm==1.75.9->databricks-agents) (4.11.0)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.12/site-packages (from httpx>=0.23.0->litellm==1.75.9->databricks-agents) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx>=0.23.0->litellm==1.75.9->databricks-agents) (1.0.2)\nRequirement already satisfied: idna in /databricks/python3/lib/python3.12/site-packages (from httpx>=0.23.0->litellm==1.75.9->databricks-agents) (3.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.75.9->databricks-agents) (0.14.0)\nRequirement already satisfied: zipp>=3.20 in /databricks/python3/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm==1.75.9->databricks-agents) (3.21.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /databricks/python3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.75.9->databricks-agents) (2023.7.1)\nRequirement already satisfied: referencing>=0.28.4 in /databricks/python3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.75.9->databricks-agents) (0.30.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.75.9->databricks-agents) (0.22.3)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.99.5->litellm==1.75.9->databricks-agents) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from openai>=1.99.5->litellm==1.75.9->databricks-agents) (0.10.0)\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.12/site-packages (from openai>=1.99.5->litellm==1.75.9->databricks-agents) (1.3.0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (0.57b0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas>=1.0.5->databricks-connect->databricks-agents) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas>=1.0.5->databricks-connect->databricks-agents) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk>=0.58.0->databricks-sdk[openai]>=0.58.0->databricks-agents) (3.3.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->databricks-agents) (1.0.0)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.78 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (0.3.78)\nRequirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from tokenizers->litellm==1.75.9->databricks-agents) (0.35.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<4.0.0,>=3.4.0->databricks-agents) (5.0.0)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.75.9->databricks-agents) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.75.9->databricks-agents) (2023.5.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.75.9->databricks-agents) (1.1.10)\nRequirement already satisfied: langsmith<1.0.0,>=0.3.45 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (0.4.33)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (1.33)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk>=0.58.0->databricks-sdk[openai]>=0.58.0->databricks-agents) (0.4.8)\nRequirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.78->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (3.0.0)\nRequirement already satisfied: orjson>=3.9.14 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (3.11.2)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (1.0.0)\nRequirement already satisfied: zstandard>=0.23.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents) (0.25.0)\nUsing cached databricks_agents-1.7.0-py3-none-any.whl (200 kB)\nUsing cached litellm-1.75.9-py3-none-any.whl (8.9 MB)\nUsing cached whenever-0.7.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (419 kB)\nUsing cached databricks_sdk-0.67.0-py3-none-any.whl (718 kB)\nUsing cached mlflow_skinny-3.4.0-py3-none-any.whl (2.2 MB)\nUsing cached databricks_connect-17.2.2-py2.py3-none-any.whl (2.7 MB)\nUsing cached py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\nUsing cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\nUsing cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\nUsing cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\nUsing cached openai-2.2.0-py3-none-any.whl (998 kB)\nUsing cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\nUsing cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nUsing cached langchain_openai-0.3.35-py3-none-any.whl (75 kB)\nInstalling collected packages: py4j, whenever, typing-inspect, protobuf, marshmallow, importlib-metadata, dataclasses-json, openai, databricks-sdk, litellm, databricks-connect, mlflow-skinny, langchain-openai, databricks-agents\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 4.25.8\n    Uninstalling protobuf-4.25.8:\n      Successfully uninstalled protobuf-4.25.8\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib-metadata 6.6.0\n    Not uninstalling importlib-metadata at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'importlib-metadata'. No files were found to uninstall.\n  Attempting uninstall: openai\n    Found existing installation: openai 1.69.0\n    Not uninstalling openai at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'openai'. No files were found to uninstall.\n  Attempting uninstall: databricks-sdk\n    Found existing installation: databricks-sdk 0.49.0\n    Not uninstalling databricks-sdk at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'databricks-sdk'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.22.2\n    Uninstalling mlflow-skinny-2.22.2:\n      Successfully uninstalled mlflow-skinny-2.22.2\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-vectorsearch 0.45 requires mlflow-skinny<3,>=2.11.3, but you have mlflow-skinny 3.4.0 which is incompatible.\ndatabricks-vectorsearch 0.45 requires protobuf<5,>=3.12.0, but you have protobuf 5.29.5 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed databricks-agents-1.7.0 databricks-connect-17.2.2 databricks-sdk-0.67.0 dataclasses-json-0.6.7 importlib-metadata-8.7.0 langchain-openai-0.3.35 litellm-1.75.9 marshmallow-3.26.1 mlflow-skinny-3.4.0 openai-2.2.0 protobuf-5.29.5 py4j-0.10.9.9 typing-inspect-0.9.0 whenever-0.7.3\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting databricks-feature-engineering==0.8.1a2\n  Using cached databricks_feature_engineering-0.8.1a2-py3-none-any.whl.metadata (4.2 kB)\nCollecting mlflow-skinny<3,>=2.11.0 (from mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2)\n  Using cached mlflow_skinny-2.22.2-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: pyyaml<7,>=6 in /databricks/python3/lib/python3.12/site-packages (from databricks-feature-engineering==0.8.1a2) (6.0.2)\nRequirement already satisfied: boto3<2,>=1.16.7 in /databricks/python3/lib/python3.12/site-packages (from databricks-feature-engineering==0.8.1a2) (1.36.2)\nRequirement already satisfied: dbl-tempo<1,>=0.1.26 in /databricks/python3/lib/python3.12/site-packages (from databricks-feature-engineering==0.8.1a2) (0.1.26)\nRequirement already satisfied: azure-cosmos==4.3.1 in /databricks/python3/lib/python3.12/site-packages (from databricks-feature-engineering==0.8.1a2) (4.3.1)\nCollecting numpy<2,>=1.19.2 (from databricks-feature-engineering==0.8.1a2)\n  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\nCollecting protobuf<5,>=3.12.0 (from databricks-feature-engineering==0.8.1a2)\n  Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nRequirement already satisfied: flask<3,>=1.1.2 in /databricks/python3/lib/python3.12/site-packages (from databricks-feature-engineering==0.8.1a2) (2.2.5)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-feature-engineering==0.8.1a2) (0.4.2)\nRequirement already satisfied: azure-core<2.0.0,>=1.23.0 in /databricks/python3/lib/python3.12/site-packages (from azure-cosmos==4.3.1->databricks-feature-engineering==0.8.1a2) (1.35.0)\nRequirement already satisfied: botocore<1.37.0,>=1.36.2 in /databricks/python3/lib/python3.12/site-packages (from boto3<2,>=1.16.7->databricks-feature-engineering==0.8.1a2) (1.36.3)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from boto3<2,>=1.16.7->databricks-feature-engineering==0.8.1a2) (1.0.1)\nRequirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /databricks/python3/lib/python3.12/site-packages (from boto3<2,>=1.16.7->databricks-feature-engineering==0.8.1a2) (0.11.3)\nRequirement already satisfied: ipython in /databricks/python3/lib/python3.12/site-packages (from dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (8.30.0)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (from dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (2.2.3)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.12/site-packages (from dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (1.15.1)\nRequirement already satisfied: Werkzeug>=2.2.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from flask<3,>=1.1.2->databricks-feature-engineering==0.8.1a2) (3.1.1)\nRequirement already satisfied: Jinja2>=3.0 in /databricks/python3/lib/python3.12/site-packages (from flask<3,>=1.1.2->databricks-feature-engineering==0.8.1a2) (3.1.5)\nRequirement already satisfied: itsdangerous>=2.0 in /databricks/python3/lib/python3.12/site-packages (from flask<3,>=1.1.2->databricks-feature-engineering==0.8.1a2) (2.2.0)\nRequirement already satisfied: click>=8.0 in /databricks/python3/lib/python3.12/site-packages (from flask<3,>=1.1.2->databricks-feature-engineering==0.8.1a2) (8.1.7)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (5.5.1)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (3.0.0)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (0.67.0)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (0.116.1)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (8.7.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (1.36.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (1.36.0)\nRequirement already satisfied: packaging<25 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (24.2)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (2.12.0)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (2.32.3)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (4.15.0)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (0.35.0)\nRequirement already satisfied: azure-storage-file-datalake>12 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (12.17.0)\nRequirement already satisfied: google-cloud-storage>=1.30.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (2.10.0)\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core<2.0.0,>=1.23.0->azure-cosmos==4.3.1->databricks-feature-engineering==0.8.1a2) (1.16.0)\nRequirement already satisfied: azure-storage-blob>=12.23.0 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake>12->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (12.23.0)\nRequirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake>12->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (0.6.1)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.12/site-packages (from botocore<1.37.0,>=1.36.2->boto3<2,>=1.16.7->databricks-feature-engineering==0.8.1a2) (2.9.0.post0)\nRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /databricks/python3/lib/python3.12/site-packages (from botocore<1.37.0,>=1.36.2->boto3<2,>=1.16.7->databricks-feature-engineering==0.8.1a2) (2.3.0)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (2.40.3)\nRequirement already satisfied: starlette<0.48.0,>=0.40.0 in /databricks/python3/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (0.47.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (4.0.11)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (2.20.0)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (2.4.3)\nRequirement already satisfied: google-resumable-media>=2.3.2 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (2.7.2)\nRequirement already satisfied: zipp>=3.20 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from Jinja2>=3.0->flask<3,>=1.1.2->databricks-feature-engineering==0.8.1a2) (3.0.2)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (0.57b0)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (2.41.1)\nRequirement already satisfied: typing-inspection>=0.4.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (0.4.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (2025.1.31)\nRequirement already satisfied: h11>=0.8 in /databricks/python3/lib/python3.12/site-packages (from uvicorn<1->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (0.14.0)\nRequirement already satisfied: decorator in /databricks/python3/lib/python3.12/site-packages (from ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /databricks/python3/lib/python3.12/site-packages (from ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (0.19.2)\nRequirement already satisfied: matplotlib-inline in /databricks/python3/lib/python3.12/site-packages (from ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /databricks/python3/lib/python3.12/site-packages (from ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (4.8.0)\nRequirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /databricks/python3/lib/python3.12/site-packages (from ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (3.0.43)\nRequirement already satisfied: pygments>=2.4.0 in /databricks/python3/lib/python3.12/site-packages (from ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (2.15.1)\nRequirement already satisfied: stack_data in /databricks/python3/lib/python3.12/site-packages (from ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (0.2.0)\nRequirement already satisfied: traitlets>=5.13.0 in /databricks/python3/lib/python3.12/site-packages (from ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (5.14.3)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas->dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas->dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (2024.1)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-blob>=12.23.0->azure-storage-file-datalake>12->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (43.0.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (5.0.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /databricks/python3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (1.65.0)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /databricks/python3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (1.26.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (4.9.1)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /databricks/python3/lib/python3.12/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (1.7.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /databricks/python3/lib/python3.12/site-packages (from jedi>=0.16->ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /databricks/python3/lib/python3.12/site-packages (from pexpect>4.3->ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (0.7.0)\nRequirement already satisfied: wcwidth in /databricks/python3/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (0.2.5)\nRequirement already satisfied: anyio<5,>=3.6.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (4.11.0)\nRequirement already satisfied: executing in /databricks/python3/lib/python3.12/site-packages (from stack_data->ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (0.8.3)\nRequirement already satisfied: asttokens in /databricks/python3/lib/python3.12/site-packages (from stack_data->ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (2.0.5)\nRequirement already satisfied: pure-eval in /databricks/python3/lib/python3.12/site-packages (from stack_data->ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering==0.8.1a2) (0.2.2)\nRequirement already satisfied: sniffio>=1.1 in /databricks/python3/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (1.3.0)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.12/site-packages (from cryptography>=2.1.4->azure-storage-blob>=12.23.0->azure-storage-file-datalake>12->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (1.17.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=2.11.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (0.4.8)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.23.0->azure-storage-file-datalake>12->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.1a2) (2.21)\nUsing cached databricks_feature_engineering-0.8.1a2-py3-none-any.whl (272 kB)\nUsing cached mlflow_skinny-2.22.2-py3-none-any.whl (6.3 MB)\nUsing cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\nUsing cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\nInstalling collected packages: protobuf, numpy, mlflow-skinny, databricks-feature-engineering\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 5.29.5\n    Uninstalling protobuf-5.29.5:\n      Successfully uninstalled protobuf-5.29.5\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.1.3\n    Not uninstalling numpy at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'numpy'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 3.4.0\n    Uninstalling mlflow-skinny-3.4.0:\n      Successfully uninstalled mlflow-skinny-3.4.0\n  Attempting uninstall: databricks-feature-engineering\n    Found existing installation: databricks-feature-engineering 0.12.1\n    Not uninstalling databricks-feature-engineering at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'databricks-feature-engineering'. No files were found to uninstall.\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-agents 1.7.0 requires mlflow-skinny<4.0.0,>=3.4.0, but you have mlflow-skinny 2.22.2 which is incompatible.\nmlflow 3.4.0 requires mlflow-skinny==3.4.0, but you have mlflow-skinny 2.22.2 which is incompatible.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\ngrpcio-status 1.67.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed databricks-feature-engineering-0.8.1a2 mlflow-skinny-2.22.2 numpy-1.26.4 protobuf-4.25.8\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting pypdf\n  Using cached pypdf-6.1.1-py3-none-any.whl.metadata (7.1 kB)\nUsing cached pypdf-6.1.1-py3-none-any.whl (323 kB)\nInstalling collected packages: pypdf\nSuccessfully installed pypdf-6.1.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting langchain_community\n  Using cached langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: langchain-core<2.0.0,>=0.3.78 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langchain_community) (0.3.78)\nRequirement already satisfied: langchain<2.0.0,>=0.3.27 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langchain_community) (0.3.27)\nRequirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /databricks/python3/lib/python3.12/site-packages (from langchain_community) (2.0.37)\nCollecting requests<3.0.0,>=2.32.5 (from langchain_community)\n  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /databricks/python3/lib/python3.12/site-packages (from langchain_community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /databricks/python3/lib/python3.12/site-packages (from langchain_community) (3.11.10)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /databricks/python3/lib/python3.12/site-packages (from langchain_community) (9.0.0)\nRequirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langchain_community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langchain_community) (2.11.0)\nRequirement already satisfied: langsmith<1.0.0,>=0.1.125 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langchain_community) (0.4.33)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langchain_community) (0.4.2)\nRequirement already satisfied: numpy>=1.26.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langchain_community) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (0.3.11)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (2.12.0)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=0.3.78->langchain_community) (1.33)\nRequirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langchain-core<2.0.0,>=0.3.78->langchain_community) (4.15.0)\nRequirement already satisfied: packaging<26.0.0,>=23.2.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=0.3.78->langchain_community) (24.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.28.1)\nRequirement already satisfied: orjson>=3.9.14 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (3.11.2)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\nRequirement already satisfied: zstandard>=0.23.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\nRequirement already satisfied: python-dotenv>=0.21.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.12/site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.1.1)\nRequirement already satisfied: anyio in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.2)\nRequirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=0.3.78->langchain_community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (2.41.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.0.0)\nRequirement already satisfied: sniffio>=1.1 in /databricks/python3/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.3.0)\nUsing cached langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\nUsing cached requests-2.32.5-py3-none-any.whl (64 kB)\nInstalling collected packages: requests, langchain_community\n  Attempting uninstall: requests\n    Found existing installation: requests 2.32.3\n    Not uninstalling requests at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'requests'. No files were found to uninstall.\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmlflow 3.4.0 requires mlflow-skinny==3.4.0, but you have mlflow-skinny 2.22.2 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed langchain_community-0.3.31 requests-2.32.5\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ./util/install-prep-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e31e8f-3871-422f-8e49-c0a2a7fa7e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow>=3.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow[databricks]>=3.1.0) (3.4.0)\nRequirement already satisfied: protobuf<5.0.0,>=3.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (4.25.8)\nCollecting databricks-feature-engineering>=0.9.0\n  Using cached databricks_feature_engineering-0.13.0-py3-none-any.whl.metadata (4.3 kB)\nCollecting databricks-vectorsearch>=0.50\n  Using cached databricks_vectorsearch-0.60-py3-none-any.whl.metadata (2.8 kB)\nCollecting mlflow-skinny==3.4.0 (from mlflow>=3.1.0->mlflow[databricks]>=3.1.0)\n  Using cached mlflow_skinny-3.4.0-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: mlflow-tracing==3.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (3.4.0)\nRequirement already satisfied: Flask<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2.2.5)\nRequirement already satisfied: alembic!=1.10.0,<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.16.4)\nRequirement already satisfied: cryptography<46,>=43.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (43.0.3)\nRequirement already satisfied: docker<8,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (7.1.0)\nRequirement already satisfied: fastmcp<3,>=2.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2.12.4)\nRequirement already satisfied: graphene<4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (3.4.3)\nRequirement already satisfied: gunicorn<24 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (20.1.0)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (3.10.0)\nRequirement already satisfied: numpy<3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.26.4)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2.2.3)\nRequirement already satisfied: pyarrow<22,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (19.0.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.6.1)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.15.1)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2.0.37)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (5.5.1)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (3.0.0)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.67.0)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.116.1)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (8.7.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.36.0)\nRequirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.37.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.36.0)\nRequirement already satisfied: packaging<26 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (24.2)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2.12.0)\nRequirement already satisfied: python-dotenv<2,>=0.19.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.1.1)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (6.0.2)\nRequirement already satisfied: requests<3,>=2.17.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2.32.5)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.4.2)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (4.15.0)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.35.0)\nRequirement already satisfied: boto3<2,>=1.16.7 in /databricks/python3/lib/python3.12/site-packages (from databricks-feature-engineering>=0.9.0) (1.36.2)\nRequirement already satisfied: dbl-tempo<1,>=0.1.26 in /databricks/python3/lib/python3.12/site-packages (from databricks-feature-engineering>=0.9.0) (0.1.26)\nRequirement already satisfied: azure-cosmos==4.3.1 in /databricks/python3/lib/python3.12/site-packages (from databricks-feature-engineering>=0.9.0) (4.3.1)\nINFO: pip is looking at multiple versions of databricks-feature-engineering to determine which version is compatible with other requirements. This could take a while.\nCollecting databricks-feature-engineering>=0.9.0\n  Using cached databricks_feature_engineering-0.12.1-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: azure-core<2.0.0,>=1.23.0 in /databricks/python3/lib/python3.12/site-packages (from azure-cosmos==4.3.1->databricks-feature-engineering>=0.9.0) (1.35.0)\nRequirement already satisfied: deprecation>=2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from databricks-vectorsearch>=0.50) (2.1.0)\nRequirement already satisfied: azure-storage-file-datalake>12 in /databricks/python3/lib/python3.12/site-packages (from mlflow[databricks]>=3.1.0) (12.17.0)\nRequirement already satisfied: google-cloud-storage>=1.30.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow[databricks]>=3.1.0) (2.10.0)\nRequirement already satisfied: botocore in /databricks/python3/lib/python3.12/site-packages (from mlflow[databricks]>=3.1.0) (1.36.3)\nRequirement already satisfied: databricks-agents<2.0,>=1.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mlflow[databricks]>=3.1.0) (1.7.0)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.12/site-packages (from alembic!=1.10.0,<2->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.2.0)\nRequirement already satisfied: azure-storage-blob>=12.23.0 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]>=3.1.0) (12.23.0)\nRequirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.12/site-packages (from azure-storage-file-datalake>12->mlflow[databricks]>=3.1.0) (0.6.1)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from boto3<2,>=1.16.7->databricks-feature-engineering>=0.9.0) (1.0.1)\nRequirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /databricks/python3/lib/python3.12/site-packages (from boto3<2,>=1.16.7->databricks-feature-engineering>=0.9.0) (0.11.3)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.12/site-packages (from botocore->mlflow[databricks]>=3.1.0) (2.9.0.post0)\nRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /databricks/python3/lib/python3.12/site-packages (from botocore->mlflow[databricks]>=3.1.0) (2.3.0)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.12/site-packages (from cryptography<46,>=43.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.17.1)\nRequirement already satisfied: databricks-connect in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (17.2.2)\nRequirement already satisfied: dataclasses-json in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.6.7)\nRequirement already satisfied: jinja2>=3.0.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (3.1.5)\nRequirement already satisfied: tenacity>=8.5 in /databricks/python3/lib/python3.12/site-packages (from databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (9.0.0)\nRequirement already satisfied: tiktoken>=0.8.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.12.0)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.12/site-packages (from databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (4.67.1)\nRequirement already satisfied: whenever==0.7.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.7.3)\nRequirement already satisfied: litellm==1.75.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (1.75.9)\nRequirement already satisfied: aiohttp>=3.10 in /databricks/python3/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (3.11.10)\nRequirement already satisfied: httpx>=0.23.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.28.1)\nRequirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (4.23.0)\nRequirement already satisfied: openai>=1.99.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (2.2.0)\nRequirement already satisfied: tokenizers in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.22.1)\nRequirement already satisfied: ipython in /databricks/python3/lib/python3.12/site-packages (from dbl-tempo<1,>=0.1.26->databricks-feature-engineering>=0.9.0) (8.30.0)\nRequirement already satisfied: authlib>=1.5.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.6.5)\nRequirement already satisfied: cyclopts>=3.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (3.24.0)\nRequirement already satisfied: exceptiongroup>=1.2.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.3.0)\nRequirement already satisfied: mcp<2.0.0,>=1.12.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.16.0)\nRequirement already satisfied: openapi-core>=0.19.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.19.5)\nRequirement already satisfied: openapi-pydantic>=0.5.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.5.1)\nRequirement already satisfied: pyperclip>=1.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.11.0)\nRequirement already satisfied: rich>=13.9.4 in /databricks/python3/lib/python3.12/site-packages (from fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (13.9.4)\nRequirement already satisfied: Werkzeug>=2.2.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from Flask<4->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (3.1.1)\nRequirement already satisfied: itsdangerous>=2.0 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2.2.0)\nRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]>=3.1.0) (2.40.3)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]>=3.1.0) (2.20.0)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]>=3.1.0) (2.4.3)\nRequirement already satisfied: google-resumable-media>=2.3.2 in /databricks/python3/lib/python3.12/site-packages (from google-cloud-storage>=1.30.0->mlflow[databricks]>=3.1.0) (2.7.2)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /databricks/python3/lib/python3.12/site-packages (from graphene<4->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (3.2.4)\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from graphene<4->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (3.2.0)\nRequirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.12/dist-packages (from gunicorn<24->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (74.0.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.4.8)\nRequirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (3.2.0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas<3->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas<3->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2024.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2025.1.31)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (3.5.0)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.12/site-packages (from sqlalchemy<3,>=1.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (3.1.1)\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core<2.0.0,>=1.23.0->azure-cosmos==4.3.1->databricks-feature-engineering>=0.9.0) (1.16.0)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.12->cryptography<46,>=43.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2.21)\nRequirement already satisfied: attrs>=23.1.0 in /databricks/python3/lib/python3.12/site-packages (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (24.3.0)\nRequirement already satisfied: docstring-parser>=0.15 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.17.0)\nRequirement already satisfied: rich-rst<2.0.0,>=1.3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.3.1)\nRequirement already satisfied: langchain-openai in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from databricks-sdk[openai]>=0.58.0->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.3.35)\nRequirement already satisfied: starlette<0.48.0,>=0.40.0 in /databricks/python3/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.47.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (4.0.11)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /databricks/python3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage>=1.30.0->mlflow[databricks]>=3.1.0) (1.65.0)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /databricks/python3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage>=1.30.0->mlflow[databricks]>=3.1.0) (1.26.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage>=1.30.0->mlflow[databricks]>=3.1.0) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage>=1.30.0->mlflow[databricks]>=3.1.0) (4.9.1)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /databricks/python3/lib/python3.12/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage>=1.30.0->mlflow[databricks]>=3.1.0) (1.7.1)\nRequirement already satisfied: anyio in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from httpx>=0.23.0->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /databricks/python3/lib/python3.12/site-packages (from httpx>=0.23.0->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (1.0.2)\nRequirement already satisfied: h11<0.15,>=0.13 in /databricks/python3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.14.0)\nRequirement already satisfied: zipp>=3.20 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from jinja2>=3.0.0->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (3.0.2)\nRequirement already satisfied: httpx-sse>=0.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.4.2)\nRequirement already satisfied: pydantic-settings>=2.5.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2.11.0)\nRequirement already satisfied: python-multipart>=0.0.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.0.20)\nRequirement already satisfied: sse-starlette>=1.6.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (3.0.2)\nRequirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.3.4)\nRequirement already satisfied: more-itertools in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (10.8.0)\nRequirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.6.3)\nRequirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.7.2)\nRequirement already satisfied: parse in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.20.2)\nINFO: pip is looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\nCollecting opentelemetry-proto<3,>=1.9.0 (from mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0)\n  Using cached opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n  Using cached opentelemetry_proto-1.35.0-py3-none-any.whl.metadata (2.3 kB)\n  Using cached opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n  Using cached opentelemetry_proto-1.34.0-py3-none-any.whl.metadata (2.4 kB)\n  Using cached opentelemetry_proto-1.33.1-py3-none-any.whl.metadata (2.4 kB)\n  Using cached opentelemetry_proto-1.33.0-py3-none-any.whl.metadata (2.4 kB)\n  Using cached opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\nINFO: pip is still looking at multiple versions of opentelemetry-proto to determine which version is compatible with other requirements. This could take a while.\n  Using cached opentelemetry_proto-1.32.0-py3-none-any.whl.metadata (2.4 kB)\n  Using cached opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n  Using cached opentelemetry_proto-1.31.0-py3-none-any.whl.metadata (2.4 kB)\n  Using cached opentelemetry_proto-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n  Using cached opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Using cached opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\n  Using cached opentelemetry_proto-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n  Using cached opentelemetry_proto-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n  Using cached opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.57b0)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2.41.1)\nRequirement already satisfied: typing-inspection>=0.4.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.4.2)\nRequirement already satisfied: email-validator>=2.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from pydantic[email]>=2.11.7->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2.3.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /databricks/python3/lib/python3.12/site-packages (from rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /databricks/python3/lib/python3.12/site-packages (from rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2.15.1)\nRequirement already satisfied: regex>=2022.1.18 in /databricks/python3/lib/python3.12/site-packages (from tiktoken>=0.8.0->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (2024.11.6)\nRequirement already satisfied: grpcio-status>=1.67.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (1.67.0)\nRequirement already satisfied: grpcio>=1.67.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-connect->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (1.67.0)\nRequirement already satisfied: py4j==0.10.9.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from databricks-connect->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.10.9.9)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from dataclasses-json->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from dataclasses-json->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.9.0)\nRequirement already satisfied: decorator in /databricks/python3/lib/python3.12/site-packages (from ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering>=0.9.0) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /databricks/python3/lib/python3.12/site-packages (from ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering>=0.9.0) (0.19.2)\nRequirement already satisfied: matplotlib-inline in /databricks/python3/lib/python3.12/site-packages (from ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering>=0.9.0) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /databricks/python3/lib/python3.12/site-packages (from ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering>=0.9.0) (4.8.0)\nRequirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /databricks/python3/lib/python3.12/site-packages (from ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering>=0.9.0) (3.0.43)\nRequirement already satisfied: stack_data in /databricks/python3/lib/python3.12/site-packages (from ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering>=0.9.0) (0.2.0)\nRequirement already satisfied: traitlets>=5.13.0 in /databricks/python3/lib/python3.12/site-packages (from ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering>=0.9.0) (5.14.3)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (1.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.2.0)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (1.18.0)\nRequirement already satisfied: sniffio>=1.1 in /databricks/python3/lib/python3.12/site-packages (from anyio->httpx>=0.23.0->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (1.3.0)\nRequirement already satisfied: dnspython>=2.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from email-validator>=2.0.0->pydantic[email]>=2.11.7->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (2.8.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.4.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (5.0.0)\nINFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\nCollecting grpcio-status>=1.67.0 (from databricks-connect->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0)\n  Using cached grpcio_status-1.75.1-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.75.0-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\nINFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n  Using cached grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Using cached grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\nCollecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage>=1.30.0->mlflow[databricks]>=3.1.0)\n  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n  Using cached googleapis_common_protos-1.69.2-py3-none-any.whl.metadata (9.3 kB)\n  Using cached googleapis_common_protos-1.69.1-py2.py3-none-any.whl.metadata (9.3 kB)\n  Using cached googleapis_common_protos-1.69.0-py2.py3-none-any.whl.metadata (5.1 kB)\n  Using cached googleapis_common_protos-1.68.0-py2.py3-none-any.whl.metadata (5.1 kB)\n  Using cached googleapis_common_protos-1.67.0-py2.py3-none-any.whl.metadata (5.1 kB)\n  Using cached googleapis_common_protos-1.67.0rc1-py2.py3-none-any.whl.metadata (5.1 kB)\n  Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n  Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting databricks-connect (from databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0)\n  Using cached databricks_connect-17.2.2-py2.py3-none-any.whl.metadata (2.9 kB)\n  Using cached databricks_connect-17.2.1-py2.py3-none-any.whl.metadata (2.9 kB)\n  Using cached databricks_connect-17.2.0-py2.py3-none-any.whl.metadata (2.9 kB)\n  Using cached databricks_connect-17.1.5-py2.py3-none-any.whl.metadata (2.9 kB)\n  Using cached databricks_connect-17.1.4-py2.py3-none-any.whl.metadata (2.9 kB)\n  Using cached databricks_connect-17.1.3-py2.py3-none-any.whl.metadata (2.9 kB)\n  Using cached databricks_connect-17.1.2-py2.py3-none-any.whl.metadata (2.9 kB)\n  Using cached databricks_connect-17.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n  Using cached databricks_connect-17.1.0-py2.py3-none-any.whl.metadata (2.9 kB)\n  Using cached databricks_connect-17.0.8-py2.py3-none-any.whl.metadata (2.6 kB)\n  Using cached databricks_connect-17.0.7-py2.py3-none-any.whl.metadata (2.6 kB)\n  Using cached databricks_connect-17.0.6-py2.py3-none-any.whl.metadata (2.6 kB)\n  Using cached databricks_connect-17.0.5-py2.py3-none-any.whl.metadata (2.6 kB)\n  Using cached databricks_connect-17.0.4-py2.py3-none-any.whl.metadata (2.6 kB)\n  Using cached databricks_connect-17.0.3-py2.py3-none-any.whl.metadata (2.6 kB)\n  Using cached databricks_connect-17.0.2-py2.py3-none-any.whl.metadata (2.6 kB)\n  Using cached databricks_connect-17.0.1-py2.py3-none-any.whl.metadata (2.6 kB)\n  Using cached databricks_connect-17.0.0-py2.py3-none-any.whl.metadata (2.6 kB)\n  Using cached databricks_connect-16.4.7-py2.py3-none-any.whl.metadata (2.6 kB)\nCollecting grpcio-status>=1.59.3 (from databricks-connect->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0)\n  Using cached grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n  Using cached grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /databricks/python3/lib/python3.12/site-packages (from jedi>=0.16->ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering>=0.9.0) (0.8.4)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /databricks/python3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (2023.7.1)\nRequirement already satisfied: referencing>=0.28.4 in /databricks/python3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.30.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.22.3)\nRequirement already satisfied: pathable<0.5.0,>=0.4.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.4.4)\nRequirement already satisfied: mdurl~=0.1 in /databricks/python3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.1.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.99.5->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from openai>=1.99.5->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.10.0)\nRequirement already satisfied: rfc3339-validator in /databricks/python3/lib/python3.12/site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.1.4)\nRequirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (1.12.0)\nRequirement already satisfied: ptyprocess>=0.5 in /databricks/python3/lib/python3.12/site-packages (from pexpect>4.3->ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering>=0.9.0) (0.7.0)\nRequirement already satisfied: wcwidth in /databricks/python3/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering>=0.9.0) (0.2.5)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage>=1.30.0->mlflow[databricks]>=3.1.0) (0.4.8)\nRequirement already satisfied: docutils in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from rich-rst<2.0.0,>=1.3.1->cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow>=3.1.0->mlflow[databricks]>=3.1.0) (0.22.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (1.0.0)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.78 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.3.78)\nRequirement already satisfied: executing in /databricks/python3/lib/python3.12/site-packages (from stack_data->ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering>=0.9.0) (0.8.3)\nRequirement already satisfied: asttokens in /databricks/python3/lib/python3.12/site-packages (from stack_data->ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering>=0.9.0) (2.0.5)\nRequirement already satisfied: pure-eval in /databricks/python3/lib/python3.12/site-packages (from stack_data->ipython->dbl-tempo<1,>=0.1.26->databricks-feature-engineering>=0.9.0) (0.2.2)\nRequirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from tokenizers->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.35.3)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (2023.5.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.75.9->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (1.1.10)\nRequirement already satisfied: langsmith<1.0.0,>=0.3.45 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.4.33)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.78->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (1.33)\nRequirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.78->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (3.0.0)\nRequirement already satisfied: orjson>=3.9.14 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (3.11.2)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (1.0.0)\nRequirement already satisfied: zstandard>=0.23.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.78->langchain-openai->databricks-sdk[openai]>=0.58.0->databricks-agents<2.0,>=1.2.0->mlflow[databricks]>=3.1.0) (0.25.0)\nUsing cached mlflow_skinny-3.4.0-py3-none-any.whl (2.2 MB)\nUsing cached databricks_feature_engineering-0.12.1-py3-none-any.whl (278 kB)\nUsing cached databricks_vectorsearch-0.60-py3-none-any.whl (17 kB)\nUsing cached opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\nUsing cached databricks_connect-16.4.7-py2.py3-none-any.whl (2.5 MB)\nUsing cached grpcio_status-1.62.3-py3-none-any.whl (14 kB)\nInstalling collected packages: opentelemetry-proto, grpcio-status, databricks-connect, mlflow-skinny, databricks-vectorsearch, databricks-feature-engineering\n  Attempting uninstall: opentelemetry-proto\n    Found existing installation: opentelemetry-proto 1.37.0\n    Uninstalling opentelemetry-proto-1.37.0:\n      Successfully uninstalled opentelemetry-proto-1.37.0\n  Attempting uninstall: grpcio-status\n    Found existing installation: grpcio-status 1.67.0\n    Not uninstalling grpcio-status at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a\n    Can't uninstall 'grpcio-status'. No files were found to uninstall.\n  Attempting uninstall: databricks-connect\n    Found existing installation: databricks-connect 17.2.2\n    Uninstalling databricks-connect-17.2.2:\n      Successfully uninstalled databricks-connect-17.2.2\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.22.2\n    Uninstalling mlflow-skinny-2.22.2:\n      Successfully uninstalled mlflow-skinny-2.22.2\n  Attempting uninstall: databricks-vectorsearch\n    Found existing installation: databricks-vectorsearch 0.45\n    Uninstalling databricks-vectorsearch-0.45:\n      Successfully uninstalled databricks-vectorsearch-0.45\n  Attempting uninstall: databricks-feature-engineering\n    Found existing installation: databricks-feature-engineering 0.8.1a2\n    Uninstalling databricks-feature-engineering-0.8.1a2:\n      Successfully uninstalled databricks-feature-engineering-0.8.1a2\nSuccessfully installed databricks-connect-16.4.7 databricks-feature-engineering-0.12.1 databricks-vectorsearch-0.60 grpcio-status-1.62.3 mlflow-skinny-3.4.0 opentelemetry-proto-1.27.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# upgrade mlflow\n",
    "%pip install \"mlflow[databricks]>=3.1.0\" \"protobuf>=3.12.0,<5.0.0\" \"databricks-feature-engineering>=0.9.0\" \"databricks-vectorsearch>=0.50\" --upgrade\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce2f2b6b-5b6a-45e5-beec-e6f9345d787b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'3.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65365fd7-e0c1-4e95-90e7-8072c055185a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating a dropdown widget for model selection, as well as defining the file paths where our PDFs are stored, where we want to cache the HuggingFace model downloads, and where we want to persist our vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34076389-24d8-4468-b0c8-0302d395aeeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# where you want the PDFs to be saved in your environment\n",
    "#dbutils.widgets.text(\"UC_Volume_Path\", \"hls_llm_qa_demo.data.pdf_docs\")\n",
    "dbutils.widgets.text(\"UC_Volume_Path\", \"/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume\")\n",
    "\n",
    "# which embeddings model we want to use. We are going to use the foundation model API, but you can use custom models (i.e. from HuggingFace), External Models (Azure OpenAI), etc.\n",
    "dbutils.widgets.text(\"Embeddings_Model\", \"databricks-bge-large-en\")\n",
    "\n",
    "# publicly accessible bucket with PDFs for this demo\n",
    "#dbutils.widgets.text(\"Source_Documents\", \"s3a://db-gtm-industry-solutions/data/hls/llm_qa/\")\n",
    "dbutils.widgets.text(\"Source_Documents\", \"/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/resumes_9.22\")\n",
    "\n",
    "# Location for the split documents to be saved  \n",
    "#dbutils.widgets.text(\"Persisted_UC_Table_Location\", \"hls_llm_qa_demo.vse.hls_llm_qa_raw_docs\")\n",
    "dbutils.widgets.text(\"Persisted_UC_Table_Location\", \"dbc_adv_anlaytics_dev.yh_agent_resume.resumes_chunked_p4\")\n",
    "\n",
    "# Vector Search Endpoint Name - , hls_llm_qa_demo_vse\n",
    "#dbutils.widgets.text(\"Vector_Search_Endpoint\", \"VS_ENDPOINT\")\n",
    "dbutils.widgets.text(\"Vector_Search_Endpoint\", \"yhu01_resume_search\")\n",
    "\n",
    "# Vector Index Name \n",
    "#dbutils.widgets.text(\"Vector_Index\", \"hls_llm_qa_demo.vse.hls_llm_qa_embeddings\")\n",
    "dbutils.widgets.text(\"Vector_Index\", \"dbc_adv_anlaytics_dev.yh_agent_resume.resumes_chunked_index_p4\")\n",
    "\n",
    "# Target Catalog Name\n",
    "#dbutils.widgets.text(\"Catalog_Name\", \"hls_llm_qa_demo\")\n",
    "dbutils.widgets.text(\"Catalog_Name\", \"dbc_adv_anlaytics_dev\")\n",
    "\n",
    "# Target VSE Schema Name\n",
    "dbutils.widgets.text(\"Vse_Schema_Name\", \"yh_agent_resume\")\n",
    "\n",
    "# Target UC_CATALOG Schema Name\n",
    "dbutils.widgets.text(\"UC_CATALOG\", \"dbc_adv_anlaytics_dev\")\n",
    "\n",
    "# Target UC_SCHEMA Name\n",
    "dbutils.widgets.text(\"UC_SCHEMA\", \"yh_agent_resume\")\n",
    "\n",
    "# Target CHUNKS_DELTA_TABLE Name\n",
    "dbutils.widgets.text(\"CHUNKS_DELTA_TABLE\", \"resumes_chunked_p4\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "422ca590-f8e2-464e-aca1-e90231ad395f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#get widget values\n",
    "pdf_path = dbutils.widgets.get(\"UC_Volume_Path\")\n",
    "source_pdfs = dbutils.widgets.get(\"Source_Documents\")\n",
    "embeddings_model = dbutils.widgets.get(\"Embeddings_Model\")\n",
    "vector_search_endpoint_name = dbutils.widgets.get(\"Vector_Search_Endpoint\")\n",
    "vector_index_name = dbutils.widgets.get(\"Vector_Index\")\n",
    "UC_table_save_location = dbutils.widgets.get(\"Persisted_UC_Table_Location\")\n",
    "Persisted_UC_Table_Location = dbutils.widgets.get(\"Persisted_UC_Table_Location\")\n",
    "Vse_Schema_Name = dbutils.widgets.get(\"Vse_Schema_Name\")\n",
    "UC_CATALOG = dbutils.widgets.get(\"UC_CATALOG\")\n",
    "UC_SCHEMA = dbutils.widgets.get(\"UC_SCHEMA\")\n",
    "CHUNKS_DELTA_TABLE = dbutils.widgets.get(\"CHUNKS_DELTA_TABLE\")\n",
    "\n",
    "\n",
    "# TEMPORARY - NEED TO ADD STRING LOGIC TO GENERATE:\n",
    "volume_path = pdf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41bfa64-9784-4e56-80d6-ca8884a093f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf_path = /Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume\nsource_pdfs = /Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/Resumes_10.8\nembeddings_model = databricks-bge-large-en\nvector_search_endpoint_name = yhu01_resume_search\nvector_index_name = dbc_adv_anlaytics_dev.yh_agent_resume.resumes_chunked_index_p4\nUC_table_save_location = dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked\nPersisted_UC_Table_Location = dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked\nVse_Schema_Name = yh_agent_resume\nvolume_path = /Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume\nUC_CATALOG = dbc_adv_anlaytics_dev\nUC_SCHEMA = yh_agent_resume\nCHUNKS_DELTA_TABLE = resumes_chunked_p4\n"
     ]
    }
   ],
   "source": [
    "print(f\"pdf_path = {pdf_path}\")\n",
    "print(f\"source_pdfs = {source_pdfs}\")\n",
    "print(f\"embeddings_model = {embeddings_model}\")\n",
    "print(f\"vector_search_endpoint_name = {vector_search_endpoint_name}\")\n",
    "print(f\"vector_index_name = {vector_index_name}\")\n",
    "print(f\"UC_table_save_location = {UC_table_save_location}\")\n",
    "print(f\"Persisted_UC_Table_Location = {Persisted_UC_Table_Location}\")\n",
    "print(f\"Vse_Schema_Name = {Vse_Schema_Name}\")\n",
    "print(f\"volume_path = {volume_path}\")\n",
    "print(f\"UC_CATALOG = {UC_CATALOG}\")\n",
    "print(f\"UC_SCHEMA = {UC_SCHEMA}\")\n",
    "print(f\"CHUNKS_DELTA_TABLE = {CHUNKS_DELTA_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0daa4f-632a-4753-a33c-cfac04e1bb7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create Unity schema if it does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "748ff237-f4d8-4fa4-9917-ecd116db79c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create Unity schema if it does not exist in the Unity catalog\n",
    "-- Use IF NOT EXISTS clause to avoid errors if the schema already exists\n",
    "CREATE SCHEMA IF NOT EXISTS ${Catalog_Name}.yh_agent_resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e53d578-603c-489b-94b5-5a7e211f783f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Prep\n",
    "\n",
    "This data preparation need only happen one time to create data sets that can then be reused in later sections without re-running this part.\n",
    "\n",
    "- Grab the set of PDFs (ex: Arxiv papers allow curl, PubMed does not)\n",
    "- We have are providing a set of PDFs from PubMedCentral relating to Cystic Fibrosis (all from [PubMedCentral Open Access](https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/), all with the CC BY license), but any topic area would work\n",
    "- If you already have a repository of PDFs then you can skip this step, just organize them all in an accessible Unity Catalog Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c173e8-e263-4641-9785-906f7e4c9eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create an external volume under the newly created directory\n",
    "CREATE SCHEMA IF NOT EXISTS ${Catalog_Name}.${Vse_Schema_Name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11043b01-10da-4e11-814a-57185a86366e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/Resumes_10.8'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742b49ec-38a4-4c7b-85c9-79cdf862e430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/Resumes_10.8\n"
     ]
    }
   ],
   "source": [
    "print('file:'+source_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7683d6f-794b-4d79-bc53-8ec489e3d62e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c33531-6704-44c2-bb42-d8ac9142b7da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "copy the files to the volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b5c430-f24e-4dce-927b-fabe292287d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='file:/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/Resumes_10.8/Venkatesh Terikuti.pdf', name='Venkatesh Terikuti.pdf', size=228198, modificationTime=1759970427976),\n",
       " FileInfo(path='file:/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/Resumes_10.8/Vivek Reddy.pdf', name='Vivek Reddy.pdf', size=297823, modificationTime=1759970428018),\n",
       " FileInfo(path='file:/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/Resumes_10.8/ChunMin Jen.pdf', name='ChunMin Jen.pdf', size=63014, modificationTime=1759970428043),\n",
       " FileInfo(path='file:/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/Resumes_10.8/Parisa Sahraeian.pdf', name='Parisa Sahraeian.pdf', size=168163, modificationTime=1759970428054),\n",
       " FileInfo(path='file:/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/Resumes_10.8/Mohammad Hassanpour.pdf', name='Mohammad Hassanpour.pdf', size=65326, modificationTime=1759970428060),\n",
       " FileInfo(path='file:/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/Resumes_10.8/Apoorva Tyagi.pdf', name='Apoorva Tyagi.pdf', size=257953, modificationTime=1759970428182),\n",
       " FileInfo(path='file:/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/Resumes_10.8/Jia Yang.pdf', name='Jia Yang.pdf', size=405286, modificationTime=1759970428259)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(f'file:'+source_pdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7b9dd60-35f9-4530-b63b-0c1f30782b27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "clean up the volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "468809b9-ee6b-4049-b5ff-4327d94caa64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Source directory in workspace\n",
    "source_dir = 'file:'+source_pdfs\n",
    "\n",
    "# Destination volume path\n",
    "destination_volume = volume_path\n",
    "\n",
    "#clean up the volume\n",
    "\n",
    "destination_dir = volume_path\n",
    "# List files in the source directory\n",
    "files = dbutils.fs.ls(destination_dir)\n",
    "\n",
    "# Remove eachj file in the volume\n",
    "for file_info in files:\n",
    "    source_path = file_info.path\n",
    "    destination_path = f\"{destination_volume}/{file_info.name}\"\n",
    "    dbutils.fs.rm(destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d896d09-a2c1-425a-8065-b4919c3c50e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Move the documents to volume on unity catalog\n",
    "\n",
    "# List files in the source directory\n",
    "files = dbutils.fs.ls(source_dir)\n",
    "\n",
    "# Copy each file to the volume\n",
    "for file_info in files:\n",
    "    source_path = file_info.path\n",
    "    destination_path = f\"{destination_volume}/{file_info.name}\"\n",
    "    dbutils.fs.cp(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b999c65d-a6fc-4eaf-a9fd-601b0f51c89d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "All of the PDFs should now be accessible in the `Unity Catalog Volume` now; you can run the below command to check if you want.\n",
    "\n",
    "`dbutils.fs.ls(volume_path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32c17ed3-4de3-4a54-bad2-bbd3a3ba1e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', name='Apoorva Tyagi.pdf', size=257953, modificationTime=1759982541000),\n",
       " FileInfo(path='dbfs:/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', name='ChunMin Jen.pdf', size=63014, modificationTime=1759982540000),\n",
       " FileInfo(path='dbfs:/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', name='Jia Yang.pdf', size=405286, modificationTime=1759982541000),\n",
       " FileInfo(path='dbfs:/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', name='Mohammad Hassanpour.pdf', size=65326, modificationTime=1759982540000),\n",
       " FileInfo(path='dbfs:/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', name='Parisa Sahraeian.pdf', size=168163, modificationTime=1759982540000),\n",
       " FileInfo(path='dbfs:/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', name='Venkatesh Terikuti.pdf', size=228198, modificationTime=1759982539000),\n",
       " FileInfo(path='dbfs:/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', name='Vivek Reddy.pdf', size=297823, modificationTime=1759982539000)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(volume_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df3317c0-173a-4031-b539-2c92f2e811e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare Document DB\n",
    "\n",
    "Now it's time to load the texts that have been generated, and create a searchable database of text for use in the `langchain` pipeline. \n",
    "These documents are embedded, so that later queries can be embedded too, and matched to relevant text chunks by embedding.\n",
    "\n",
    "- Use `langchain` to reading directly from PDFs, although LangChain also supports txt, HTML, Word docs, GDrive, PDFs, etc.\n",
    "- Create a Databricks Vector Search endpoint to have a persistent vector index.\n",
    "- Use the Foundation Model APIs to generate the embeddings to sync against the vector index.\n",
    "- Sync the vector index to populate for our rag implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c7ff25-6839-4c62-b24d-3e2618b01112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create the document database:\n",
    "- Here we are using the `PyPDFDirectoryLoader` loader from LangChain ([docs page](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html#using-pypdf)) to form `documents`; `langchain` can also form doc collections directly from PDFs, GDrive files, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1244c75b-409c-47bb-955d-0f953d783a97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a6da54-bf6d-4684-b9ca-5303d4bf8dd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 04:02:42.135163: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-10-09 04:02:42.137654: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-10-09 04:02:42.256224: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-10-09 04:02:42.380744: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759982562.503396 1320943 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759982562.554780 1320943 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1759982562.906817 1320943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1759982562.906876 1320943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1759982562.906880 1320943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1759982562.906883 1320943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-10-09 04:02:43.121749: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the volume of pdf's into a list of text.  \n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# Load directly from Unity Catalog Volumes\n",
    "loader_path = volume_path\n",
    "\n",
    "pdf_loader = PyPDFDirectoryLoader(loader_path)\n",
    "docs = pdf_loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1f8f260-12d2-4d50-9ece-e1b320c2c8c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838abded-4b21-4531-b880-5d0bb45815df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc719def-f2c2-48f7-8f0a-6034ba82e7f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here we are using a text splitter from LangChain to split our PDFs into manageable chunks. This is for a few reasons, primarily:\n",
    "- LLMs (currently) have a limited context length. DRBX by default has a context length of 32k tokens. tokens (roughly words) in the prompt.\n",
    "- When we create embeddings for these documents, an NLP model (sentence transformer) creates a numerical representation (a high-dimensional vector) of that chunk of text that captures the semantic meaning of what is being embedded. If we were to embed large documents, the NLP model would need to capture the meaning of the entire document in one vector; by splitting the document, we can capture the meaning of chunks throughout that document and retrieve only what is most relevant.\n",
    "- In this case, the embeddings model we use can except a very limited number of tokens. \n",
    "- More info on embeddings: [Hugging Face: Getting Started with Embeddings](https://huggingface.co/blog/getting-started-with-embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd1de22-3787-4c9c-8225-f1679d87e26f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For PDFs we need to split them for embedding:\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# this is splitting into chunks based on a fixed number of tokens\n",
    "# the embeddings model we use below can take a maximum of 128 tokens (and truncates beyond that) so we keep our chunks at that max size\n",
    "text_splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=32)\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62a430d9-0080-45de-95ae-4dfebfbaaa16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Drop the chunked table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3da9b72a-c67a-4129-9087-0b5d551b5953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create Unity schema if it does not exist in the Unity catalog\n",
    "-- Use IF NOT EXISTS clause to avoid errors if the schema already exists\n",
    "drop table IF EXISTS dbc_adv_anlaytics_dev.yh_agent_resume.resumes_chunked_p4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba05581c-9bd4-4edb-adaa-7e7c095694f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Load the pandas data into a spark dataframe and then write to the chunked table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d04ae544-b769-49ff-86e4-cc8b45af09a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Workspace URL for printing links to the delta table/vector index\n",
    "workspace_url = SparkSession.getActiveSession().conf.get(\n",
    "    \"spark.databricks.workspaceUrl\", None\n",
    ")\n",
    "\n",
    "# Vector Search client\n",
    "vsc = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Define the schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"text\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Assuming 'docs' is a list of strings\n",
    "rows = [Row(text=doc) for doc in documents]\n",
    "\n",
    "# Create a DataFrame with the defined schema\n",
    "chunked_docs_df = spark.createDataFrame(rows, schema)\n",
    "\n",
    "# Rename columns and add an 'id' column for the primary key\n",
    "documents_with_id = chunked_docs_df.withColumnRenamed(\"text\", \"chunked_text\") \\\n",
    "                            .withColumn(\"chunk_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5161f48b-5673-4131-8627-64db8115e7f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>chunked_text</th><th>chunk_id</th></tr></thead><tbody><tr><td>page_content='Apoorva Tyagi                                   \n",
       " \n",
       "         Milpitas, CA | (669) 264 9455 | apoorva.tyagi0195@gmail.com| http://linkedin.com/in/apoorva-tyagi19 \n",
       " \n",
       " \n",
       " \n",
       "EDUCATION  \n",
       " \n",
       "California State University, East Bay, California, USA         ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>0</td></tr><tr><td>page_content='\n",
       " \n",
       " \n",
       "EDUCATION  \n",
       " \n",
       "California State University, East Bay, California, USA                                                                                                         ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>1</td></tr><tr><td>page_content='                                          Aug 2022  May 2024 \n",
       "Master of Science in Data/Business Analytics                                                                        ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>2</td></tr><tr><td>page_content='                                                                                                      \n",
       "Birla Institute of Management and Technology (BIMTECH)            ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>3</td></tr><tr><td>page_content='      \n",
       "Birla Institute of Management and Technology (BIMTECH)                                                                                                        Aug 2017  Mar 2019' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>4</td></tr><tr><td>page_content='                            Aug 2017  Mar 2019 \n",
       " \n",
       "  \n",
       " \n",
       "Master of Business Administration (MBA)                                                                               ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>5</td></tr><tr><td>page_content='                                                                                                                                ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>6</td></tr><tr><td>page_content='                                                                       \n",
       "University of Delhi                                                     ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>7</td></tr><tr><td>page_content='                                                                                                                                ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>8</td></tr><tr><td>page_content='                                                             Jul 2013  May 2016 \n",
       "Bachelors in Finance                                                        ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>8589934592</td></tr><tr><td>page_content='                                                                                                                                ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>8589934593</td></tr><tr><td>page_content='                                                       \n",
       " \n",
       " \n",
       " \n",
       "WORK EXPERIENCE \n",
       " \n",
       "Associate Data Scientist, Thermo Fisher Scientific, California, USA                                             ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>8589934594</td></tr><tr><td>page_content='                                                                    Oct 2024  Jun 2025 \n",
       "Skills: Python, SQL, Databricks, Snowflake, Azure DevOps, Machine Learning Models, ETL, Data Analysis, GIT, PySpark, LLM, NLP \n",
       " Developed and validated predictive models on high-dimensional clinical' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>8589934595</td></tr><tr><td>page_content=' ETL, Data Analysis, GIT, PySpark, LLM, NLP \n",
       " Developed and validated predictive models on high-dimensional clinical trial datasets through exploratory data analysis (EDA), achieving \n",
       "a 36% performance improvement with increased predictive stability and accuracy. \n",
       " Refactored pandas -based feature engineering pipeline to PySpark to efficiently handle millions of clinical site data rows , enabling \n",
       "scalable analysis and creation of feature tables for downstream modeling in Unity Catalog.  \n",
       " Designed and implemented data drift detection and model performance monitoring using Evidently and MLflow, improving ML pipeline' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>8589934596</td></tr><tr><td>page_content=' tables for downstream modeling in Unity Catalog.  \n",
       " Designed and implemented data drift detection and model performance monitoring using Evidently and MLflow, improving ML pipeline \n",
       "reliability and automating compliance checks. \n",
       " Automated model card generation using Azure OpenAI and GPT -based NLP models, applying structured text summarization to \n",
       "streamline documentation workflows and reduce manual effort. \n",
       " Collaborating with business stakeholders to understand requirements, present model performance metrics, and incorporate their \n",
       "feedback into feature enhancements and model refinements.   \n",
       " \n",
       "Product & Data Analytics Intern, GEICO (' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>8589934597</td></tr><tr><td>page_content=' metrics, and incorporate their \n",
       "feedback into feature enhancements and model refinements.   \n",
       " \n",
       "Product & Data Analytics Intern, GEICO (Berkshire Hathaway subsidiary), Maryland, USA                                        Jun 2023  Aug 2023 \n",
       "Skills: SQL, SAS, R, Python, PowerBI, Excel, Data Analysis, Agile \n",
       " \n",
       " Enhanced insurance product rating models by optimizing segmentation and valuation strategies' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>8589934598</td></tr><tr><td>page_content=', R, Python, PowerBI, Excel, Data Analysis, Agile \n",
       " \n",
       " Enhanced insurance product rating models by optimizing segmentation and valuation strategies, increasing market competitivene ss \n",
       "and profitability. \n",
       " Applied R to uncover patterns and statistical insights in large datasets, supporting strategic decision -making across product lines. \n",
       " Extracted, cleaned, and analyzed data from complex databases using SQL and SAS, ensuring accuracy and reliability in reportin g. \n",
       " Developed interactive dashboards in Power BI using DAX to visualize trends and improve data-driven communication with stakeholders. \n",
       " Adapted' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>8589934599</td></tr><tr><td>page_content='. \n",
       " Developed interactive dashboards in Power BI using DAX to visualize trends and improve data-driven communication with stakeholders. \n",
       " Adapted to Agile methodologies, actively participating in daily stand -ups, and consistently achieving bi-weekly sprint goals. \n",
       " \n",
       "Data Analyst (Associate Manager), Mahindra Insurance Brokers                                                        ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>8589934600</td></tr><tr><td>page_content='                                                            May 2019  Oct 2021 \n",
       "Skills: SQL, Tableau, Excel, Data Analysis, ETL, Data Mapping, KPI Reporting, Predictive Analytics \n",
       " Prepared and analyzed data by writing SQL queries, created and updated over 50 data tables in the database to store various data units, \n",
       "and used them as' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>17179869184</td></tr><tr><td>page_content=' Prepared and analyzed data by writing SQL queries, created and updated over 50 data tables in the database to store various data units, \n",
       "and used them as sources for dashboards.  \n",
       " Created Tableau dashboards to dynamically represent critical business metrics, enhancing data -centric strategic decisions across the \n",
       "organization, identifying opportunities for business growth and revenue enhancement, resulting in a 15% increase in revenue.  \n",
       " Coordinated cross -functional teams to develop and execute data -driven strategies aimed at increasing customer acquisition and \n",
       "retention by 35%.  \n",
       " Consolidated data from multiple sources via ETL' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>17179869185</td></tr><tr><td>page_content=' and execute data -driven strategies aimed at increasing customer acquisition and \n",
       "retention by 35%.  \n",
       " Consolidated data from multiple sources via ETL processes to identify patterns and streamline operations, improving data acce ssibility \n",
       "and efficiency.  \n",
       " Proactively did trend analysis and pursued new business opportunities through market research, and lead generation. Creation of sales \n",
       "dashboards through querying database and collection of relevant information, including marketing analysis, clients prefer ences, \n",
       "budget, and timeline.  \n",
       " \n",
       "PROJECTS  \n",
       " \n",
       "    Multi' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>17179869186</td></tr><tr><td>page_content='s prefer ences, \n",
       "budget, and timeline.  \n",
       " \n",
       "PROJECTS  \n",
       " \n",
       "    Multi-Document Retrieval Chatbot [Python, Visual Studio, LangChain, OpenAI API, Embeddings, Vector stores] \n",
       " Developed a multi -document retrieval and chatbot system using LangChain and ChatGPT, enabling efficient extraction and \n",
       "summarization of information from diverse text sources.  Integrated document parsing, vector embeddings, and conversational AI to \n",
       "enhance user interactions, providing accurate and context-aware responses in a' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>17179869187</td></tr><tr><td>page_content='.  Integrated document parsing, vector embeddings, and conversational AI to \n",
       "enhance user interactions, providing accurate and context-aware responses in a scalable framework.  \n",
       "Customer Churn Prediction Model Development [Python, Logistic Regression, kNN, XG Boost, Data Analysis, Data Modeling] \n",
       " Conducted data analysis and data modeling using logistic regression, KNN, and Decision Trees models to predict customer churn \n",
       "for a telecom company.  \n",
       " \n",
       "TECHNICAL SKILLS \n",
       " \n",
       "Coursework  Data Analytics, Data Warehousing, Business Intelligence,' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>17179869188</td></tr><tr><td>page_content=' telecom company.  \n",
       " \n",
       "TECHNICAL SKILLS \n",
       " \n",
       "Coursework  Data Analytics, Data Warehousing, Business Intelligence, Data Visualization, Database Management, Machine Learning \n",
       "Programming Languages  Python, SQL, R \n",
       "Tools  Databricks, Snowflake, Azure DevOps, SAS, Tableau, MySQL, MS Excel, Power BI, Coginiti, Visual Studio, GIT, Google Analytics' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}</td><td>17179869189</td></tr><tr><td>page_content='CHUN MIN JENCurrent Location: San Jose, CA 95129SUMMARY:Innovative Data Scientist and Machine Learning/GenAI Developer with over 7 years of experience delivering end-to-end data-driven solutions across healthcare, finance, automotive, and research industries. Expertise in building scalable ETL pipelines, architecting cloud-native analytics platforms, and deploying AI/ML models using AWS services (SageMaker, Bedrock, Lambda, Glue, QuickSight). Skilled in LLM-powered RAG systems, vector databases, and advanced statistical modeling to generate actionable insights that improve decision' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}</td><td>17179869190</td></tr><tr><td>page_content='ue, QuickSight). Skilled in LLM-powered RAG systems, vector databases, and advanced statistical modeling to generate actionable insights that improve decision-making and operational efficiency. Proven track record in developing interactive dashboards, predictive models, and recommendation systems that drive measurable business outcomes. Adept at leading cross-functional teams, optimizing data pipelines for high accuracy and performance, and spearheading GenAI adoption strategies to reduce costs and accelerate innovation. Holds a PhD in Experimental Nuclear Physics and five AWS certifications, including Machine Learning Specialty and Data Engineer Associate.EDUCATION AND CERTIFICATION:PhD in' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}</td><td>17179869191</td></tr><tr><td>page_content=' in Experimental Nuclear Physics and five AWS certifications, including Machine Learning Specialty and Data Engineer Associate.EDUCATION AND CERTIFICATION:PhD in Experimental Nuclear Physics Syracuse University Syracuse, NY 2013 \"AWS Certified Machine Learning  \u0013 Associate. \"AWS Certified AI Practitioner Early Adopter. \"AWS Certified Data Engineer  \u0013 Associate. \"AWS Certified Machine Learning  \u0013 Specialty. \"AWS Certified Cloud Practitioner. \"TECHNICAL SKILLS:Programming & Data Science: Python, R, SQL, PySpark, TensorFlow, Keras,' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}</td><td>17179869192</td></tr><tr><td>page_content=' \"TECHNICAL SKILLS:Programming & Data Science: Python, R, SQL, PySpark, TensorFlow, Keras, LangChain, Pandas, NumPy, Scikit- \"learn, Matplotlib, Seaborn, NLP, Reinforcement Learning.Cloud & Big Data Platforms: AWS (SageMaker, Bedrock, Lambda, Step Functions, Glue, Athena, EMR, EC2, S3,  \"QuickSight, CloudFormation), Snowflake, Databricks, Apache Spark, HadoopDatabases & Storage: PostgreSQL' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}</td><td>25769803776</td></tr><tr><td>page_content='3,  \"QuickSight, CloudFormation), Snowflake, Databricks, Apache Spark, HadoopDatabases & Storage: PostgreSQL (Pgvector), MySQL, NoSQL, MongoDB, Redshift, Data Lakes, OMOP CDM \"Machine Learning & GenAI: LLMs, RAG systems, Vector Databases, Embedding Models, Recommendation  \"Systems, Predictive Modeling, Deep Learning, Synthetic Data (Synthea)ETL & Data Engineering: ETL Pipeline Design, Data Warehousing, Data Quality Monitoring, Workflow  \"Automation, Serverless' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}</td><td>25769803777</td></tr><tr><td>page_content='Synthea)ETL & Data Engineering: ETL Pipeline Design, Data Warehousing, Data Quality Monitoring, Workflow  \"Automation, Serverless ArchitectureVisualization & BI Tools: AWS QuickSight, Tableau, Power BI, Apache Superset, Dashboard Development,  \"Geospatial AnalyticsSoftware Development: C/C++, MATLAB, Flask/RESTful APIs, Git, Linux, Agile/Scrum \"Statistical & Analytical Tools: Hypothesis Testing, Time Series Analysis, Regression Models, Forecasting, Data  \"Mining, Feature EngineeringPROFESSIONAL' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}</td><td>25769803778</td></tr><tr><td>page_content=' Analytical Tools: Hypothesis Testing, Time Series Analysis, Regression Models, Forecasting, Data  \"Mining, Feature EngineeringPROFESSIONAL EXPERIENCE: THECLINICIAN HOLDINGS LIMITED| San Jose, CA                                                                           Nov 2024  ' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}</td><td>25769803779</td></tr><tr><td>page_content='                             Nov 2024  \u0013 PresentData Scientist & Machine Learning/Gen AI Developer (Full-time Contract)Created PROM-agent (Patient Report Outcome Measured) on AWS bedrock to automate Transformer-based  \"semantic analytics developed on AWS Sagemaker with LLM-based RAG-enriched patient appointment data in TensorFlowBuilt the LangChain integrated framework for the contextual, role-based patient report with text representation  \"techniques for RAG systems embedded in vector' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}</td><td>25769803780</td></tr><tr><td>page_content=' TensorFlowBuilt the LangChain integrated framework for the contextual, role-based patient report with text representation  \"techniques for RAG systems embedded in vector Postgres databases with which the Pgvector is associated as the search engine. Also developing and integrating an MCP server to empower the agentic AI functionality Refined database queries using query optimization techniques, slashing average load times by 42% and  \"demonstrably improving client satisfaction ratings via monthly performance reports.Designed and developed interactive dashboards in AWS QuickSight for patient demographics, encounters, and  \"clinical insights.Built visualizations to analyze hospital performance, surgeon-' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}</td><td>25769803781</td></tr><tr><td>page_content=' and developed interactive dashboards in AWS QuickSight for patient demographics, encounters, and  \"clinical insights.Built visualizations to analyze hospital performance, surgeon-level surgeries, and encounter trends across  \"multiple facilities.Implemented COVID-19 case tracking dashboards with encounter types, procedures, and treatment cost  \"breakdowns.Created financial dashboards to monitor revenue, treatment costs, and engagement rates by department and  \"activity.' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}</td><td>25769803782</td></tr><tr><td>page_content='Leveraged QuickSight for geographical analysis, mapping patient data by postal codes to identify regional  \"healthcare trends.Automated KPI monitoring for encounter volume, patient outcomes, and average encounter duration. \"Delivered data-driven insights for executive reporting, population health, and clinical operations optimization. \"Launched a predictive market forecasting model using historical outcome measure data (2-year trend) to estimate  \"a 12% growth response rate, optimizing survey effectiveness strategies, and increasing positive feedback rate projections by 20% quarterlyAccelerated data accessibility for stakeholders by building scalable ETL pipelines in SQL and Python, validating  \"' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}</td><td>25769803783</td></tr><tr><td>page_content=' and increasing positive feedback rate projections by 20% quarterlyAccelerated data accessibility for stakeholders by building scalable ETL pipelines in SQL and Python, validating  \"500,000 records per day with 99.99% accuracy, and reducing product integration time by 15% per release.Leveraged 5+ years of experience to spearhead the migration of legacy data systems to cloud-based solutions  \"using AWS, reducing processing time by 40%.Conducted survey research to assess patient satisfaction, applying statistical tools such as R and Python to  \"analyze data trends, resulting in a 25% increase in actionable insights by creating data' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}</td><td>25769803784</td></tr><tr><td>page_content=' patient satisfaction, applying statistical tools such as R and Python to  \"analyze data trends, resulting in a 25% increase in actionable insights by creating data visualizations and dashboards, enabling stakeholders to make informed decisions and accelerate strategic initiatives by reducing decision-making time by 2 weeks to get prototype developed within 1-2 months.Crafted a centralized data visualization catalog for all analytics projects, resulting in a 50% increase in data  \"discoverability and reuse, which fostered greater collaboration across teams and increased data integrity.Orchestrated the creation of 10+ interactive business intelligence dashboards utilizing Tableau, Power BI,' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}</td><td>34359738368</td></tr><tr><td>page_content=' fostered greater collaboration across teams and increased data integrity.Orchestrated the creation of 10+ interactive business intelligence dashboards utilizing Tableau, Power BI, and  \"Apache Superset, displaying critical KPIs like patient readmission rates and treatment progress, presented weekly to executives.Revolutionized data processing by constructing a serverless ETL framework with AWS Lambda and Step  \"Functions; boosted real-time analysis of 600 GB daily and reduced data lag by 12 hours.Expanded role to lead GenAI tech evaluation, creating three data pipeline prototypes within three months and  \"reducing model training expenses by 15%' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}</td><td>34359738369</td></tr><tr><td>page_content=' 12 hours.Expanded role to lead GenAI tech evaluation, creating three data pipeline prototypes within three months and  \"reducing model training expenses by 15% through infrastructure investment insights.Executed the adoption of serverless architecture utilizing AWS Lambda, personally refactoring 10+ legacy  \"functions and decreasing individual function execution time by an average of 20ms.Self-Employed/Independent Contractor | San Jose, CA                                         ' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}</td><td>34359738370</td></tr><tr><td>page_content='                                                          Oct 2023  \u0013 Apr 2025                                                               ' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}</td><td>34359738371</td></tr><tr><td>page_content='                                                AWS Cloud Data Scientist (Part-time Contract)  Steered feature engineering initiatives for the core recommendation system, lifting the model's precision by 15%  \"and driving a 22% surge in user interaction, contributing to higher click-through rates.Discovered biases in training data using statistical methods and collaborated with cross-functional teams to refine  \"data collection, decreasing model errors by 40% and improving' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}</td><td>34359738372</td></tr><tr><td>page_content='.Discovered biases in training data using statistical methods and collaborated with cross-functional teams to refine  \"data collection, decreasing model errors by 40% and improving overall accuracy.Spearheaded data mining projects, analyzing large data sets over 12 months, utilizing SQL and Python to achieve  \"a 35% improvement in predictive modelConducted comprehensive statistical analyses on datasets, using R and Python over 9 months, to optimize  \"decision-making processes, resulting in a 20% increase in forecast accuracy.Developed an AI-driven recommendation system in TensorFlow utilizing synthetic healthcare data, prompting  \"various LLM models for' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}</td><td>34359738373</td></tr><tr><td>page_content='% increase in forecast accuracy.Developed an AI-driven recommendation system in TensorFlow utilizing synthetic healthcare data, prompting  \"various LLM models for patient insights. Integrated Synthea-generated records mapped to OMOP CDM, enabling clinicians to identify optimal chronic disease treatments and improve early intervention strategies. VOLKSWAGEN | Belmont, CA                                                        ' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}</td><td>34359738374</td></tr><tr><td>page_content='                                                                                     Apr 2023  \u0013 May 2023  Sr. Software Engineer & Machine Learning Developer Engineered a synthetic ETL data pipeline utilizing electric vehicle simulations to train reinforcement learning  \"models in TensorFlow, achieving a' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}</td><td>34359738375</td></tr><tr><td>page_content=' Software Engineer & Machine Learning Developer Engineered a synthetic ETL data pipeline utilizing electric vehicle simulations to train reinforcement learning  \"models in TensorFlow, achieving a 10x improvement in read-in and read-out event processing speeds, completing the project 2 weeks ahead of schedule.Reduced data latency in the reinforcement learning pipeline by 60 milliseconds by optimizing data compression  \"algorithms, improving event rates by a factor of ten.Spearheaded the integration of real-world driving data into EV simulation models, resulting in solutions to fix the  \"three biggest causes of simulation-reality gaps, thus increasing accuracy by 15%.Faraday' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}</td><td>34359738376</td></tr><tr><td>page_content=' driving data into EV simulation models, resulting in solutions to fix the  \"three biggest causes of simulation-reality gaps, thus increasing accuracy by 15%.Faraday Future | San Jose, CA                                                                                          ' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}</td><td>42949672960</td></tr><tr><td>page_content='                                                 May 2022  \u0013 Mar 2023Staff Data Engineer Forged automation workflows within AWS infrastructure on EC2; empowered teams with near-real-time, scalable  \"failure statistic dashboards; and leveraged Apache Superset for big data visualization, achieving 2x runtime efficiency.Implemented Flask/RESTful APIs for real-time analytics, resolving large-scale operational challenges' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}</td><td>42949672961</td></tr><tr><td>page_content=' for big data visualization, achieving 2x runtime efficiency.Implemented Flask/RESTful APIs for real-time analytics, resolving large-scale operational challenges and leading  \"to a 40% reduction in system downtime during peak traffic hours.Streamlined feature engineering using AWS Crawl, Glue, and Athena, achieving a 99.99% data ETL pipeline  \"uptime; Also, personally validated the accuracy of 500+ data transformations daily.' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}</td><td>42949672962</td></tr><tr><td>page_content='Piloted the creation of five novel data quality thresholds by deploying AWS Glue and Athena, which allowed the  \"team to detect 50+ data defects each day, elevating overall data integrity.Instituted automated data quality controls with AWS Glue and Athena, flagging 75+ daily data defects, thereby  \"increasing ETL data pipeline success rates by 35% and driving down inconsistencies by 60%.MORGAN STANLEY| Providence, RI                                    ' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}</td><td>42949672963</td></tr><tr><td>page_content='                                                                                               Jan 2021 - May 2022  Data Software Engineer & Machine Learning Developer Executed 50+ data quality tests weekly on top of ETL data pipeline via complex SQL and Python3' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}</td><td>42949672964</td></tr><tr><td>page_content=' - May 2022  Data Software Engineer & Machine Learning Developer Executed 50+ data quality tests weekly on top of ETL data pipeline via complex SQL and Python3 on Snowflake,  \"guaranteeing 100% accuracy in data, leading to zero data-related incidents and top performer.Mastered the data migration process from SQL to Salesforce, achieving 99.9% accuracy on over 10,000 rows of  \"data, and identified data duplicates using Pandas, within two weeks of starting.Automated feature extraction from 10,000+ scraped web pages using NLP models in TensorFlow, and facilitated ' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}</td><td>42949672965</td></tr><tr><td>page_content=', within two weeks of starting.Automated feature extraction from 10,000+ scraped web pages using NLP models in TensorFlow, and facilitated  \"80% accuracy in job seeker recommendations, using Python, Pandas, and AWS EMR via Visual Studio.Revamped data preprocessing stage for training neural networks on PySpark; findings led to fixing the three  \"biggest causes of model bias, improving fairness metrics by 8%. LOS ALAMOS NATIONAL LAB | Batavia, IL                       ' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}</td><td>42949672966</td></tr><tr><td>page_content=' NATIONAL LAB | Batavia, IL                                                                                       Aug 2018 - Feb 2020      Postdoc Scientist Integrated the data trigger emulator into the BNL-PHENIX-Fun4All framework using C' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}</td><td>42949672967</td></tr><tr><td>page_content=' 2018 - Feb 2020      Postdoc Scientist Integrated the data trigger emulator into the BNL-PHENIX-Fun4All framework using C++ on Linux, achieving over  \"90% precision to support time series analysis and timing data production.Championed the C++ algorithm calibration process for detector readout electronics, reducing average noise rate  \"across all channels by two orders of magnitude; resolved signal processing issues through systematic troubleshooting.Enhanced data calibration algorithm reconstruction in C++ on Linux, improving multi-class classification accuracy  \"from 30+% to a range of 80 \u001399%.Formulated MAT' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}</td><td>42949672968</td></tr><tr><td>page_content=' algorithm reconstruction in C++ on Linux, improving multi-class classification accuracy  \"from 30+% to a range of 80 \u001399%.Formulated MATLAB scripts to refine brain image calibration, diminishing artifacts by 5% and amplifying ROI- \"masked regression matrix intensity by 15% for precise 3D image segmentation processes.Fabricated a data capture system using C/C++ within a Linux environment, improving multi-channel reading rates  \"by 400%, from 20MB/sec to 80MB/sec; this infrastructure now supports 10+ concurrent research projects.' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}</td><td>51539607552</td></tr><tr><td>page_content='JIA  YANG  \n",
       "\uD83D\uDCCD San Diego, CA  \n",
       " (860) 834-3983  \n",
       "\n",
       " jiayanglu01@gmail.com   LinkedIn URL \n",
       "P R O F I L E \n",
       "Data Scientist with extensive experience delivering end -to-end analytics solutions, from data \n",
       "pipelines to production-ready machine learning and AI models. Expertise spans predictive modeling, \n",
       "advanced statistical analysis, NLP, and Generative AI, leveraging Python, R, Spark, and cloud  \n",
       "platforms. Proven' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>51539607553</td></tr><tr><td>page_content=' \n",
       "advanced statistical analysis, NLP, and Generative AI, leveraging Python, R, Spark, and cloud  \n",
       "platforms. Proven ability to translate complex analytics into actionable insights, drive process \n",
       "optimization, and support healthcare and operational decision -making at scale. Adept at \n",
       "collaborating with cross-functional teams to integrate data-driven solutions into business processes. \n",
       "TE C H N I C A L S K I L L S \n",
       " Programming & Frameworks: Python, SQL, R, SAS, Git, Docker \n",
       " Machine Learning &' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>51539607554</td></tr><tr><td>page_content=' L L S \n",
       " Programming & Frameworks: Python, SQL, R, SAS, Git, Docker \n",
       " Machine Learning & Statistics: Supervised & Unsupervised Learning, Regression, Ensemble \n",
       "Methods, Experimental Design, Causal Inference, Bayesian Modeling, Hypothesis Testing, A/B \n",
       "Testing, Time Series Analysis, Survival Analysis, Clustering \n",
       " Deep Learning & AI: Natural Language Processing (NLP), Transformers, Neural Networks \n",
       " Big Data & Cloud: Databricks, Apache Spark, Real-time Streaming' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>51539607555</td></tr><tr><td>page_content=' Natural Language Processing (NLP), Transformers, Neural Networks \n",
       " Big Data & Cloud: Databricks, Apache Spark, Real-time Streaming \n",
       " Data Visualization & Tools : Tableau, Matplotlib, Seaborn, Plotly, Jupyter, Quarto, Interactive \n",
       "Dashboards, APIs \n",
       "P R O F E S S I O N A L E X P E R I E N C E \n",
       "Data Science/Statistical Consultant, North Carolina State University | 2023  Present \n",
       " Established robust data engineering frameworks using Py' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>51539607556</td></tr><tr><td>page_content=' \n",
       "Data Science/Statistical Consultant, North Carolina State University | 2023  Present \n",
       " Established robust data engineering frameworks using PySpark and Spark SQL for real-time and \n",
       "large-scale analytics. Created forecasting pipelines with  advanced feature engineering (e.g., \n",
       "PCA) and Elastic Net regularization for proactive energy management, and analyzed  NFL \n",
       "statistics (2005-2023) for player evaluations, maximizing strategic and operational insights. \n",
       " Drove strategic decision-making by deploying production-ready predictive modeling pipelines for \n",
       "critical classification and regression problems' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>51539607557</td></tr><tr><td>page_content=' strategic and operational insights. \n",
       " Drove strategic decision-making by deploying production-ready predictive modeling pipelines for \n",
       "critical classification and regression problems. Applied diverse  ML models (e.g., XGBoost, \n",
       "Random Forest, SVM, KNN)  with feature engineering  and regularization to evaluate diabetes \n",
       "risk, lung cancer severity, and wine quality/type, achieving up to  100% accuracy and ensuring \n",
       "scalability via a Dockerized Plumber API.  \n",
       " Synthesized advanced AI and big data methodologies  to deliver actionable insights' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>51539607558</td></tr><tr><td>page_content='alability via a Dockerized Plumber API.  \n",
       " Synthesized advanced AI and big data methodologies  to deliver actionable insights and inform \n",
       "strategic understanding.  Fine-tuned DeBERTa and DistilBERT models with TensorFlow, \n",
       "achieving >81% accuracy on Google reviews for sentiment analysis, and documented  real-time \n",
       "data streaming architectures (Kafka, Kinesis) to inform data ingestion and processing strategies. \n",
       " Engineered advanced statistical solutions  optimizing business strategy through  survival \n",
       "analysis for lung cancer mortality risk,' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>51539607559</td></tr><tr><td>page_content=' data ingestion and processing strategies. \n",
       " Engineered advanced statistical solutions  optimizing business strategy through  survival \n",
       "analysis for lung cancer mortality risk,  Bayesian models  for censored hierarchi cal data in the \n",
       "poultry industry, and  time-series models for air quality evaluation.  Boosted prediction accuracy \n",
       "by 31% and informed dosing strategies and risk assessment via an interactive R Shiny app. \n",
       " Pioneered comprehensive data analysis frameworks  delivering reproducible, actionable \n",
       "insights via an  R/Quarto pipeline  with GitHub for U.S. Census data' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>51539607560</td></tr><tr><td>page_content=' analysis frameworks  delivering reproducible, actionable \n",
       "insights via an  R/Quarto pipeline  with GitHub for U.S. Census data, a  SAS pipeline  for' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>60129542144</td></tr><tr><td>page_content='JIA YANG                                                                                                                            ' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>60129542145</td></tr><tr><td>page_content='                                   \n",
       " jiayanglu01@gmail.com \n",
       "2 \n",
       " \n",
       "regulatory-style clinical trial reports, and an  interactive R Shiny app  (openFDA API) for adverse \n",
       "event trend analysis and downloadable data. \n",
       "Scientist, Sorrento Therapeutics, San Diego, CA | 2022  2023 \n",
       " Applied advanced statistical analyses, including  Survival analysis, to evaluate gene and' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>60129542146</td></tr><tr><td>page_content=' Therapeutics, San Diego, CA | 2022  2023 \n",
       " Applied advanced statistical analyses, including  Survival analysis, to evaluate gene and cell \n",
       "therapy outcomes, resulting in contributions to peer-reviewed publications.  \n",
       " Collaborated with cross-functional teams, enhancing analytical workflow efficiency and ensuring \n",
       "data reproducibility for R&D projects. \n",
       "Postdoctoral Scholar, University of California San Diego, La Jolla, CA | 2011 2014 \n",
       " Developed and led statistical and bioinformatic analyses  on large genomic datasets, \n",
       "identifying crucial' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>60129542147</td></tr><tr><td>page_content=' CA | 2011 2014 \n",
       " Developed and led statistical and bioinformatic analyses  on large genomic datasets, \n",
       "identifying crucial biomarkers that informed key research areas. \n",
       "Postdoctoral Research Associate, Wesleyan University, Middletown, CT | 2008  2011 \n",
       " Applied rigorous statistical methods and experimental design to evaluate stem cell therapies. \n",
       "E D U C AT I O N \n",
       "Master of Statistics, North Carolina State University, NC (July 2025; GPA 4.0) \n",
       "Ph.D. in Pharmacology, Shenyang Pharmaceutical University, China (2008' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>60129542148</td></tr><tr><td>page_content=' Carolina State University, NC (July 2025; GPA 4.0) \n",
       "Ph.D. in Pharmacology, Shenyang Pharmaceutical University, China (2008) \n",
       "B.S. in Pharmaceutical Sciences, Shenyang Pharmaceutical University, China (2003) \n",
       "C E R T I F I C A T I O N &  H O N O R S \n",
       " Micro-MBA, Rady School of Management, UC San Diego \n",
       " Early Career Forum Travel Award, The Endocrine Society \n",
       " Postdoctoral Travel Award, American Society for Biochemistry and Molecular Biology ' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>60129542149</td></tr><tr><td>page_content=' Early Career Forum Travel Award, The Endocrine Society \n",
       " Postdoctoral Travel Award, American Society for Biochemistry and Molecular Biology \n",
       " Outstanding Doctoral Dissertation, Shenyang Pharmaceutical University \n",
       "S E L E C T E D P U B L I C AT I O N S & P R E S E N T AT I O N S \n",
       "Tahir Y, Yang J , et al. Impact of Sodium Bisulfate and Water Activity on Salmonella Survival in \n",
       "Poultry Litter. Manuscript in preparation. (Logistic, GLM,' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>60129542150</td></tr><tr><td>page_content=' Sodium Bisulfate and Water Activity on Salmonella Survival in \n",
       "Poultry Litter. Manuscript in preparation. (Logistic, GLM, Bayesian modeling) \n",
       "Chen Z, [], Yang J , et al. Antibody -based binding domain fused to TCR chain facilitates T cell \n",
       "cytotoxicity for potent anti-tumor response. Oncogenesis. 2023;12:33. (Survival analysis, ANOVA) \n",
       "Lu YJ 1 , Sewer MB. p54 nrb/NONO regulates cAMP -dependent glucocort' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>60129542151</td></tr><tr><td>page_content=' analysis, ANOVA) \n",
       "Lu YJ 1 , Sewer MB. p54 nrb/NONO regulates cAMP -dependent glucocorticoid production by \n",
       "modulating phosphodiesterase mRNA splicing and degradation. Mol Cell Biol.  2015;35:1223-37. \n",
       "(Bioinformatics analysis)  \n",
       "Yang J , Sewer MB. p54 nrb/NONO is required for adrenocortical circad ian gene expression and \n",
       "glucocorticoid production. The Endocrine Societys Annual Meeting , San Francisco,' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>60129542152</td></tr><tr><td>page_content='ortical circad ian gene expression and \n",
       "glucocorticoid production. The Endocrine Societys Annual Meeting , San Francisco, 2013. \n",
       "(Experimental design, Hypothesis testing) \n",
       "                                                             \n",
       "1 Publication is under the name Jia Yang Lu .' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>68719476736</td></tr><tr><td>page_content='Mohammad HassanpourHealthcare Data Scientist, Clinical AI & Predictive AnalyticsLos Angeles, CA,  mohassan99@gmail.com, 415.264.6938LinkedIn,  GitHub,  PortfolioProfessional SummaryData scientist with 10 /+ /years building interpretable deep-learning and large-scale analytics for healthcare. Expert in Python, R, PyTorch, Spark/Databricks, MLflow and Hugging Face; domain leader in ICD-10, HEDIS/STARs and HIPAA-compliant deployment.EducationM.S. Computer Science (Data Science)  \u0014' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>68719476737</td></tr><tr><td>page_content='CD-10, HEDIS/STARs and HIPAA-compliant deployment.EducationM.S. Computer Science (Data Science)  \u0014 University /of /Illinois /Urbana-ChampaignM.A. Applied Statistics  \u0014 UC Santa /BarbaraB.A. Economics  \u0014 UC San /DiegoCore Technical Skills- Languages & Libraries: Python, R, SQL, SAS, VBA, NumPy, Pandas, Scikit-learn- Deep Learning & NLP: PyTorch, TensorFlow, RETAIN, Transformer Models, Attention Mechanisms,' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>68719476738</td></tr><tr><td>page_content=', Scikit-learn- Deep Learning & NLP: PyTorch, TensorFlow, RETAIN, Transformer Models, Attention Mechanisms, LSTM, GRU, RNN, CNN, Agenic AI systems, EHR Embedding, Autoencoders, ICD-10 Concept Extraction, Clinical Concept Modeling- Classical Machine Learning: Logistic Regression, GLM, Lasso, Ridge, Elastic Net, PCA, Decision Trees, Random Forest, SVM, KNN, Naive Bayes, Discriminant Analysis, Time Series Forecasting, - MLOps & Platform' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>68719476739</td></tr><tr><td>page_content=' Decision Trees, Random Forest, SVM, KNN, Naive Bayes, Discriminant Analysis, Time Series Forecasting, - MLOps & Platforms: MLflow, Git, Databricks, SQL Server, Teradata SQL, SSIS, FastAPI, Streamlit, Torchvision- Visualization & Reporting: Power BI, Tableau- Cloud & DevOps: Azure DevOps, Bash, Multithreading, Email-integrated Automation Workflows- Healthcare Domain: Clinical Informatics, ICD-10 Coding, Claims Processing, HEDIS, STARS Quality Metrics, Healthcare Operations,' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>68719476740</td></tr><tr><td>page_content='- Healthcare Domain: Clinical Informatics, ICD-10 Coding, Claims Processing, HEDIS, STARS Quality Metrics, Healthcare Operations, EMR/EHR Data ModelingProfessional ExperienceCentene Corporation, Principal Data ScientistLos Angeles, CA, Nov /2023 / \u0013 /Feb /2025- Deployed interpretable CHF-risk model (RETAIN) on 100 /M /+ EHRs, achieving AUC /0.86 / 80 /% accuracy and surpassing logistic (0.78) and RNN (0.83) baselines.- Refactored Hugging' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>68719476741</td></tr><tr><td>page_content='86 / 80 /% accuracy and surpassing logistic (0.78) and RNN (0.83) baselines.- Refactored Hugging /Face transformers for ICD-10 NER, cutting manual coding workload 60 /% and boosting coder throughput.- Cut ETL latency 42 /% via Spark-optimized preprocessing and orchestrated MLflow tracking across production clusters.- Led HIPAA-compliant Azure /ML deployment and cross-functional rollout, tripling clinician adoption of risk dashboards.Centene Corporation, Senior Data ScientistLos Angeles, CA, Oct /2020 / \u0013 /Nov /20' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>68719476742</td></tr><tr><td>page_content=' tripling clinician adoption of risk dashboards.Centene Corporation, Senior Data ScientistLos Angeles, CA, Oct /2020 / \u0013 /Nov /2023- Migrated legacy HEDIS/STARs analytics to DAX + Power BI, shrinking reporting time from three days to two hours.- Automated end-to-end claims QA in SQL Server/SSIS/Python, reducing cycle time 30 /%.- Built anomaly-detection model on provider claims that lowered over-payment incidence 12 /%.- Mentored four junior analysts in ML best practices, raising team model-delivery speed 25 /%.Tit' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>68719476743</td></tr><tr><td>page_content=' that lowered over-payment incidence 12 /%.- Mentored four junior analysts in ML best practices, raising team model-delivery speed 25 /%.Titanium Health,  Principal Data ScientistLos Angeles, CA, Sep /2019 / \u0013 /Mar /2020- MINA ECG: Built multilevel CNN + BiLSTM + attention model for AF detection, hitting F1 /0.94 / AUROC /0.96 and surfacing beat-, rhythm-, and frequency-level risk cues.- Delivered real-time ECG insights via Streamlit UI on FastAPI micro-services adopted by care teams' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>68719476744</td></tr><tr><td>page_content='within four weeks.- Pneumonia X-ray: Developed & benchmarked four autoencoder variants, lowering reconstruction error 18 /% and generating visual heatmaps for radiologists.- Instituted Git-based peer reviews and ML test harness, doubling pipeline reliability and halving onboarding time.Agilon Health, Senior Data ScientistLong Beach, CA, Nov /2018 / \u0013 /Jul /2019-  Built Spark/Databricks feature-engineering flows and PCA models forecasting chronic-disease onset, guiding 10 /% shift in outreach spend.- Implemented regression-based cost forecasting (R /0' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>77309411328</td></tr><tr><td>page_content='A models forecasting chronic-disease onset, guiding 10 /% shift in outreach spend.- Implemented regression-based cost forecasting (R /0.71) that informed payer budgeting cycles.- Created Tableau dashboards delivering cohort insights to executives, accelerating decision cycles 40 /%.- Co-authored HIPAA-aligned clinical-prediction API schema adopted by downstream engineering teams.Blue Shield of California, Senior Healthcare Data AnalystSan Francisco, CA, May /2015 / \u0013 /Nov /2018- Created ICD-10 extraction and provider-risk models improving high-cost-provider prediction 22 /%.-' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>77309411329</td></tr><tr><td>page_content='2015 / \u0013 /Nov /2018- Created ICD-10 extraction and provider-risk models improving high-cost-provider prediction 22 /%.- Managed Power BI dashboards stratifying provider risk across 70 /+ service lines for actuarial & clinical ops.- Reduced model training time 30 /% via autoencoder-based dimensionality reduction on high-cardinality data.- Partnered with actuarial and operations teams to convert model findings into KPIs, cutting network leakage 8 /%.' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>77309411330</td></tr><tr><td>page_content='PARISA SAHRAEIAN\n",
       "Data Scientist\n",
       "@ sahraeianparisa@gmail.com ap-arkerLos Angeles, CA, USA /linkedinparisa-sahraeian /githubParisaSahraeianiheart  Google Scholar\n",
       "PROFESSIONAL SUMMARY\n",
       "Data Scientist with 4 years of experience in optimization & operations research, ma-\n",
       "chine learning, statistical modeling, AI, delivering end-to-end solutions across adver-\n",
       "tising, digital platforms, and operations. Procient in Python, SQL, R, and' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>77309411331</td></tr><tr><td>page_content='-to-end solutions across adver-\n",
       "tising, digital platforms, and operations. Procient in Python, SQL, R, and cloud plat-\n",
       "forms like AWS and Azure. Skilled at turning complex business problems into data-\n",
       "driven strategies that drive measurable impact and improve product performance.\n",
       "EXPERIENCE\n",
       "Data Scientist\n",
       "iHeartMedia Inc.\n",
       "5Jan 2024Dec 2024 ap-arkerRemote, USA\n",
       " Optimized music scheduling algorithms using AWS, Azure, Python, SQL, and FICO\n",
       "Xpress, improving model objectives by 15% and enhancing' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>77309411332</td></tr><tr><td>page_content=' USA\n",
       " Optimized music scheduling algorithms using AWS, Azure, Python, SQL, and FICO\n",
       "Xpress, improving model objectives by 15% and enhancing user experience.\n",
       " Partnered with revenue management teams using Jira and Conuence to develop\n",
       "solutions that improved digital content delivery eciency.\n",
       " Employed data science techniques, A/B test, and statistical analysis to predict\n",
       "trends in music scheduling for innovation in digital content delivery.\n",
       "Associate Data Scientist\n",
       "iHeartMedia Inc.\n",
       "5May 2022Apr 2023 ap-' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>77309411333</td></tr><tr><td>page_content=' innovation in digital content delivery.\n",
       "Associate Data Scientist\n",
       "iHeartMedia Inc.\n",
       "5May 2022Apr 2023 ap-arkerSan Antonio, TX, USA\n",
       " Developed pricing models using Python and FICO Xpress, boosting advertisement\n",
       "revenue. Conducted code reviews and rigorous A/B testing to ensure scalability\n",
       "and accuracy.\n",
       " Executed large-scale data analyses, employed advanced statistical techniques and\n",
       "machine learning models, and NLP using Python, R, and SQL, and collaborated\n",
       "with product teams to drive data-informed decision-making.\n",
       "Data Analyst' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>77309411334</td></tr><tr><td>page_content=' learning models, and NLP using Python, R, and SQL, and collaborated\n",
       "with product teams to drive data-informed decision-making.\n",
       "Data Analyst (AI, Marketing & Optimization Advisor)\n",
       "Summit Technology Aliates\n",
       "5Aug 2023Dec 2023 ap-arkerEdmond, OK, USA\n",
       " Built data pipeline and real-time Power BI dashboards, conducted in-depth analy-\n",
       "ses using Python to enhance decision-making with sales and marketing teams.\n",
       " Conducted in-depth analyses using Python, delivering actionable insights that sig' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>77309411335</td></tr><tr><td>page_content='\n",
       "ses using Python to enhance decision-making with sales and marketing teams.\n",
       " Conducted in-depth analyses using Python, delivering actionable insights that sig-\n",
       "nicantly improved advertising strategies and campaign performance.\n",
       " Utilized AI, NLP, ZoomInfo, and data-driven marketing strategies that boosted\n",
       "campaign eectiveness and revenue, translating technical insights into business\n",
       "recommendations that boosted campaign eectiveness and revenue.\n",
       "Data Scientist\n",
       "Shack Plumbing\n",
       "5May 2023Aug 2023/Feb 2025Jul 2025 ' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>77309411336</td></tr><tr><td>page_content='iveness and revenue.\n",
       "Data Scientist\n",
       "Shack Plumbing\n",
       "5May 2023Aug 2023/Feb 2025Jul 2025 ap-arkerLos Angeles, CA, USA\n",
       " Developed pricing strategies using optimization and statistical models to support\n",
       "real-time decisions and guide revenue impact.\n",
       "Data Analyst Intern\n",
       "Center for Health Systems Innovation\n",
       "5Jan 2022May 2022 ap-arkerStillwater, OK, USA\n",
       " Analyzed opioid patient data and built predictive models with physicians to un-\n",
       "cover treatment patterns and improve outcomes.\n",
       "Financial Analyst Intern\n",
       "' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>85899345920</td></tr><tr><td>page_content=' OK, USA\n",
       " Analyzed opioid patient data and built predictive models with physicians to un-\n",
       "cover treatment patterns and improve outcomes.\n",
       "Financial Analyst Intern\n",
       "Henneberry Properties\n",
       "5Aug 2021Aug 2022 ap-arkerOK, USA\n",
       " Responsible for accounts payable and nancial data, using QuickBooks and Yardi.\n",
       "SKILLS\n",
       "Python R SQL Gurobi\n",
       "FICO Xpress CPLEX AWS\n",
       "Github Tableau Power BI\n",
       "Pytorch LangChain\n",
       "MATLAB BigQuery GCP\n",
       "SAS Snowake Azure\n",
       "' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>85899345921</td></tr><tr><td>page_content='X AWS\n",
       "Github Tableau Power BI\n",
       "Pytorch LangChain\n",
       "MATLAB BigQuery GCP\n",
       "SAS Snowake Azure\n",
       "NLP C++ ZoomInfo\n",
       "AREA OF EXPERTISE\n",
       "Optimization/OR     \n",
       "Data Science     \n",
       "ML, AI & LLM     \n",
       "EDUCATION\n",
       "MBA\n",
       "California State University-LA\n",
       "5Aug 2025-present\n",
       "Designing and Building AI\n",
       "Products and Services\n",
       "program\n",
       "MIT\n",
       "' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>85899345922</td></tr><tr><td>page_content='MBA\n",
       "California State University-LA\n",
       "5Aug 2025-present\n",
       "Designing and Building AI\n",
       "Products and Services\n",
       "program\n",
       "MIT\n",
       "5May 2025-Jul 2025\n",
       "M.Sc. Industrial Engineering\n",
       "Oklahoma State University, GPA: 4\n",
       "5Aug 2019-Jul 2022\n",
       "B.Sc. Industrial Engineering\n",
       "Sharif University of Tech\n",
       "5Sep 2014-Jan 2019\n",
       "Diploma in Physics and Math\n",
       "NODET\n",
       "PUBLICATIONS\n",
       " On atomic cliques in temporal\n",
       "graphs. Y Lu, Z Miao, P Sahraeian,\n",
       "B' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>85899345923</td></tr><tr><td>page_content='ET\n",
       "PUBLICATIONS\n",
       " On atomic cliques in temporal\n",
       "graphs. Y Lu, Z Miao, P Sahraeian,\n",
       "B Balasundaram. Optimization\n",
       "Letters 17 (4), 813-828.\n",
       " Predicting friction capacity of\n",
       "driven piles using new\n",
       "combinations of neural networks\n",
       "and metaheuristic optimization\n",
       "algorithms. L Jie, P Sahraeian, KI\n",
       "Zykova, M Mirahmadi, ML Nehdi.\n",
       "Case Studies in Construction\n",
       "Materials 19, e02464.' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>85899345924</td></tr><tr><td>page_content='Graduate Research Assistant\n",
       "Oklahoma State University\n",
       "5Jan 2021July 2022 ap-arkerStillwater, OK, USA\n",
       " Developed optimization models scheduling air trac controllers at FAA.\n",
       " Applied the Atomic Clique network optimization model to analyze comorbidity\n",
       "progression in patients, leveraging Gurobi & Python for computational eciency.\n",
       " Utilized analytical skills and processed healthcare data, aiding medical decisions,\n",
       "built optimization and ML models to improve chronic pain insights.\n",
       "Graduate Teaching Assistant\n",
       "Oklahoma State University\n",
       "5' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>85899345925</td></tr><tr><td>page_content=' data, aiding medical decisions,\n",
       "built optimization and ML models to improve chronic pain insights.\n",
       "Graduate Teaching Assistant\n",
       "Oklahoma State University\n",
       "5Aug 2019Dec 2020 ap-arkerStillwater, OK, USA\n",
       " Instructor of Introductory Computer Programming, Python, VBA, and Excel.\n",
       "CERTIFICATES\n",
       " Generative AI for digital Marketers\n",
       " Articial Intelligence AI Marketing to Grow your Business\n",
       " Marketing Automation: Automate Business and Grow Sales\n",
       " Ultimate AWS Certied Cloud Practitioner\n",
       " Python Data analysis' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>85899345926</td></tr><tr><td>page_content=' Business\n",
       " Marketing Automation: Automate Business and Grow Sales\n",
       " Ultimate AWS Certied Cloud Practitioner\n",
       " Python Data analysis & visualization Masterclass\n",
       " Python for Machine Learning & Data Science Masterclass\n",
       " Machine Learning A-Z: Python & R in Data Science\n",
       " Articial Intelligence Foundations: Neural Networks\n",
       " Articial Intelligence Foundations: Machine Learning\n",
       " Power BI Essential Training\n",
       " Project Online Reporting with Power BI\n",
       " Tableau 10 for Data Scientists\n",
       " Parisa Sahraeian. Comprehensive\n",
       "Shift Scheduling: Methodologies' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>85899345927</td></tr><tr><td>page_content=' Project Online Reporting with Power BI\n",
       " Tableau 10 for Data Scientists\n",
       " Parisa Sahraeian. Comprehensive\n",
       "Shift Scheduling: Methodologies\n",
       "and Applications Across Sectors.\n",
       "Eliva Press, 2024.\n",
       "PHD LEVEL COURSEWORK\n",
       " Machine Learning\n",
       " Linear Optimization\n",
       " Nonlinear Optimization\n",
       " Combinatorial Optimization\n",
       " Network Optimization\n",
       " Optimization Under Uncertainty\n",
       " Applied Statistical analyses in R\n",
       " Decision Theory\n",
       " Advanced Operations Research\n",
       "HONORS AND AWARDS\n",
       " Won the 3rd place in Hackathon\n",
       "2022, iHeart Media' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>85899345928</td></tr><tr><td>page_content=' Decision Theory\n",
       " Advanced Operations Research\n",
       "HONORS AND AWARDS\n",
       " Won the 3rd place in Hackathon\n",
       "2022, iHeart Media Co., US, Jul\n",
       "2022.\n",
       " Ms. International Runner-Up in\n",
       "2019 Ms. and Mr. International\n",
       "Pageant, Oklahoma State\n",
       "University, OK, US., Dec 2019.' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>94489280512</td></tr><tr><td>page_content='Venkatesh Terikuti  \n",
       "Senior Data Scientist | Analytics Leader | Applied AI Researcher \n",
       "vterikuti@gmail.com | +1 (402) 6302823 | linkedin.com/in/terikuti | venkateshterikuti.github.io/ \n",
       " \n",
       "Professional Summary \n",
       "Senior Data Scientist and ML Platform Engineer with 6+ years of experience designing and deploying scalable machine learning and \n",
       "analytics systems across cloud and enterprise environments. Specialized in building robust ML pipelines for large -scale data \n",
       "processing, anomaly detection, and predictive analytics. Hands-' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>94489280513</td></tr><tr><td>page_content=' systems across cloud and enterprise environments. Specialized in building robust ML pipelines for large -scale data \n",
       "processing, anomaly detection, and predictive analytics. Hands-on with modern MLOps, distributed data architectures, and secure data \n",
       "management. Proven ability to drive product impact by bridging ML research, infrastructure, and cross -functional teams. \n",
       " \n",
       "EXPERIENCE  \n",
       "ML Research Assistant (Post Grad research) Feb 2025  Present \n",
       "University of Arizona Tucson, AZ \n",
       " Built a scalable semantic search engine for e-commerce by fine-tuning Sentence Transformers and Google FLAN-T5' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>94489280514</td></tr><tr><td>page_content=' Arizona Tucson, AZ \n",
       " Built a scalable semantic search engine for e-commerce by fine-tuning Sentence Transformers and Google FLAN-T5 on a 1.3M-\n",
       "product catalog, integrating FAISS for fast vector retrieval and enabling real -time, intent-aware search. \n",
       " Implemented a full-stack ML pipeline covering data prep, model fine-tuning, and dynamic filter extraction (e.g., price, ratings), \n",
       "enhancing structured query refinement and delivering a 90% increase in Precision@10 over traditional keyword search.  \n",
       " Led applied research on LLM' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>94489280515</td></tr><tr><td>page_content=' \n",
       "enhancing structured query refinement and delivering a 90% increase in Precision@10 over traditional keyword search.  \n",
       " Led applied research on LLM-powered search, authored a peer-reviewed paper titled \"LLM-based Semantic Search for \n",
       "Conversational Queries in E-commerce\" submitted to CIKM'25 (ACM), showcasing innovative use of Sentence -BERT for user \n",
       "query understanding and product discovery for public sector use cases.  \n",
       " \n",
       "Senior Software Engineer  Data Scientist                  ' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>94489280516</td></tr><tr><td>page_content=' use cases.  \n",
       " \n",
       "Senior Software Engineer  Data Scientist                                                                                                             Nov 2021  Jan 2023' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>94489280517</td></tr><tr><td>page_content='                           Nov 2021  Jan 2023 \n",
       "GlobalLogic                                                                                           ' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>94489280518</td></tr><tr><td>page_content='                                                                                                                            Remote  \n",
       "' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>94489280519</td></tr><tr><td>page_content='                            Remote  \n",
       " Built deep learning pipelines using TensorFlow and attention-based LSTMs to forecast demand for 100K+ SKUs with 96% \n",
       "accuracy; integrated models via CI/CD using Jenkins and GitLab CI.  \n",
       " Designed and deployed a centralized MLOps platform on AWS SageMaker, serving batch and real -time features for 10+ models; \n",
       "reduced model deployment latency by 40%.  \n",
       " Engineered ETL workflows in MySQL' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>94489280520</td></tr><tr><td>page_content=' and real -time features for 10+ models; \n",
       "reduced model deployment latency by 40%.  \n",
       " Engineered ETL workflows in MySQL, Hive, and Airflow; utilized Databricks for scalable data processing and collaborative \n",
       "analytics. Developed a real-time anomaly detection pipeline (Python, Kafka) to identify drops in order placement and app \n",
       "responsiveness, enabling proactive resolution of customer-impacting issues.  \n",
       " Partnered with marketing and supply chain stakeholders to deliver insights and dashboards supporting product inventory and \n",
       "campaign effectiveness. \n",
       " Conducted A/B test evaluation' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>103079215104</td></tr><tr><td>page_content='nered with marketing and supply chain stakeholders to deliver insights and dashboards supporting product inventory and \n",
       "campaign effectiveness. \n",
       " Conducted A/B test evaluation and user segmentation to support growth experimentation strategies; delivered insights used to \n",
       "prioritize product roadmap items and optimize user journeys. Engaged directly with cross -functional teams in Agile/Scrum \n",
       "environments; influenced technical decisions and led process improvements.  \n",
       " \n",
       "Senior Data Analyst Nov 2019  Oct 2021 \n",
       "Geetham Software (Client: Lytx ) Remote \n",
       " Led a distributed analytics and DevOps team (US, Argentina' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>103079215105</td></tr><tr><td>page_content=' 2019  Oct 2021 \n",
       "Geetham Software (Client: Lytx ) Remote \n",
       " Led a distributed analytics and DevOps team (US, Argentina, Poland) to deliver big data solutions supporting millions of \n",
       "telematics events daily. \n",
       " Built and managed SQL-based and Databricks Delta Lake pipelines for ingesting 6M+ daily events, ensuring 99.9% data freshness \n",
       "for real-time analytics, cleaning, and modeling large-scale event and customer data. \n",
       " Established dbt Core semantic models and reusable data marts for analytics self -service; drove advanced cohort' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>103079215106</td></tr><tr><td>page_content=' large-scale event and customer data. \n",
       " Established dbt Core semantic models and reusable data marts for analytics self -service; drove advanced cohort and A/B testing \n",
       "analytics. \n",
       " Designed and deployed automated workflows in AWS Glue and Airflow for campaign, CRM, and compliance datasets; \n",
       "implemented Tableau dashboards, reducing report turnaround by 40%. \n",
       " Provided production troubleshooting, performance tuning, and issue resolution across global deployments. Partnered with produ ct \n",
       "to design and analyze 10+ A/B tests (e.g., promotional offers),' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>103079215107</td></tr><tr><td>page_content=' issue resolution across global deployments. Partnered with produ ct \n",
       "to design and analyze 10+ A/B tests (e.g., promotional offers), delivering actionable insights that improved conversion by 8% .. \n",
       " Architected and implemented dimensional data models (star and snowflake schemas) in Snowflake and Databricks, supporting \n",
       "scalable, performant analytics and ad hoc reporting across sales, operations, and compliance functions.  \n",
       " \n",
       "Data Analyst  Mar 2016  Oct 2019 \n",
       "Geetham Software             Chennai  \n",
       "' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>103079215108</td></tr><tr><td>page_content='\n",
       "Data Analyst  Mar 2016  Oct 2019 \n",
       "Geetham Software             Chennai  \n",
       " Developed ETL workflows for campaign, finance, and CRM data across AWS and Azure, automating ingestion and cleaning \n",
       "tasks. \n",
       " Conducted advanced segmentation, churn analysis, and predictive modeling using Python, SQL, and Tableau.  \n",
       " Maintained and improved daily reporting pipelines, standardized data for consistent BI consumption, and supported quarterly \n",
       "business reviews. \n",
       " Acted as key point of contact for resolving pipeline issues' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>103079215109</td></tr><tr><td>page_content=' reporting pipelines, standardized data for consistent BI consumption, and supported quarterly \n",
       "business reviews. \n",
       " Acted as key point of contact for resolving pipeline issues and ensuring high data quality standards.   \n",
       " Led analytics delivery in compliance-driven environments, establishing data governance protocols, ensuring data quality, and' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>103079215110</td></tr><tr><td>page_content='partnering with legal and finance teams to enable secure, audit-ready reporting for regulatory and operational reviews. \n",
       " \n",
       " \n",
       "EDUCATION  \n",
       "Master of Science in Data Science, The University of Arizona (GPA: 3.89) Dec 2024 \n",
       "Bachelor of Technology in Mechanical Engineering, Acharya Nagarjuna University (GPA: 3.42) May 2015 \n",
       " \n",
       "PROJECTS \n",
       "Building LLM From Scratch.  \n",
       " Implemented GPT-style large language model from ground up using PyTorch with byte-level tokenizer and scaled' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>103079215111</td></tr><tr><td>page_content=' Scratch.  \n",
       " Implemented GPT-style large language model from ground up using PyTorch with byte-level tokenizer and scaled dot-product \n",
       "attention. Built complete training pipelines including pretraining on unlabeled text, classification finetuning, and instruction \n",
       "finetuning (SFT). Demonstrated deep understanding of attention mechanisms, gradient stabilization, and modern LLM \n",
       "architectures. \n",
       " \n",
       "Transformer From Scratch: Scaling Journey.  \n",
       " Built complete transformer implementation scaling from 2.4M to 52M parameters with  explicit eins' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>103079215112</td></tr><tr><td>page_content='former From Scratch: Scaling Journey.  \n",
       " Built complete transformer implementation scaling from 2.4M to 52M parameters with  explicit einsum operations. Compared \n",
       "MacBook character-level models vs H100 GPU BPE models, analyzing hardware efficiency and training dynamics. Implemented \n",
       "production infrastructure with mixed precision training, memory mapping, and automated benchmarking . \n",
       " \n",
       "High-Performance Video Diffusion Model Optimization  Wan 2.2 .  \n",
       " Optimized the Wan 2.2 text-to-video diffusion model pipeline for H100 GPUs, leveraging advanced context -parallel' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>111669149696</td></tr><tr><td>page_content='2 .  \n",
       " Optimized the Wan 2.2 text-to-video diffusion model pipeline for H100 GPUs, leveraging advanced context -parallelism, flash \n",
       "attention 3, and direct cuBLAS/triton kernel integration to achieve sub -25s 720p video generation for 81 framesoutperforming \n",
       "standard FA2/FA3 baselines (1530 TFLOPS, 24 speedup). \n",
       " \n",
       "Predicting Customer Churn in E-Commerce  \n",
       " Created churn prediction models using Random Forest, SVM, and XGBoost with' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>111669149697</td></tr><tr><td>page_content='\n",
       "Predicting Customer Churn in E-Commerce  \n",
       " Created churn prediction models using Random Forest, SVM, and XGBoost with >90% accuracy on the 'Online Retail II' dataset \n",
       "(500K+ records).  \n",
       " Conducted retention cohort analysis and lifetime value estimation; tuned model performance using cross -validation and grid \n",
       "search. Delivered Tableau dashboards and actionable insights to simulate real -world e-commerce churn scenarios and KPIs. \n",
       " \n",
       "Enterprise Knowledge Copilot: Agentic RAG for Internal Q&A & SOP Automation' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>111669149698</td></tr><tr><td>page_content='-commerce churn scenarios and KPIs. \n",
       " \n",
       "Enterprise Knowledge Copilot: Agentic RAG for Internal Q&A & SOP Automation  .  \n",
       " Developed an enterprise-grade RAG platform for knowledge workers to ask natural language questions against internal \n",
       "documentation (SOPs, runbooks, tech specs, policy PDFs).  \n",
       " Leveraged LangChain with pgVector and Redis to embed, retrieve, and rerank context chunks; OpenAI GPT -4 for synthesis and \n",
       "summarization. Designed multi-agent orchestration for use cases like IT support' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>111669149699</td></tr><tr><td>page_content=' rerank context chunks; OpenAI GPT -4 for synthesis and \n",
       "summarization. Designed multi-agent orchestration for use cases like IT support, onboarding, compliance checks, and change \n",
       "management documentation. \n",
       " \n",
       "Skills \n",
       "Languages: Python, SQL, Bash, C++ \n",
       "Big Data & Cloud: Spark, PySpark, Databricks, Airflow, AWS (Lambda, Glue, S3, EC2), Azure (Data Factory, Data Lake), \n",
       "Snowflake, Hadoop, Kubernetes, Delta Lake, dbt Core' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>111669149700</td></tr><tr><td>page_content=', EC2), Azure (Data Factory, Data Lake), \n",
       "Snowflake, Hadoop, Kubernetes, Delta Lake, dbt Core \n",
       "Data Engineering: ETL/ELT Pipelines, Data Modeling, Data Cleaning/Standardization, Data Warehousing  \n",
       "BI & Visualization: Tableau, Power BI, Google Data Studio \n",
       "DevOps: Docker, CI/CD, GitLab CI, Jenkins, Terraform (IaC), Prometheus/Grafana  \n",
       "Data & ML Platforms: SQL, dbt Core, Airflow, Tableau, Looker' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>111669149701</td></tr><tr><td>page_content='IaC), Prometheus/Grafana  \n",
       "Data & ML Platforms: SQL, dbt Core, Airflow, Tableau, Looker (familiar), Power BI, Databricks, Snowflake, AWS (Lambda, S3, \n",
       "Glue, SageMaker), Data Warehousing  \n",
       "Statistical Modeling: Regression, Classification, Clustering, Experimentation/A/B Testing, Causal Inference  \n",
       "Leadership: Agile/Scrum , Cross-functional Team Leadership, Mentorship, Analytics Enablement, Stakeholder Management, Change \n",
       "' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>111669149702</td></tr><tr><td>page_content=' \n",
       "Leadership: Agile/Scrum , Cross-functional Team Leadership, Mentorship, Analytics Enablement, Stakeholder Management, Change \n",
       "Management, OKR Development \n",
       " \n",
       "Certifications \n",
       "Deep Learning Specialization  Andrew Ng, Coursera \n",
       "SQL(Advanced) - Hackerrank' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>111669149703</td></tr><tr><td>page_content='Vivek Reddy \n",
       "562-879-1191 | vreddy704@berkeley.edu | Los Angeles, CA | Linkedin | GitHub \n",
       "SUMMARY \n",
       " \n",
       "Data Engineer / Data Scientist with 6+ years of experience building ML, NLP, and Generative AI solutions across \n",
       "healthcare, insurance, and consumer applications. Skilled in customer-facing AI systems, LLMs, intent classification, \n",
       "and recommendation pipelines. Proficient in Python, SQL, AWS (SageMaker, Bedrock, Lambda, ECS) with strong \n",
       "experience in' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>111669149704</td></tr><tr><td>page_content=' recommendation pipelines. Proficient in Python, SQL, AWS (SageMaker, Bedrock, Lambda, ECS) with strong \n",
       "experience in scaling models to production. Adept at translating complex data into automated, customer-centric AI \n",
       "tools that improve service efficiency and user experience. \n",
       "EDUCATION \n",
       " \n",
       "University of California, Berkeley                                                     ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>120259084288</td></tr><tr><td>page_content='                                                                                                           August 2024 \n",
       "Master of Information and Data Science \n",
       " \n",
       "University of California, Los Angeles (U' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>120259084289</td></tr><tr><td>page_content='           August 2024 \n",
       "Master of Information and Data Science \n",
       " \n",
       "University of California, Los Angeles (UCLA)                                                                                              ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>120259084290</td></tr><tr><td>page_content='                                        December 2016 \n",
       "B.S., Computational and Systems Biology \n",
       "  PROFESSIONAL EXPERIENCE \n",
       " \n",
       "Data Engineer (Infrastructure and Analytics) | Venture Connect                                                   ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>120259084291</td></tr><tr><td>page_content='                                                 July 2025 - Present \n",
       " Codeveloped GitHub Actions workflows for continuous integration and delivery of data pipelines, \n",
       "enabling reliable deployment of new features. \n",
       " Contributed to the development of AWS Lambda functions for making updates to data tables on AWS \n",
       "RDS.   \n",
       " Authored and executed unit tests to validate database update logic, strengthening overall system ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>120259084292</td></tr><tr><td>page_content=' updates to data tables on AWS \n",
       "RDS.   \n",
       " Authored and executed unit tests to validate database update logic, strengthening overall system \n",
       "robustness. \n",
       " \n",
       "Data Engineer | Enterprise LLC                                                                                  ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>120259084293</td></tr><tr><td>page_content='                                                        August 2024  February 2025 \n",
       " Automated ingestion and orchestration of API data into ETL pipelines, enabling scheduled updates. \n",
       " Enhanced a social data scoring algorithm by integrating California AirNow API data, delivering weekly \n",
       "refreshed environmental metrics. \n",
       " \n",
       "Data Scientist 2 | MOTER Technologies         ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>120259084294</td></tr><tr><td>page_content=' data, delivering weekly \n",
       "refreshed environmental metrics. \n",
       " \n",
       "Data Scientist 2 | MOTER Technologies                                                                                          April 2023  November 2023 \n",
       " Developed pipeline for extracting road' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>120259084295</td></tr><tr><td>page_content='                 April 2023  November 2023 \n",
       " Developed pipeline for extracting road type information from vehicular GPS points to enrich model training data \n",
       "utilizing Docker, Open Street Map, and PostgreSQL. \n",
       " Contributed to development of a pipeline leveraging Featuretools library to generate and test many features against \n",
       "important statistical and insurance metrics during iterative model retraining.    \n",
       " Developed automated SQL-based ETL pipelines with data validation and monitoring dashboards in \n",
       "Amazon Quicksight and Power BI. \n",
       "' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>120259084296</td></tr><tr><td>page_content=' \n",
       " Developed automated SQL-based ETL pipelines with data validation and monitoring dashboards in \n",
       "Amazon Quicksight and Power BI. \n",
       "Data Scientist 1 | MOTER Technologies                                                                                         ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>128849018880</td></tr><tr><td>page_content='                                          May 2022  April 2023 \n",
       " Developed predictive models using R, SQL, and Python that used onboard vehicle telemetrics data to develop insurance \n",
       "products for vehicle fleets   \n",
       " Worked on Python pipelines for automated data ingestion, preprocessing, model fitting, and outputting insurance KPIs.  \n",
       " Contributed to development of a model and a scoring method that was successfully filed and approved for' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>128849018881</td></tr><tr><td>page_content=' model fitting, and outputting insurance KPIs.  \n",
       " Contributed to development of a model and a scoring method that was successfully filed and approved for use in Indiana.  \n",
       " Actively collaborated in a customer-focused, cross-functional environment to drive projects to fast completion. \n",
       " \n",
       "Associate Scientist II, Computational Biology | Neogenomics Laboratories                                    December 2020 - April 2022 \n",
       " Utilized python to implement unsuper' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>128849018882</td></tr><tr><td>page_content='                 December 2020 - April 2022 \n",
       " Utilized python to implement unsupervised deep learning algorithms to identify unique cellular morphologies. \n",
       " Presented results and published conference paper at 2021 American Association of Cancer Research \n",
       "conference  https://cancerres.aacrjournals.org/content/81/13_Supplement/154 \n",
       " \n",
       "Associate Scientist I, Computational Biology | Neogenomics Laboratories                         ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>128849018883</td></tr><tr><td>page_content='ational Biology | Neogenomics Laboratories                                  October 2018 - December 2020 \n",
       " Utilized Keras-Tensorflow to perform deep learning applications including cell classification, object detection, and \n",
       "tissue segmentation from cancerous tissue images.  \n",
       " Generated and analyzed statistical reports for global pharmaceutical firms and business clients. \n",
       " Created and presented data visualizations using Python libraries such as seaborn and matplotlib to deliver \n",
       "actionable insights' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>128849018884</td></tr><tr><td>page_content=' and business clients. \n",
       " Created and presented data visualizations using Python libraries such as seaborn and matplotlib to deliver \n",
       "actionable insights to stakeholders.  \n",
       "PROJECTS \n",
       " \n",
       "Assistive Vision Mobile App                                                                            ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>128849018885</td></tr><tr><td>page_content='                                                                                                August 2025 \n",
       " Trained a custom YOLOv8 object detection model on 1000+ self-collected images and 5,000+ OpenImages samples' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>128849018886</td></tr><tr><td>page_content=' 2025 \n",
       " Trained a custom YOLOv8 object detection model on 1000+ self-collected images and 5,000+ OpenImages samples, \n",
       "achieving real-time, high-precision detection across 20+ grocery item classes.' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}</td><td>128849018887</td></tr><tr><td>page_content=' Built an AI-powered iOS app that performs on-device detection and selectively routes high-confidence frames to a \n",
       "serverless AWS pipeline (S3  Lambda  Bedrock) for LLM-based captioning and live voice narration. \n",
       " Engineered a low-latency edge-cloud architecture combining CoreML, Vision, and AWS Bedrock, reducing cloud calls \n",
       "by 80% and enabling scalable assistive vision for visually impaired users with <3s end-to-end response time. \n",
       "Airbnb Housing Recommendation Chatbot (RAG)          ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>128849018888</td></tr><tr><td>page_content='3s end-to-end response time. \n",
       "Airbnb Housing Recommendation Chatbot (RAG)                                                                                                     August 2024  \n",
       "' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>128849018889</td></tr><tr><td>page_content='                           August 2024  \n",
       " Built a retrieval-augmented generation chatbot with Llama3.1 on AWS Bedrock, FAISS vector store on LangChain \n",
       "for efficient NLP-based retrieval. \n",
       " Optimized ranking and filtering for high-relevance recommendations. \n",
       "Natural Language to SQL Query Translation       April 2024 \n",
       " Finetuned a T5 sequence-to-sequence model to translate natural language questions to SQL given a database ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>128849018890</td></tr><tr><td>page_content='     April 2024 \n",
       " Finetuned a T5 sequence-to-sequence model to translate natural language questions to SQL given a database \n",
       "schema. \n",
       " Achieved an exact string match of 80% and a ROUGE score of 90 using a large cross-domain dataset of over 100 \n",
       "domains utilizing data augmentation, transfer learning, and curriculum learning techniques. \n",
       " \n",
       "SKILLS & TOOLS \n",
       " \n",
       " Languages: Python, R, SQL, Javascript \n",
       " Machine Learning: Clustering, Logistic Regression, SVM, Random' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>128849018891</td></tr><tr><td>page_content='\n",
       " \n",
       " Languages: Python, R, SQL, Javascript \n",
       " Machine Learning: Clustering, Logistic Regression, SVM, Random Forest, Neural Networks, PCA, Entity Matching, YOLOv8, T5, LLM fine-tuning, RAG pipelines \n",
       " Analytics & Modeling: Experimentation design, Metrics Design, EDA, Hypothesis Testing, A/B Testing, \n",
       "Regression, Feature Engineering \n",
       " Visualization: Tableau, Quicksight, Power BI, Seaborn, ggplot2 \n",
       " Cloud &' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>128849018892</td></tr><tr><td>page_content=', Feature Engineering \n",
       " Visualization: Tableau, Quicksight, Power BI, Seaborn, ggplot2 \n",
       " Cloud & Infrastructure: AWS (CDK, S3, Athena, SageMaker, Bedrock, ECS, Glue), Docker, FastAPI, \n",
       "CI/CD, version control \n",
       " ML Frameworks: Scikit-learn, XGBoost, PyTorch, Keras, LangChain \n",
       " Data Ops: Docker, Airflow, Spark, ETL design, MLOps, Featuretools' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}</td><td>128849018893</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "page_content='Apoorva Tyagi                                   \n \n         Milpitas, CA | (669) 264 9455 | apoorva.tyagi0195@gmail.com| http://linkedin.com/in/apoorva-tyagi19 \n \n \n \nEDUCATION  \n \nCalifornia State University, East Bay, California, USA         ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         0
        ],
        [
         "page_content='\n \n \nEDUCATION  \n \nCalifornia State University, East Bay, California, USA                                                                                                         ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         1
        ],
        [
         "page_content='                                          Aug 2022  May 2024 \nMaster of Science in Data/Business Analytics                                                                        ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         2
        ],
        [
         "page_content='                                                                                                      \nBirla Institute of Management and Technology (BIMTECH)            ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         3
        ],
        [
         "page_content='      \nBirla Institute of Management and Technology (BIMTECH)                                                                                                        Aug 2017  Mar 2019' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         4
        ],
        [
         "page_content='                            Aug 2017  Mar 2019 \n \n  \n \nMaster of Business Administration (MBA)                                                                               ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         5
        ],
        [
         "page_content='                                                                                                                                ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         6
        ],
        [
         "page_content='                                                                       \nUniversity of Delhi                                                     ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         7
        ],
        [
         "page_content='                                                                                                                                ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         8
        ],
        [
         "page_content='                                                             Jul 2013  May 2016 \nBachelors in Finance                                                        ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         8589934592
        ],
        [
         "page_content='                                                                                                                                ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         8589934593
        ],
        [
         "page_content='                                                       \n \n \n \nWORK EXPERIENCE \n \nAssociate Data Scientist, Thermo Fisher Scientific, California, USA                                             ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         8589934594
        ],
        [
         "page_content='                                                                    Oct 2024  Jun 2025 \nSkills: Python, SQL, Databricks, Snowflake, Azure DevOps, Machine Learning Models, ETL, Data Analysis, GIT, PySpark, LLM, NLP \n Developed and validated predictive models on high-dimensional clinical' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         8589934595
        ],
        [
         "page_content=' ETL, Data Analysis, GIT, PySpark, LLM, NLP \n Developed and validated predictive models on high-dimensional clinical trial datasets through exploratory data analysis (EDA), achieving \na 36% performance improvement with increased predictive stability and accuracy. \n Refactored pandas -based feature engineering pipeline to PySpark to efficiently handle millions of clinical site data rows , enabling \nscalable analysis and creation of feature tables for downstream modeling in Unity Catalog.  \n Designed and implemented data drift detection and model performance monitoring using Evidently and MLflow, improving ML pipeline' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         8589934596
        ],
        [
         "page_content=' tables for downstream modeling in Unity Catalog.  \n Designed and implemented data drift detection and model performance monitoring using Evidently and MLflow, improving ML pipeline \nreliability and automating compliance checks. \n Automated model card generation using Azure OpenAI and GPT -based NLP models, applying structured text summarization to \nstreamline documentation workflows and reduce manual effort. \n Collaborating with business stakeholders to understand requirements, present model performance metrics, and incorporate their \nfeedback into feature enhancements and model refinements.   \n \nProduct & Data Analytics Intern, GEICO (' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         8589934597
        ],
        [
         "page_content=' metrics, and incorporate their \nfeedback into feature enhancements and model refinements.   \n \nProduct & Data Analytics Intern, GEICO (Berkshire Hathaway subsidiary), Maryland, USA                                        Jun 2023  Aug 2023 \nSkills: SQL, SAS, R, Python, PowerBI, Excel, Data Analysis, Agile \n \n Enhanced insurance product rating models by optimizing segmentation and valuation strategies' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         8589934598
        ],
        [
         "page_content=', R, Python, PowerBI, Excel, Data Analysis, Agile \n \n Enhanced insurance product rating models by optimizing segmentation and valuation strategies, increasing market competitivene ss \nand profitability. \n Applied R to uncover patterns and statistical insights in large datasets, supporting strategic decision -making across product lines. \n Extracted, cleaned, and analyzed data from complex databases using SQL and SAS, ensuring accuracy and reliability in reportin g. \n Developed interactive dashboards in Power BI using DAX to visualize trends and improve data-driven communication with stakeholders. \n Adapted' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         8589934599
        ],
        [
         "page_content='. \n Developed interactive dashboards in Power BI using DAX to visualize trends and improve data-driven communication with stakeholders. \n Adapted to Agile methodologies, actively participating in daily stand -ups, and consistently achieving bi-weekly sprint goals. \n \nData Analyst (Associate Manager), Mahindra Insurance Brokers                                                        ' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         8589934600
        ],
        [
         "page_content='                                                            May 2019  Oct 2021 \nSkills: SQL, Tableau, Excel, Data Analysis, ETL, Data Mapping, KPI Reporting, Predictive Analytics \n Prepared and analyzed data by writing SQL queries, created and updated over 50 data tables in the database to store various data units, \nand used them as' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         17179869184
        ],
        [
         "page_content=' Prepared and analyzed data by writing SQL queries, created and updated over 50 data tables in the database to store various data units, \nand used them as sources for dashboards.  \n Created Tableau dashboards to dynamically represent critical business metrics, enhancing data -centric strategic decisions across the \norganization, identifying opportunities for business growth and revenue enhancement, resulting in a 15% increase in revenue.  \n Coordinated cross -functional teams to develop and execute data -driven strategies aimed at increasing customer acquisition and \nretention by 35%.  \n Consolidated data from multiple sources via ETL' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         17179869185
        ],
        [
         "page_content=' and execute data -driven strategies aimed at increasing customer acquisition and \nretention by 35%.  \n Consolidated data from multiple sources via ETL processes to identify patterns and streamline operations, improving data acce ssibility \nand efficiency.  \n Proactively did trend analysis and pursued new business opportunities through market research, and lead generation. Creation of sales \ndashboards through querying database and collection of relevant information, including marketing analysis, clients prefer ences, \nbudget, and timeline.  \n \nPROJECTS  \n \n    Multi' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         17179869186
        ],
        [
         "page_content='s prefer ences, \nbudget, and timeline.  \n \nPROJECTS  \n \n    Multi-Document Retrieval Chatbot [Python, Visual Studio, LangChain, OpenAI API, Embeddings, Vector stores] \n Developed a multi -document retrieval and chatbot system using LangChain and ChatGPT, enabling efficient extraction and \nsummarization of information from diverse text sources.  Integrated document parsing, vector embeddings, and conversational AI to \nenhance user interactions, providing accurate and context-aware responses in a' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         17179869187
        ],
        [
         "page_content='.  Integrated document parsing, vector embeddings, and conversational AI to \nenhance user interactions, providing accurate and context-aware responses in a scalable framework.  \nCustomer Churn Prediction Model Development [Python, Logistic Regression, kNN, XG Boost, Data Analysis, Data Modeling] \n Conducted data analysis and data modeling using logistic regression, KNN, and Decision Trees models to predict customer churn \nfor a telecom company.  \n \nTECHNICAL SKILLS \n \nCoursework  Data Analytics, Data Warehousing, Business Intelligence,' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         17179869188
        ],
        [
         "page_content=' telecom company.  \n \nTECHNICAL SKILLS \n \nCoursework  Data Analytics, Data Warehousing, Business Intelligence, Data Visualization, Database Management, Machine Learning \nProgramming Languages  Python, SQL, R \nTools  Databricks, Snowflake, Azure DevOps, SAS, Tableau, MySQL, MS Excel, Power BI, Coginiti, Visual Studio, GIT, Google Analytics' metadata={'producer': 'Microsoft Word 2021', 'creator': 'Microsoft Word 2021', 'creationdate': '2025-06-30T16:05:31-07:00', 'author': 'Windows User', 'moddate': '2025-06-30T16:05:31-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Apoorva Tyagi.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}",
         17179869189
        ],
        [
         "page_content='CHUN MIN JENCurrent Location: San Jose, CA 95129SUMMARY:Innovative Data Scientist and Machine Learning/GenAI Developer with over 7 years of experience delivering end-to-end data-driven solutions across healthcare, finance, automotive, and research industries. Expertise in building scalable ETL pipelines, architecting cloud-native analytics platforms, and deploying AI/ML models using AWS services (SageMaker, Bedrock, Lambda, Glue, QuickSight). Skilled in LLM-powered RAG systems, vector databases, and advanced statistical modeling to generate actionable insights that improve decision' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}",
         17179869190
        ],
        [
         "page_content='ue, QuickSight). Skilled in LLM-powered RAG systems, vector databases, and advanced statistical modeling to generate actionable insights that improve decision-making and operational efficiency. Proven track record in developing interactive dashboards, predictive models, and recommendation systems that drive measurable business outcomes. Adept at leading cross-functional teams, optimizing data pipelines for high accuracy and performance, and spearheading GenAI adoption strategies to reduce costs and accelerate innovation. Holds a PhD in Experimental Nuclear Physics and five AWS certifications, including Machine Learning Specialty and Data Engineer Associate.EDUCATION AND CERTIFICATION:PhD in' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}",
         17179869191
        ],
        [
         "page_content=' in Experimental Nuclear Physics and five AWS certifications, including Machine Learning Specialty and Data Engineer Associate.EDUCATION AND CERTIFICATION:PhD in Experimental Nuclear Physics Syracuse University Syracuse, NY 2013 \"AWS Certified Machine Learning  \u0013 Associate. \"AWS Certified AI Practitioner Early Adopter. \"AWS Certified Data Engineer  \u0013 Associate. \"AWS Certified Machine Learning  \u0013 Specialty. \"AWS Certified Cloud Practitioner. \"TECHNICAL SKILLS:Programming & Data Science: Python, R, SQL, PySpark, TensorFlow, Keras,' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}",
         17179869192
        ],
        [
         "page_content=' \"TECHNICAL SKILLS:Programming & Data Science: Python, R, SQL, PySpark, TensorFlow, Keras, LangChain, Pandas, NumPy, Scikit- \"learn, Matplotlib, Seaborn, NLP, Reinforcement Learning.Cloud & Big Data Platforms: AWS (SageMaker, Bedrock, Lambda, Step Functions, Glue, Athena, EMR, EC2, S3,  \"QuickSight, CloudFormation), Snowflake, Databricks, Apache Spark, HadoopDatabases & Storage: PostgreSQL' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}",
         25769803776
        ],
        [
         "page_content='3,  \"QuickSight, CloudFormation), Snowflake, Databricks, Apache Spark, HadoopDatabases & Storage: PostgreSQL (Pgvector), MySQL, NoSQL, MongoDB, Redshift, Data Lakes, OMOP CDM \"Machine Learning & GenAI: LLMs, RAG systems, Vector Databases, Embedding Models, Recommendation  \"Systems, Predictive Modeling, Deep Learning, Synthetic Data (Synthea)ETL & Data Engineering: ETL Pipeline Design, Data Warehousing, Data Quality Monitoring, Workflow  \"Automation, Serverless' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}",
         25769803777
        ],
        [
         "page_content='Synthea)ETL & Data Engineering: ETL Pipeline Design, Data Warehousing, Data Quality Monitoring, Workflow  \"Automation, Serverless ArchitectureVisualization & BI Tools: AWS QuickSight, Tableau, Power BI, Apache Superset, Dashboard Development,  \"Geospatial AnalyticsSoftware Development: C/C++, MATLAB, Flask/RESTful APIs, Git, Linux, Agile/Scrum \"Statistical & Analytical Tools: Hypothesis Testing, Time Series Analysis, Regression Models, Forecasting, Data  \"Mining, Feature EngineeringPROFESSIONAL' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}",
         25769803778
        ],
        [
         "page_content=' Analytical Tools: Hypothesis Testing, Time Series Analysis, Regression Models, Forecasting, Data  \"Mining, Feature EngineeringPROFESSIONAL EXPERIENCE: THECLINICIAN HOLDINGS LIMITED| San Jose, CA                                                                           Nov 2024  ' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}",
         25769803779
        ],
        [
         "page_content='                             Nov 2024  \u0013 PresentData Scientist & Machine Learning/Gen AI Developer (Full-time Contract)Created PROM-agent (Patient Report Outcome Measured) on AWS bedrock to automate Transformer-based  \"semantic analytics developed on AWS Sagemaker with LLM-based RAG-enriched patient appointment data in TensorFlowBuilt the LangChain integrated framework for the contextual, role-based patient report with text representation  \"techniques for RAG systems embedded in vector' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}",
         25769803780
        ],
        [
         "page_content=' TensorFlowBuilt the LangChain integrated framework for the contextual, role-based patient report with text representation  \"techniques for RAG systems embedded in vector Postgres databases with which the Pgvector is associated as the search engine. Also developing and integrating an MCP server to empower the agentic AI functionality Refined database queries using query optimization techniques, slashing average load times by 42% and  \"demonstrably improving client satisfaction ratings via monthly performance reports.Designed and developed interactive dashboards in AWS QuickSight for patient demographics, encounters, and  \"clinical insights.Built visualizations to analyze hospital performance, surgeon-' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}",
         25769803781
        ],
        [
         "page_content=' and developed interactive dashboards in AWS QuickSight for patient demographics, encounters, and  \"clinical insights.Built visualizations to analyze hospital performance, surgeon-level surgeries, and encounter trends across  \"multiple facilities.Implemented COVID-19 case tracking dashboards with encounter types, procedures, and treatment cost  \"breakdowns.Created financial dashboards to monitor revenue, treatment costs, and engagement rates by department and  \"activity.' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}",
         25769803782
        ],
        [
         "page_content='Leveraged QuickSight for geographical analysis, mapping patient data by postal codes to identify regional  \"healthcare trends.Automated KPI monitoring for encounter volume, patient outcomes, and average encounter duration. \"Delivered data-driven insights for executive reporting, population health, and clinical operations optimization. \"Launched a predictive market forecasting model using historical outcome measure data (2-year trend) to estimate  \"a 12% growth response rate, optimizing survey effectiveness strategies, and increasing positive feedback rate projections by 20% quarterlyAccelerated data accessibility for stakeholders by building scalable ETL pipelines in SQL and Python, validating  \"' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}",
         25769803783
        ],
        [
         "page_content=' and increasing positive feedback rate projections by 20% quarterlyAccelerated data accessibility for stakeholders by building scalable ETL pipelines in SQL and Python, validating  \"500,000 records per day with 99.99% accuracy, and reducing product integration time by 15% per release.Leveraged 5+ years of experience to spearhead the migration of legacy data systems to cloud-based solutions  \"using AWS, reducing processing time by 40%.Conducted survey research to assess patient satisfaction, applying statistical tools such as R and Python to  \"analyze data trends, resulting in a 25% increase in actionable insights by creating data' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}",
         25769803784
        ],
        [
         "page_content=' patient satisfaction, applying statistical tools such as R and Python to  \"analyze data trends, resulting in a 25% increase in actionable insights by creating data visualizations and dashboards, enabling stakeholders to make informed decisions and accelerate strategic initiatives by reducing decision-making time by 2 weeks to get prototype developed within 1-2 months.Crafted a centralized data visualization catalog for all analytics projects, resulting in a 50% increase in data  \"discoverability and reuse, which fostered greater collaboration across teams and increased data integrity.Orchestrated the creation of 10+ interactive business intelligence dashboards utilizing Tableau, Power BI,' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}",
         34359738368
        ],
        [
         "page_content=' fostered greater collaboration across teams and increased data integrity.Orchestrated the creation of 10+ interactive business intelligence dashboards utilizing Tableau, Power BI, and  \"Apache Superset, displaying critical KPIs like patient readmission rates and treatment progress, presented weekly to executives.Revolutionized data processing by constructing a serverless ETL framework with AWS Lambda and Step  \"Functions; boosted real-time analysis of 600 GB daily and reduced data lag by 12 hours.Expanded role to lead GenAI tech evaluation, creating three data pipeline prototypes within three months and  \"reducing model training expenses by 15%' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}",
         34359738369
        ],
        [
         "page_content=' 12 hours.Expanded role to lead GenAI tech evaluation, creating three data pipeline prototypes within three months and  \"reducing model training expenses by 15% through infrastructure investment insights.Executed the adoption of serverless architecture utilizing AWS Lambda, personally refactoring 10+ legacy  \"functions and decreasing individual function execution time by an average of 20ms.Self-Employed/Independent Contractor | San Jose, CA                                         ' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}",
         34359738370
        ],
        [
         "page_content='                                                          Oct 2023  \u0013 Apr 2025                                                               ' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}",
         34359738371
        ],
        [
         "page_content='                                                AWS Cloud Data Scientist (Part-time Contract)  Steered feature engineering initiatives for the core recommendation system, lifting the model's precision by 15%  \"and driving a 22% surge in user interaction, contributing to higher click-through rates.Discovered biases in training data using statistical methods and collaborated with cross-functional teams to refine  \"data collection, decreasing model errors by 40% and improving' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}",
         34359738372
        ],
        [
         "page_content='.Discovered biases in training data using statistical methods and collaborated with cross-functional teams to refine  \"data collection, decreasing model errors by 40% and improving overall accuracy.Spearheaded data mining projects, analyzing large data sets over 12 months, utilizing SQL and Python to achieve  \"a 35% improvement in predictive modelConducted comprehensive statistical analyses on datasets, using R and Python over 9 months, to optimize  \"decision-making processes, resulting in a 20% increase in forecast accuracy.Developed an AI-driven recommendation system in TensorFlow utilizing synthetic healthcare data, prompting  \"various LLM models for' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}",
         34359738373
        ],
        [
         "page_content='% increase in forecast accuracy.Developed an AI-driven recommendation system in TensorFlow utilizing synthetic healthcare data, prompting  \"various LLM models for patient insights. Integrated Synthea-generated records mapped to OMOP CDM, enabling clinicians to identify optimal chronic disease treatments and improve early intervention strategies. VOLKSWAGEN | Belmont, CA                                                        ' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}",
         34359738374
        ],
        [
         "page_content='                                                                                     Apr 2023  \u0013 May 2023  Sr. Software Engineer & Machine Learning Developer Engineered a synthetic ETL data pipeline utilizing electric vehicle simulations to train reinforcement learning  \"models in TensorFlow, achieving a' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}",
         34359738375
        ],
        [
         "page_content=' Software Engineer & Machine Learning Developer Engineered a synthetic ETL data pipeline utilizing electric vehicle simulations to train reinforcement learning  \"models in TensorFlow, achieving a 10x improvement in read-in and read-out event processing speeds, completing the project 2 weeks ahead of schedule.Reduced data latency in the reinforcement learning pipeline by 60 milliseconds by optimizing data compression  \"algorithms, improving event rates by a factor of ten.Spearheaded the integration of real-world driving data into EV simulation models, resulting in solutions to fix the  \"three biggest causes of simulation-reality gaps, thus increasing accuracy by 15%.Faraday' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}",
         34359738376
        ],
        [
         "page_content=' driving data into EV simulation models, resulting in solutions to fix the  \"three biggest causes of simulation-reality gaps, thus increasing accuracy by 15%.Faraday Future | San Jose, CA                                                                                          ' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}",
         42949672960
        ],
        [
         "page_content='                                                 May 2022  \u0013 Mar 2023Staff Data Engineer Forged automation workflows within AWS infrastructure on EC2; empowered teams with near-real-time, scalable  \"failure statistic dashboards; and leveraged Apache Superset for big data visualization, achieving 2x runtime efficiency.Implemented Flask/RESTful APIs for real-time analytics, resolving large-scale operational challenges' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}",
         42949672961
        ],
        [
         "page_content=' for big data visualization, achieving 2x runtime efficiency.Implemented Flask/RESTful APIs for real-time analytics, resolving large-scale operational challenges and leading  \"to a 40% reduction in system downtime during peak traffic hours.Streamlined feature engineering using AWS Crawl, Glue, and Athena, achieving a 99.99% data ETL pipeline  \"uptime; Also, personally validated the accuracy of 500+ data transformations daily.' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}",
         42949672962
        ],
        [
         "page_content='Piloted the creation of five novel data quality thresholds by deploying AWS Glue and Athena, which allowed the  \"team to detect 50+ data defects each day, elevating overall data integrity.Instituted automated data quality controls with AWS Glue and Athena, flagging 75+ daily data defects, thereby  \"increasing ETL data pipeline success rates by 35% and driving down inconsistencies by 60%.MORGAN STANLEY| Providence, RI                                    ' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}",
         42949672963
        ],
        [
         "page_content='                                                                                               Jan 2021 - May 2022  Data Software Engineer & Machine Learning Developer Executed 50+ data quality tests weekly on top of ETL data pipeline via complex SQL and Python3' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}",
         42949672964
        ],
        [
         "page_content=' - May 2022  Data Software Engineer & Machine Learning Developer Executed 50+ data quality tests weekly on top of ETL data pipeline via complex SQL and Python3 on Snowflake,  \"guaranteeing 100% accuracy in data, leading to zero data-related incidents and top performer.Mastered the data migration process from SQL to Salesforce, achieving 99.9% accuracy on over 10,000 rows of  \"data, and identified data duplicates using Pandas, within two weeks of starting.Automated feature extraction from 10,000+ scraped web pages using NLP models in TensorFlow, and facilitated ' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}",
         42949672965
        ],
        [
         "page_content=', within two weeks of starting.Automated feature extraction from 10,000+ scraped web pages using NLP models in TensorFlow, and facilitated  \"80% accuracy in job seeker recommendations, using Python, Pandas, and AWS EMR via Visual Studio.Revamped data preprocessing stage for training neural networks on PySpark; findings led to fixing the three  \"biggest causes of model bias, improving fairness metrics by 8%. LOS ALAMOS NATIONAL LAB | Batavia, IL                       ' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}",
         42949672966
        ],
        [
         "page_content=' NATIONAL LAB | Batavia, IL                                                                                       Aug 2018 - Feb 2020      Postdoc Scientist Integrated the data trigger emulator into the BNL-PHENIX-Fun4All framework using C' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}",
         42949672967
        ],
        [
         "page_content=' 2018 - Feb 2020      Postdoc Scientist Integrated the data trigger emulator into the BNL-PHENIX-Fun4All framework using C++ on Linux, achieving over  \"90% precision to support time series analysis and timing data production.Championed the C++ algorithm calibration process for detector readout electronics, reducing average noise rate  \"across all channels by two orders of magnitude; resolved signal processing issues through systematic troubleshooting.Enhanced data calibration algorithm reconstruction in C++ on Linux, improving multi-class classification accuracy  \"from 30+% to a range of 80 \u001399%.Formulated MAT' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}",
         42949672968
        ],
        [
         "page_content=' algorithm reconstruction in C++ on Linux, improving multi-class classification accuracy  \"from 30+% to a range of 80 \u001399%.Formulated MATLAB scripts to refine brain image calibration, diminishing artifacts by 5% and amplifying ROI- \"masked regression matrix intensity by 15% for precise 3D image segmentation processes.Fabricated a data capture system using C/C++ within a Linux environment, improving multi-channel reading rates  \"by 400%, from 20MB/sec to 80MB/sec; this infrastructure now supports 10+ concurrent research projects.' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/ChunMin Jen.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}",
         51539607552
        ],
        [
         "page_content='JIA  YANG  \n\uD83D\uDCCD San Diego, CA  \n (860) 834-3983  \n\n jiayanglu01@gmail.com   LinkedIn URL \nP R O F I L E \nData Scientist with extensive experience delivering end -to-end analytics solutions, from data \npipelines to production-ready machine learning and AI models. Expertise spans predictive modeling, \nadvanced statistical analysis, NLP, and Generative AI, leveraging Python, R, Spark, and cloud  \nplatforms. Proven' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         51539607553
        ],
        [
         "page_content=' \nadvanced statistical analysis, NLP, and Generative AI, leveraging Python, R, Spark, and cloud  \nplatforms. Proven ability to translate complex analytics into actionable insights, drive process \noptimization, and support healthcare and operational decision -making at scale. Adept at \ncollaborating with cross-functional teams to integrate data-driven solutions into business processes. \nTE C H N I C A L S K I L L S \n Programming & Frameworks: Python, SQL, R, SAS, Git, Docker \n Machine Learning &' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         51539607554
        ],
        [
         "page_content=' L L S \n Programming & Frameworks: Python, SQL, R, SAS, Git, Docker \n Machine Learning & Statistics: Supervised & Unsupervised Learning, Regression, Ensemble \nMethods, Experimental Design, Causal Inference, Bayesian Modeling, Hypothesis Testing, A/B \nTesting, Time Series Analysis, Survival Analysis, Clustering \n Deep Learning & AI: Natural Language Processing (NLP), Transformers, Neural Networks \n Big Data & Cloud: Databricks, Apache Spark, Real-time Streaming' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         51539607555
        ],
        [
         "page_content=' Natural Language Processing (NLP), Transformers, Neural Networks \n Big Data & Cloud: Databricks, Apache Spark, Real-time Streaming \n Data Visualization & Tools : Tableau, Matplotlib, Seaborn, Plotly, Jupyter, Quarto, Interactive \nDashboards, APIs \nP R O F E S S I O N A L E X P E R I E N C E \nData Science/Statistical Consultant, North Carolina State University | 2023  Present \n Established robust data engineering frameworks using Py' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         51539607556
        ],
        [
         "page_content=' \nData Science/Statistical Consultant, North Carolina State University | 2023  Present \n Established robust data engineering frameworks using PySpark and Spark SQL for real-time and \nlarge-scale analytics. Created forecasting pipelines with  advanced feature engineering (e.g., \nPCA) and Elastic Net regularization for proactive energy management, and analyzed  NFL \nstatistics (2005-2023) for player evaluations, maximizing strategic and operational insights. \n Drove strategic decision-making by deploying production-ready predictive modeling pipelines for \ncritical classification and regression problems' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         51539607557
        ],
        [
         "page_content=' strategic and operational insights. \n Drove strategic decision-making by deploying production-ready predictive modeling pipelines for \ncritical classification and regression problems. Applied diverse  ML models (e.g., XGBoost, \nRandom Forest, SVM, KNN)  with feature engineering  and regularization to evaluate diabetes \nrisk, lung cancer severity, and wine quality/type, achieving up to  100% accuracy and ensuring \nscalability via a Dockerized Plumber API.  \n Synthesized advanced AI and big data methodologies  to deliver actionable insights' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         51539607558
        ],
        [
         "page_content='alability via a Dockerized Plumber API.  \n Synthesized advanced AI and big data methodologies  to deliver actionable insights and inform \nstrategic understanding.  Fine-tuned DeBERTa and DistilBERT models with TensorFlow, \nachieving >81% accuracy on Google reviews for sentiment analysis, and documented  real-time \ndata streaming architectures (Kafka, Kinesis) to inform data ingestion and processing strategies. \n Engineered advanced statistical solutions  optimizing business strategy through  survival \nanalysis for lung cancer mortality risk,' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         51539607559
        ],
        [
         "page_content=' data ingestion and processing strategies. \n Engineered advanced statistical solutions  optimizing business strategy through  survival \nanalysis for lung cancer mortality risk,  Bayesian models  for censored hierarchi cal data in the \npoultry industry, and  time-series models for air quality evaluation.  Boosted prediction accuracy \nby 31% and informed dosing strategies and risk assessment via an interactive R Shiny app. \n Pioneered comprehensive data analysis frameworks  delivering reproducible, actionable \ninsights via an  R/Quarto pipeline  with GitHub for U.S. Census data' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         51539607560
        ],
        [
         "page_content=' analysis frameworks  delivering reproducible, actionable \ninsights via an  R/Quarto pipeline  with GitHub for U.S. Census data, a  SAS pipeline  for' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         60129542144
        ],
        [
         "page_content='JIA YANG                                                                                                                            ' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         60129542145
        ],
        [
         "page_content='                                   \n jiayanglu01@gmail.com \n2 \n \nregulatory-style clinical trial reports, and an  interactive R Shiny app  (openFDA API) for adverse \nevent trend analysis and downloadable data. \nScientist, Sorrento Therapeutics, San Diego, CA | 2022  2023 \n Applied advanced statistical analyses, including  Survival analysis, to evaluate gene and' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         60129542146
        ],
        [
         "page_content=' Therapeutics, San Diego, CA | 2022  2023 \n Applied advanced statistical analyses, including  Survival analysis, to evaluate gene and cell \ntherapy outcomes, resulting in contributions to peer-reviewed publications.  \n Collaborated with cross-functional teams, enhancing analytical workflow efficiency and ensuring \ndata reproducibility for R&D projects. \nPostdoctoral Scholar, University of California San Diego, La Jolla, CA | 2011 2014 \n Developed and led statistical and bioinformatic analyses  on large genomic datasets, \nidentifying crucial' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         60129542147
        ],
        [
         "page_content=' CA | 2011 2014 \n Developed and led statistical and bioinformatic analyses  on large genomic datasets, \nidentifying crucial biomarkers that informed key research areas. \nPostdoctoral Research Associate, Wesleyan University, Middletown, CT | 2008  2011 \n Applied rigorous statistical methods and experimental design to evaluate stem cell therapies. \nE D U C AT I O N \nMaster of Statistics, North Carolina State University, NC (July 2025; GPA 4.0) \nPh.D. in Pharmacology, Shenyang Pharmaceutical University, China (2008' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         60129542148
        ],
        [
         "page_content=' Carolina State University, NC (July 2025; GPA 4.0) \nPh.D. in Pharmacology, Shenyang Pharmaceutical University, China (2008) \nB.S. in Pharmaceutical Sciences, Shenyang Pharmaceutical University, China (2003) \nC E R T I F I C A T I O N &  H O N O R S \n Micro-MBA, Rady School of Management, UC San Diego \n Early Career Forum Travel Award, The Endocrine Society \n Postdoctoral Travel Award, American Society for Biochemistry and Molecular Biology ' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         60129542149
        ],
        [
         "page_content=' Early Career Forum Travel Award, The Endocrine Society \n Postdoctoral Travel Award, American Society for Biochemistry and Molecular Biology \n Outstanding Doctoral Dissertation, Shenyang Pharmaceutical University \nS E L E C T E D P U B L I C AT I O N S & P R E S E N T AT I O N S \nTahir Y, Yang J , et al. Impact of Sodium Bisulfate and Water Activity on Salmonella Survival in \nPoultry Litter. Manuscript in preparation. (Logistic, GLM,' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         60129542150
        ],
        [
         "page_content=' Sodium Bisulfate and Water Activity on Salmonella Survival in \nPoultry Litter. Manuscript in preparation. (Logistic, GLM, Bayesian modeling) \nChen Z, [], Yang J , et al. Antibody -based binding domain fused to TCR chain facilitates T cell \ncytotoxicity for potent anti-tumor response. Oncogenesis. 2023;12:33. (Survival analysis, ANOVA) \nLu YJ 1 , Sewer MB. p54 nrb/NONO regulates cAMP -dependent glucocort' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         60129542151
        ],
        [
         "page_content=' analysis, ANOVA) \nLu YJ 1 , Sewer MB. p54 nrb/NONO regulates cAMP -dependent glucocorticoid production by \nmodulating phosphodiesterase mRNA splicing and degradation. Mol Cell Biol.  2015;35:1223-37. \n(Bioinformatics analysis)  \nYang J , Sewer MB. p54 nrb/NONO is required for adrenocortical circad ian gene expression and \nglucocorticoid production. The Endocrine Societys Annual Meeting , San Francisco,' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         60129542152
        ],
        [
         "page_content='ortical circad ian gene expression and \nglucocorticoid production. The Endocrine Societys Annual Meeting , San Francisco, 2013. \n(Experimental design, Hypothesis testing) \n                                                             \n1 Publication is under the name Jia Yang Lu .' metadata={'producer': 'www.ilovepdf.com', 'creator': 'Microsoft Word 2016', 'creationdate': '2025-10-01T17:13:35+00:00', 'author': 'python-docx', 'moddate': '2025-10-01T17:13:35+00:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Jia Yang.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         68719476736
        ],
        [
         "page_content='Mohammad HassanpourHealthcare Data Scientist, Clinical AI & Predictive AnalyticsLos Angeles, CA,  mohassan99@gmail.com, 415.264.6938LinkedIn,  GitHub,  PortfolioProfessional SummaryData scientist with 10 /+ /years building interpretable deep-learning and large-scale analytics for healthcare. Expert in Python, R, PyTorch, Spark/Databricks, MLflow and Hugging Face; domain leader in ICD-10, HEDIS/STARs and HIPAA-compliant deployment.EducationM.S. Computer Science (Data Science)  \u0014' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         68719476737
        ],
        [
         "page_content='CD-10, HEDIS/STARs and HIPAA-compliant deployment.EducationM.S. Computer Science (Data Science)  \u0014 University /of /Illinois /Urbana-ChampaignM.A. Applied Statistics  \u0014 UC Santa /BarbaraB.A. Economics  \u0014 UC San /DiegoCore Technical Skills- Languages & Libraries: Python, R, SQL, SAS, VBA, NumPy, Pandas, Scikit-learn- Deep Learning & NLP: PyTorch, TensorFlow, RETAIN, Transformer Models, Attention Mechanisms,' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         68719476738
        ],
        [
         "page_content=', Scikit-learn- Deep Learning & NLP: PyTorch, TensorFlow, RETAIN, Transformer Models, Attention Mechanisms, LSTM, GRU, RNN, CNN, Agenic AI systems, EHR Embedding, Autoencoders, ICD-10 Concept Extraction, Clinical Concept Modeling- Classical Machine Learning: Logistic Regression, GLM, Lasso, Ridge, Elastic Net, PCA, Decision Trees, Random Forest, SVM, KNN, Naive Bayes, Discriminant Analysis, Time Series Forecasting, - MLOps & Platform' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         68719476739
        ],
        [
         "page_content=' Decision Trees, Random Forest, SVM, KNN, Naive Bayes, Discriminant Analysis, Time Series Forecasting, - MLOps & Platforms: MLflow, Git, Databricks, SQL Server, Teradata SQL, SSIS, FastAPI, Streamlit, Torchvision- Visualization & Reporting: Power BI, Tableau- Cloud & DevOps: Azure DevOps, Bash, Multithreading, Email-integrated Automation Workflows- Healthcare Domain: Clinical Informatics, ICD-10 Coding, Claims Processing, HEDIS, STARS Quality Metrics, Healthcare Operations,' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         68719476740
        ],
        [
         "page_content='- Healthcare Domain: Clinical Informatics, ICD-10 Coding, Claims Processing, HEDIS, STARS Quality Metrics, Healthcare Operations, EMR/EHR Data ModelingProfessional ExperienceCentene Corporation, Principal Data ScientistLos Angeles, CA, Nov /2023 / \u0013 /Feb /2025- Deployed interpretable CHF-risk model (RETAIN) on 100 /M /+ EHRs, achieving AUC /0.86 / 80 /% accuracy and surpassing logistic (0.78) and RNN (0.83) baselines.- Refactored Hugging' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         68719476741
        ],
        [
         "page_content='86 / 80 /% accuracy and surpassing logistic (0.78) and RNN (0.83) baselines.- Refactored Hugging /Face transformers for ICD-10 NER, cutting manual coding workload 60 /% and boosting coder throughput.- Cut ETL latency 42 /% via Spark-optimized preprocessing and orchestrated MLflow tracking across production clusters.- Led HIPAA-compliant Azure /ML deployment and cross-functional rollout, tripling clinician adoption of risk dashboards.Centene Corporation, Senior Data ScientistLos Angeles, CA, Oct /2020 / \u0013 /Nov /20' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         68719476742
        ],
        [
         "page_content=' tripling clinician adoption of risk dashboards.Centene Corporation, Senior Data ScientistLos Angeles, CA, Oct /2020 / \u0013 /Nov /2023- Migrated legacy HEDIS/STARs analytics to DAX + Power BI, shrinking reporting time from three days to two hours.- Automated end-to-end claims QA in SQL Server/SSIS/Python, reducing cycle time 30 /%.- Built anomaly-detection model on provider claims that lowered over-payment incidence 12 /%.- Mentored four junior analysts in ML best practices, raising team model-delivery speed 25 /%.Tit' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         68719476743
        ],
        [
         "page_content=' that lowered over-payment incidence 12 /%.- Mentored four junior analysts in ML best practices, raising team model-delivery speed 25 /%.Titanium Health,  Principal Data ScientistLos Angeles, CA, Sep /2019 / \u0013 /Mar /2020- MINA ECG: Built multilevel CNN + BiLSTM + attention model for AF detection, hitting F1 /0.94 / AUROC /0.96 and surfacing beat-, rhythm-, and frequency-level risk cues.- Delivered real-time ECG insights via Streamlit UI on FastAPI micro-services adopted by care teams' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         68719476744
        ],
        [
         "page_content='within four weeks.- Pneumonia X-ray: Developed & benchmarked four autoencoder variants, lowering reconstruction error 18 /% and generating visual heatmaps for radiologists.- Instituted Git-based peer reviews and ML test harness, doubling pipeline reliability and halving onboarding time.Agilon Health, Senior Data ScientistLong Beach, CA, Nov /2018 / \u0013 /Jul /2019-  Built Spark/Databricks feature-engineering flows and PCA models forecasting chronic-disease onset, guiding 10 /% shift in outreach spend.- Implemented regression-based cost forecasting (R /0' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         77309411328
        ],
        [
         "page_content='A models forecasting chronic-disease onset, guiding 10 /% shift in outreach spend.- Implemented regression-based cost forecasting (R /0.71) that informed payer budgeting cycles.- Created Tableau dashboards delivering cohort insights to executives, accelerating decision cycles 40 /%.- Co-authored HIPAA-aligned clinical-prediction API schema adopted by downstream engineering teams.Blue Shield of California, Senior Healthcare Data AnalystSan Francisco, CA, May /2015 / \u0013 /Nov /2018- Created ICD-10 extraction and provider-risk models improving high-cost-provider prediction 22 /%.-' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         77309411329
        ],
        [
         "page_content='2015 / \u0013 /Nov /2018- Created ICD-10 extraction and provider-risk models improving high-cost-provider prediction 22 /%.- Managed Power BI dashboards stratifying provider risk across 70 /+ service lines for actuarial & clinical ops.- Reduced model training time 30 /% via autoencoder-based dimensionality reduction on high-cardinality data.- Partnered with actuarial and operations teams to convert model findings into KPIs, cutting network leakage 8 /%.' metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Mohammad Hassanpour.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         77309411330
        ],
        [
         "page_content='PARISA SAHRAEIAN\nData Scientist\n@ sahraeianparisa@gmail.com ap-arkerLos Angeles, CA, USA /linkedinparisa-sahraeian /githubParisaSahraeianiheart  Google Scholar\nPROFESSIONAL SUMMARY\nData Scientist with 4 years of experience in optimization & operations research, ma-\nchine learning, statistical modeling, AI, delivering end-to-end solutions across adver-\ntising, digital platforms, and operations. Procient in Python, SQL, R, and' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         77309411331
        ],
        [
         "page_content='-to-end solutions across adver-\ntising, digital platforms, and operations. Procient in Python, SQL, R, and cloud plat-\nforms like AWS and Azure. Skilled at turning complex business problems into data-\ndriven strategies that drive measurable impact and improve product performance.\nEXPERIENCE\nData Scientist\niHeartMedia Inc.\n5Jan 2024Dec 2024 ap-arkerRemote, USA\n Optimized music scheduling algorithms using AWS, Azure, Python, SQL, and FICO\nXpress, improving model objectives by 15% and enhancing' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         77309411332
        ],
        [
         "page_content=' USA\n Optimized music scheduling algorithms using AWS, Azure, Python, SQL, and FICO\nXpress, improving model objectives by 15% and enhancing user experience.\n Partnered with revenue management teams using Jira and Conuence to develop\nsolutions that improved digital content delivery eciency.\n Employed data science techniques, A/B test, and statistical analysis to predict\ntrends in music scheduling for innovation in digital content delivery.\nAssociate Data Scientist\niHeartMedia Inc.\n5May 2022Apr 2023 ap-' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         77309411333
        ],
        [
         "page_content=' innovation in digital content delivery.\nAssociate Data Scientist\niHeartMedia Inc.\n5May 2022Apr 2023 ap-arkerSan Antonio, TX, USA\n Developed pricing models using Python and FICO Xpress, boosting advertisement\nrevenue. Conducted code reviews and rigorous A/B testing to ensure scalability\nand accuracy.\n Executed large-scale data analyses, employed advanced statistical techniques and\nmachine learning models, and NLP using Python, R, and SQL, and collaborated\nwith product teams to drive data-informed decision-making.\nData Analyst' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         77309411334
        ],
        [
         "page_content=' learning models, and NLP using Python, R, and SQL, and collaborated\nwith product teams to drive data-informed decision-making.\nData Analyst (AI, Marketing & Optimization Advisor)\nSummit Technology Aliates\n5Aug 2023Dec 2023 ap-arkerEdmond, OK, USA\n Built data pipeline and real-time Power BI dashboards, conducted in-depth analy-\nses using Python to enhance decision-making with sales and marketing teams.\n Conducted in-depth analyses using Python, delivering actionable insights that sig' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         77309411335
        ],
        [
         "page_content='\nses using Python to enhance decision-making with sales and marketing teams.\n Conducted in-depth analyses using Python, delivering actionable insights that sig-\nnicantly improved advertising strategies and campaign performance.\n Utilized AI, NLP, ZoomInfo, and data-driven marketing strategies that boosted\ncampaign eectiveness and revenue, translating technical insights into business\nrecommendations that boosted campaign eectiveness and revenue.\nData Scientist\nShack Plumbing\n5May 2023Aug 2023/Feb 2025Jul 2025 ' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         77309411336
        ],
        [
         "page_content='iveness and revenue.\nData Scientist\nShack Plumbing\n5May 2023Aug 2023/Feb 2025Jul 2025 ap-arkerLos Angeles, CA, USA\n Developed pricing strategies using optimization and statistical models to support\nreal-time decisions and guide revenue impact.\nData Analyst Intern\nCenter for Health Systems Innovation\n5Jan 2022May 2022 ap-arkerStillwater, OK, USA\n Analyzed opioid patient data and built predictive models with physicians to un-\ncover treatment patterns and improve outcomes.\nFinancial Analyst Intern\n' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         85899345920
        ],
        [
         "page_content=' OK, USA\n Analyzed opioid patient data and built predictive models with physicians to un-\ncover treatment patterns and improve outcomes.\nFinancial Analyst Intern\nHenneberry Properties\n5Aug 2021Aug 2022 ap-arkerOK, USA\n Responsible for accounts payable and nancial data, using QuickBooks and Yardi.\nSKILLS\nPython R SQL Gurobi\nFICO Xpress CPLEX AWS\nGithub Tableau Power BI\nPytorch LangChain\nMATLAB BigQuery GCP\nSAS Snowake Azure\n' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         85899345921
        ],
        [
         "page_content='X AWS\nGithub Tableau Power BI\nPytorch LangChain\nMATLAB BigQuery GCP\nSAS Snowake Azure\nNLP C++ ZoomInfo\nAREA OF EXPERTISE\nOptimization/OR     \nData Science     \nML, AI & LLM     \nEDUCATION\nMBA\nCalifornia State University-LA\n5Aug 2025-present\nDesigning and Building AI\nProducts and Services\nprogram\nMIT\n' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         85899345922
        ],
        [
         "page_content='MBA\nCalifornia State University-LA\n5Aug 2025-present\nDesigning and Building AI\nProducts and Services\nprogram\nMIT\n5May 2025-Jul 2025\nM.Sc. Industrial Engineering\nOklahoma State University, GPA: 4\n5Aug 2019-Jul 2022\nB.Sc. Industrial Engineering\nSharif University of Tech\n5Sep 2014-Jan 2019\nDiploma in Physics and Math\nNODET\nPUBLICATIONS\n On atomic cliques in temporal\ngraphs. Y Lu, Z Miao, P Sahraeian,\nB' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         85899345923
        ],
        [
         "page_content='ET\nPUBLICATIONS\n On atomic cliques in temporal\ngraphs. Y Lu, Z Miao, P Sahraeian,\nB Balasundaram. Optimization\nLetters 17 (4), 813-828.\n Predicting friction capacity of\ndriven piles using new\ncombinations of neural networks\nand metaheuristic optimization\nalgorithms. L Jie, P Sahraeian, KI\nZykova, M Mirahmadi, ML Nehdi.\nCase Studies in Construction\nMaterials 19, e02464.' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         85899345924
        ],
        [
         "page_content='Graduate Research Assistant\nOklahoma State University\n5Jan 2021July 2022 ap-arkerStillwater, OK, USA\n Developed optimization models scheduling air trac controllers at FAA.\n Applied the Atomic Clique network optimization model to analyze comorbidity\nprogression in patients, leveraging Gurobi & Python for computational eciency.\n Utilized analytical skills and processed healthcare data, aiding medical decisions,\nbuilt optimization and ML models to improve chronic pain insights.\nGraduate Teaching Assistant\nOklahoma State University\n5' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         85899345925
        ],
        [
         "page_content=' data, aiding medical decisions,\nbuilt optimization and ML models to improve chronic pain insights.\nGraduate Teaching Assistant\nOklahoma State University\n5Aug 2019Dec 2020 ap-arkerStillwater, OK, USA\n Instructor of Introductory Computer Programming, Python, VBA, and Excel.\nCERTIFICATES\n Generative AI for digital Marketers\n Articial Intelligence AI Marketing to Grow your Business\n Marketing Automation: Automate Business and Grow Sales\n Ultimate AWS Certied Cloud Practitioner\n Python Data analysis' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         85899345926
        ],
        [
         "page_content=' Business\n Marketing Automation: Automate Business and Grow Sales\n Ultimate AWS Certied Cloud Practitioner\n Python Data analysis & visualization Masterclass\n Python for Machine Learning & Data Science Masterclass\n Machine Learning A-Z: Python & R in Data Science\n Articial Intelligence Foundations: Neural Networks\n Articial Intelligence Foundations: Machine Learning\n Power BI Essential Training\n Project Online Reporting with Power BI\n Tableau 10 for Data Scientists\n Parisa Sahraeian. Comprehensive\nShift Scheduling: Methodologies' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         85899345927
        ],
        [
         "page_content=' Project Online Reporting with Power BI\n Tableau 10 for Data Scientists\n Parisa Sahraeian. Comprehensive\nShift Scheduling: Methodologies\nand Applications Across Sectors.\nEliva Press, 2024.\nPHD LEVEL COURSEWORK\n Machine Learning\n Linear Optimization\n Nonlinear Optimization\n Combinatorial Optimization\n Network Optimization\n Optimization Under Uncertainty\n Applied Statistical analyses in R\n Decision Theory\n Advanced Operations Research\nHONORS AND AWARDS\n Won the 3rd place in Hackathon\n2022, iHeart Media' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         85899345928
        ],
        [
         "page_content=' Decision Theory\n Advanced Operations Research\nHONORS AND AWARDS\n Won the 3rd place in Hackathon\n2022, iHeart Media Co., US, Jul\n2022.\n Ms. International Runner-Up in\n2019 Ms. and Mr. International\nPageant, Oklahoma State\nUniversity, OK, US., Dec 2019.' metadata={'producer': 'pdfTeX', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-24T19:28:48+00:00', 'gts_pdfa1version': 'PDF/A-1b:2005', 'moddate': '2025-09-24T19:28:48+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'trapped': '/False', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Parisa Sahraeian.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         94489280512
        ],
        [
         "page_content='Venkatesh Terikuti  \nSenior Data Scientist | Analytics Leader | Applied AI Researcher \nvterikuti@gmail.com | +1 (402) 6302823 | linkedin.com/in/terikuti | venkateshterikuti.github.io/ \n \nProfessional Summary \nSenior Data Scientist and ML Platform Engineer with 6+ years of experience designing and deploying scalable machine learning and \nanalytics systems across cloud and enterprise environments. Specialized in building robust ML pipelines for large -scale data \nprocessing, anomaly detection, and predictive analytics. Hands-' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         94489280513
        ],
        [
         "page_content=' systems across cloud and enterprise environments. Specialized in building robust ML pipelines for large -scale data \nprocessing, anomaly detection, and predictive analytics. Hands-on with modern MLOps, distributed data architectures, and secure data \nmanagement. Proven ability to drive product impact by bridging ML research, infrastructure, and cross -functional teams. \n \nEXPERIENCE  \nML Research Assistant (Post Grad research) Feb 2025  Present \nUniversity of Arizona Tucson, AZ \n Built a scalable semantic search engine for e-commerce by fine-tuning Sentence Transformers and Google FLAN-T5' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         94489280514
        ],
        [
         "page_content=' Arizona Tucson, AZ \n Built a scalable semantic search engine for e-commerce by fine-tuning Sentence Transformers and Google FLAN-T5 on a 1.3M-\nproduct catalog, integrating FAISS for fast vector retrieval and enabling real -time, intent-aware search. \n Implemented a full-stack ML pipeline covering data prep, model fine-tuning, and dynamic filter extraction (e.g., price, ratings), \nenhancing structured query refinement and delivering a 90% increase in Precision@10 over traditional keyword search.  \n Led applied research on LLM' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         94489280515
        ],
        [
         "page_content=' \nenhancing structured query refinement and delivering a 90% increase in Precision@10 over traditional keyword search.  \n Led applied research on LLM-powered search, authored a peer-reviewed paper titled \"LLM-based Semantic Search for \nConversational Queries in E-commerce\" submitted to CIKM'25 (ACM), showcasing innovative use of Sentence -BERT for user \nquery understanding and product discovery for public sector use cases.  \n \nSenior Software Engineer  Data Scientist                  ' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         94489280516
        ],
        [
         "page_content=' use cases.  \n \nSenior Software Engineer  Data Scientist                                                                                                             Nov 2021  Jan 2023' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         94489280517
        ],
        [
         "page_content='                           Nov 2021  Jan 2023 \nGlobalLogic                                                                                           ' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         94489280518
        ],
        [
         "page_content='                                                                                                                            Remote  \n' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         94489280519
        ],
        [
         "page_content='                            Remote  \n Built deep learning pipelines using TensorFlow and attention-based LSTMs to forecast demand for 100K+ SKUs with 96% \naccuracy; integrated models via CI/CD using Jenkins and GitLab CI.  \n Designed and deployed a centralized MLOps platform on AWS SageMaker, serving batch and real -time features for 10+ models; \nreduced model deployment latency by 40%.  \n Engineered ETL workflows in MySQL' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         94489280520
        ],
        [
         "page_content=' and real -time features for 10+ models; \nreduced model deployment latency by 40%.  \n Engineered ETL workflows in MySQL, Hive, and Airflow; utilized Databricks for scalable data processing and collaborative \nanalytics. Developed a real-time anomaly detection pipeline (Python, Kafka) to identify drops in order placement and app \nresponsiveness, enabling proactive resolution of customer-impacting issues.  \n Partnered with marketing and supply chain stakeholders to deliver insights and dashboards supporting product inventory and \ncampaign effectiveness. \n Conducted A/B test evaluation' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         103079215104
        ],
        [
         "page_content='nered with marketing and supply chain stakeholders to deliver insights and dashboards supporting product inventory and \ncampaign effectiveness. \n Conducted A/B test evaluation and user segmentation to support growth experimentation strategies; delivered insights used to \nprioritize product roadmap items and optimize user journeys. Engaged directly with cross -functional teams in Agile/Scrum \nenvironments; influenced technical decisions and led process improvements.  \n \nSenior Data Analyst Nov 2019  Oct 2021 \nGeetham Software (Client: Lytx ) Remote \n Led a distributed analytics and DevOps team (US, Argentina' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         103079215105
        ],
        [
         "page_content=' 2019  Oct 2021 \nGeetham Software (Client: Lytx ) Remote \n Led a distributed analytics and DevOps team (US, Argentina, Poland) to deliver big data solutions supporting millions of \ntelematics events daily. \n Built and managed SQL-based and Databricks Delta Lake pipelines for ingesting 6M+ daily events, ensuring 99.9% data freshness \nfor real-time analytics, cleaning, and modeling large-scale event and customer data. \n Established dbt Core semantic models and reusable data marts for analytics self -service; drove advanced cohort' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         103079215106
        ],
        [
         "page_content=' large-scale event and customer data. \n Established dbt Core semantic models and reusable data marts for analytics self -service; drove advanced cohort and A/B testing \nanalytics. \n Designed and deployed automated workflows in AWS Glue and Airflow for campaign, CRM, and compliance datasets; \nimplemented Tableau dashboards, reducing report turnaround by 40%. \n Provided production troubleshooting, performance tuning, and issue resolution across global deployments. Partnered with produ ct \nto design and analyze 10+ A/B tests (e.g., promotional offers),' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         103079215107
        ],
        [
         "page_content=' issue resolution across global deployments. Partnered with produ ct \nto design and analyze 10+ A/B tests (e.g., promotional offers), delivering actionable insights that improved conversion by 8% .. \n Architected and implemented dimensional data models (star and snowflake schemas) in Snowflake and Databricks, supporting \nscalable, performant analytics and ad hoc reporting across sales, operations, and compliance functions.  \n \nData Analyst  Mar 2016  Oct 2019 \nGeetham Software             Chennai  \n' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         103079215108
        ],
        [
         "page_content='\nData Analyst  Mar 2016  Oct 2019 \nGeetham Software             Chennai  \n Developed ETL workflows for campaign, finance, and CRM data across AWS and Azure, automating ingestion and cleaning \ntasks. \n Conducted advanced segmentation, churn analysis, and predictive modeling using Python, SQL, and Tableau.  \n Maintained and improved daily reporting pipelines, standardized data for consistent BI consumption, and supported quarterly \nbusiness reviews. \n Acted as key point of contact for resolving pipeline issues' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         103079215109
        ],
        [
         "page_content=' reporting pipelines, standardized data for consistent BI consumption, and supported quarterly \nbusiness reviews. \n Acted as key point of contact for resolving pipeline issues and ensuring high data quality standards.   \n Led analytics delivery in compliance-driven environments, establishing data governance protocols, ensuring data quality, and' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         103079215110
        ],
        [
         "page_content='partnering with legal and finance teams to enable secure, audit-ready reporting for regulatory and operational reviews. \n \n \nEDUCATION  \nMaster of Science in Data Science, The University of Arizona (GPA: 3.89) Dec 2024 \nBachelor of Technology in Mechanical Engineering, Acharya Nagarjuna University (GPA: 3.42) May 2015 \n \nPROJECTS \nBuilding LLM From Scratch.  \n Implemented GPT-style large language model from ground up using PyTorch with byte-level tokenizer and scaled' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         103079215111
        ],
        [
         "page_content=' Scratch.  \n Implemented GPT-style large language model from ground up using PyTorch with byte-level tokenizer and scaled dot-product \nattention. Built complete training pipelines including pretraining on unlabeled text, classification finetuning, and instruction \nfinetuning (SFT). Demonstrated deep understanding of attention mechanisms, gradient stabilization, and modern LLM \narchitectures. \n \nTransformer From Scratch: Scaling Journey.  \n Built complete transformer implementation scaling from 2.4M to 52M parameters with  explicit eins' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         103079215112
        ],
        [
         "page_content='former From Scratch: Scaling Journey.  \n Built complete transformer implementation scaling from 2.4M to 52M parameters with  explicit einsum operations. Compared \nMacBook character-level models vs H100 GPU BPE models, analyzing hardware efficiency and training dynamics. Implemented \nproduction infrastructure with mixed precision training, memory mapping, and automated benchmarking . \n \nHigh-Performance Video Diffusion Model Optimization  Wan 2.2 .  \n Optimized the Wan 2.2 text-to-video diffusion model pipeline for H100 GPUs, leveraging advanced context -parallel' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         111669149696
        ],
        [
         "page_content='2 .  \n Optimized the Wan 2.2 text-to-video diffusion model pipeline for H100 GPUs, leveraging advanced context -parallelism, flash \nattention 3, and direct cuBLAS/triton kernel integration to achieve sub -25s 720p video generation for 81 framesoutperforming \nstandard FA2/FA3 baselines (1530 TFLOPS, 24 speedup). \n \nPredicting Customer Churn in E-Commerce  \n Created churn prediction models using Random Forest, SVM, and XGBoost with' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         111669149697
        ],
        [
         "page_content='\nPredicting Customer Churn in E-Commerce  \n Created churn prediction models using Random Forest, SVM, and XGBoost with >90% accuracy on the 'Online Retail II' dataset \n(500K+ records).  \n Conducted retention cohort analysis and lifetime value estimation; tuned model performance using cross -validation and grid \nsearch. Delivered Tableau dashboards and actionable insights to simulate real -world e-commerce churn scenarios and KPIs. \n \nEnterprise Knowledge Copilot: Agentic RAG for Internal Q&A & SOP Automation' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         111669149698
        ],
        [
         "page_content='-commerce churn scenarios and KPIs. \n \nEnterprise Knowledge Copilot: Agentic RAG for Internal Q&A & SOP Automation  .  \n Developed an enterprise-grade RAG platform for knowledge workers to ask natural language questions against internal \ndocumentation (SOPs, runbooks, tech specs, policy PDFs).  \n Leveraged LangChain with pgVector and Redis to embed, retrieve, and rerank context chunks; OpenAI GPT -4 for synthesis and \nsummarization. Designed multi-agent orchestration for use cases like IT support' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         111669149699
        ],
        [
         "page_content=' rerank context chunks; OpenAI GPT -4 for synthesis and \nsummarization. Designed multi-agent orchestration for use cases like IT support, onboarding, compliance checks, and change \nmanagement documentation. \n \nSkills \nLanguages: Python, SQL, Bash, C++ \nBig Data & Cloud: Spark, PySpark, Databricks, Airflow, AWS (Lambda, Glue, S3, EC2), Azure (Data Factory, Data Lake), \nSnowflake, Hadoop, Kubernetes, Delta Lake, dbt Core' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         111669149700
        ],
        [
         "page_content=', EC2), Azure (Data Factory, Data Lake), \nSnowflake, Hadoop, Kubernetes, Delta Lake, dbt Core \nData Engineering: ETL/ELT Pipelines, Data Modeling, Data Cleaning/Standardization, Data Warehousing  \nBI & Visualization: Tableau, Power BI, Google Data Studio \nDevOps: Docker, CI/CD, GitLab CI, Jenkins, Terraform (IaC), Prometheus/Grafana  \nData & ML Platforms: SQL, dbt Core, Airflow, Tableau, Looker' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         111669149701
        ],
        [
         "page_content='IaC), Prometheus/Grafana  \nData & ML Platforms: SQL, dbt Core, Airflow, Tableau, Looker (familiar), Power BI, Databricks, Snowflake, AWS (Lambda, S3, \nGlue, SageMaker), Data Warehousing  \nStatistical Modeling: Regression, Classification, Clustering, Experimentation/A/B Testing, Causal Inference  \nLeadership: Agile/Scrum , Cross-functional Team Leadership, Mentorship, Analytics Enablement, Stakeholder Management, Change \n' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         111669149702
        ],
        [
         "page_content=' \nLeadership: Agile/Scrum , Cross-functional Team Leadership, Mentorship, Analytics Enablement, Stakeholder Management, Change \nManagement, OKR Development \n \nCertifications \nDeep Learning Specialization  Andrew Ng, Coursera \nSQL(Advanced) - Hackerrank' metadata={'producer': 'Microsoft Word for Microsoft 365', 'creator': 'Microsoft Word for Microsoft 365', 'creationdate': '2025-09-18T08:16:17-07:00', 'author': 'ual-laptop', 'moddate': '2025-09-18T08:16:17-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Venkatesh Terikuti.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         111669149703
        ],
        [
         "page_content='Vivek Reddy \n562-879-1191 | vreddy704@berkeley.edu | Los Angeles, CA | Linkedin | GitHub \nSUMMARY \n \nData Engineer / Data Scientist with 6+ years of experience building ML, NLP, and Generative AI solutions across \nhealthcare, insurance, and consumer applications. Skilled in customer-facing AI systems, LLMs, intent classification, \nand recommendation pipelines. Proficient in Python, SQL, AWS (SageMaker, Bedrock, Lambda, ECS) with strong \nexperience in' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         111669149704
        ],
        [
         "page_content=' recommendation pipelines. Proficient in Python, SQL, AWS (SageMaker, Bedrock, Lambda, ECS) with strong \nexperience in scaling models to production. Adept at translating complex data into automated, customer-centric AI \ntools that improve service efficiency and user experience. \nEDUCATION \n \nUniversity of California, Berkeley                                                     ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         120259084288
        ],
        [
         "page_content='                                                                                                           August 2024 \nMaster of Information and Data Science \n \nUniversity of California, Los Angeles (U' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         120259084289
        ],
        [
         "page_content='           August 2024 \nMaster of Information and Data Science \n \nUniversity of California, Los Angeles (UCLA)                                                                                              ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         120259084290
        ],
        [
         "page_content='                                        December 2016 \nB.S., Computational and Systems Biology \n  PROFESSIONAL EXPERIENCE \n \nData Engineer (Infrastructure and Analytics) | Venture Connect                                                   ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         120259084291
        ],
        [
         "page_content='                                                 July 2025 - Present \n Codeveloped GitHub Actions workflows for continuous integration and delivery of data pipelines, \nenabling reliable deployment of new features. \n Contributed to the development of AWS Lambda functions for making updates to data tables on AWS \nRDS.   \n Authored and executed unit tests to validate database update logic, strengthening overall system ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         120259084292
        ],
        [
         "page_content=' updates to data tables on AWS \nRDS.   \n Authored and executed unit tests to validate database update logic, strengthening overall system \nrobustness. \n \nData Engineer | Enterprise LLC                                                                                  ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         120259084293
        ],
        [
         "page_content='                                                        August 2024  February 2025 \n Automated ingestion and orchestration of API data into ETL pipelines, enabling scheduled updates. \n Enhanced a social data scoring algorithm by integrating California AirNow API data, delivering weekly \nrefreshed environmental metrics. \n \nData Scientist 2 | MOTER Technologies         ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         120259084294
        ],
        [
         "page_content=' data, delivering weekly \nrefreshed environmental metrics. \n \nData Scientist 2 | MOTER Technologies                                                                                          April 2023  November 2023 \n Developed pipeline for extracting road' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         120259084295
        ],
        [
         "page_content='                 April 2023  November 2023 \n Developed pipeline for extracting road type information from vehicular GPS points to enrich model training data \nutilizing Docker, Open Street Map, and PostgreSQL. \n Contributed to development of a pipeline leveraging Featuretools library to generate and test many features against \nimportant statistical and insurance metrics during iterative model retraining.    \n Developed automated SQL-based ETL pipelines with data validation and monitoring dashboards in \nAmazon Quicksight and Power BI. \n' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         120259084296
        ],
        [
         "page_content=' \n Developed automated SQL-based ETL pipelines with data validation and monitoring dashboards in \nAmazon Quicksight and Power BI. \nData Scientist 1 | MOTER Technologies                                                                                         ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         128849018880
        ],
        [
         "page_content='                                          May 2022  April 2023 \n Developed predictive models using R, SQL, and Python that used onboard vehicle telemetrics data to develop insurance \nproducts for vehicle fleets   \n Worked on Python pipelines for automated data ingestion, preprocessing, model fitting, and outputting insurance KPIs.  \n Contributed to development of a model and a scoring method that was successfully filed and approved for' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         128849018881
        ],
        [
         "page_content=' model fitting, and outputting insurance KPIs.  \n Contributed to development of a model and a scoring method that was successfully filed and approved for use in Indiana.  \n Actively collaborated in a customer-focused, cross-functional environment to drive projects to fast completion. \n \nAssociate Scientist II, Computational Biology | Neogenomics Laboratories                                    December 2020 - April 2022 \n Utilized python to implement unsuper' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         128849018882
        ],
        [
         "page_content='                 December 2020 - April 2022 \n Utilized python to implement unsupervised deep learning algorithms to identify unique cellular morphologies. \n Presented results and published conference paper at 2021 American Association of Cancer Research \nconference  https://cancerres.aacrjournals.org/content/81/13_Supplement/154 \n \nAssociate Scientist I, Computational Biology | Neogenomics Laboratories                         ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         128849018883
        ],
        [
         "page_content='ational Biology | Neogenomics Laboratories                                  October 2018 - December 2020 \n Utilized Keras-Tensorflow to perform deep learning applications including cell classification, object detection, and \ntissue segmentation from cancerous tissue images.  \n Generated and analyzed statistical reports for global pharmaceutical firms and business clients. \n Created and presented data visualizations using Python libraries such as seaborn and matplotlib to deliver \nactionable insights' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         128849018884
        ],
        [
         "page_content=' and business clients. \n Created and presented data visualizations using Python libraries such as seaborn and matplotlib to deliver \nactionable insights to stakeholders.  \nPROJECTS \n \nAssistive Vision Mobile App                                                                            ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         128849018885
        ],
        [
         "page_content='                                                                                                August 2025 \n Trained a custom YOLOv8 object detection model on 1000+ self-collected images and 5,000+ OpenImages samples' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         128849018886
        ],
        [
         "page_content=' 2025 \n Trained a custom YOLOv8 object detection model on 1000+ self-collected images and 5,000+ OpenImages samples, \nachieving real-time, high-precision detection across 20+ grocery item classes.' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}",
         128849018887
        ],
        [
         "page_content=' Built an AI-powered iOS app that performs on-device detection and selectively routes high-confidence frames to a \nserverless AWS pipeline (S3  Lambda  Bedrock) for LLM-based captioning and live voice narration. \n Engineered a low-latency edge-cloud architecture combining CoreML, Vision, and AWS Bedrock, reducing cloud calls \nby 80% and enabling scalable assistive vision for visually impaired users with <3s end-to-end response time. \nAirbnb Housing Recommendation Chatbot (RAG)          ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         128849018888
        ],
        [
         "page_content='3s end-to-end response time. \nAirbnb Housing Recommendation Chatbot (RAG)                                                                                                     August 2024  \n' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         128849018889
        ],
        [
         "page_content='                           August 2024  \n Built a retrieval-augmented generation chatbot with Llama3.1 on AWS Bedrock, FAISS vector store on LangChain \nfor efficient NLP-based retrieval. \n Optimized ranking and filtering for high-relevance recommendations. \nNatural Language to SQL Query Translation       April 2024 \n Finetuned a T5 sequence-to-sequence model to translate natural language questions to SQL given a database ' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         128849018890
        ],
        [
         "page_content='     April 2024 \n Finetuned a T5 sequence-to-sequence model to translate natural language questions to SQL given a database \nschema. \n Achieved an exact string match of 80% and a ROUGE score of 90 using a large cross-domain dataset of over 100 \ndomains utilizing data augmentation, transfer learning, and curriculum learning techniques. \n \nSKILLS & TOOLS \n \n Languages: Python, R, SQL, Javascript \n Machine Learning: Clustering, Logistic Regression, SVM, Random' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         128849018891
        ],
        [
         "page_content='\n \n Languages: Python, R, SQL, Javascript \n Machine Learning: Clustering, Logistic Regression, SVM, Random Forest, Neural Networks, PCA, Entity Matching, YOLOv8, T5, LLM fine-tuning, RAG pipelines \n Analytics & Modeling: Experimentation design, Metrics Design, EDA, Hypothesis Testing, A/B Testing, \nRegression, Feature Engineering \n Visualization: Tableau, Quicksight, Power BI, Seaborn, ggplot2 \n Cloud &' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         128849018892
        ],
        [
         "page_content=', Feature Engineering \n Visualization: Tableau, Quicksight, Power BI, Seaborn, ggplot2 \n Cloud & Infrastructure: AWS (CDK, S3, Athena, SageMaker, Bedrock, ECS, Glue), Docker, FastAPI, \nCI/CD, version control \n ML Frameworks: Scikit-learn, XGBoost, PyTorch, Keras, LangChain \n Data Ops: Docker, Airflow, Spark, ETL design, MLOps, Featuretools' metadata={'producer': 'PyPDF', 'creator': 'Microsoft Word', 'creationdate': '2025-09-27T18:36:57-07:00', 'author': 'Vivek Reddy', 'moddate': '2025-09-27T18:36:57-07:00', 'source': '/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume/Vivek Reddy.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}",
         128849018893
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "chunked_text",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "chunk_id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "display(documents_with_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd29429-a723-4c89-b788-47d15abd02f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now that we split the documents into more manageable chunks. We will now set up **Databricks Vector Search** with a **Direct Vector Access Index** which will be used with Langchain in our RAG architecture. \n",
    "- We first need to create a dataframe with an id column to be used with Vector Search.\n",
    "- We will then calculate the embeddings using BGE\n",
    "- Finally we will save this in our Vector Search as an index to be used for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b2c8a96-f849-414d-8a45-84a60415b32e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "UC_CATALOG = f'dbc_adv_anlaytics_dev'\n",
    "UC_SCHEMA = f'yh_agent_resume'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ebd54a4-01e9-4f7d-8cac-4d048ddd8714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "CHUNKS_DELTA_TABLE = f\"{UC_CATALOG}.{UC_SCHEMA}.p4_resumes_chunked\"\n",
    "CHUNKS_VECTOR_INDEX = f\"{UC_CATALOG}.{UC_SCHEMA}.p4_resumes_chunked_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6337940a-237f-45d6-b584-76746979e0b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VECTOR_SEARCH_ENDPOINT = vector_search_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e989a070-99b4-4c82-87ad-ab6d88e0cabe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked_index'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHUNKS_VECTOR_INDEX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cda0389-fdea-4a7a-94ed-240c00a8f100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is setting up the catalog for the chunked data as well as establishes the Vector Search endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23eb4bdf-9d68-4058-962c-c0b8e5ad7848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS: UC catalog `dbc_adv_anlaytics_dev` exists\nPASS: UC schema `dbc_adv_anlaytics_dev.yh_agent_resume` exists\nPASS: Vector Search endpoint `yhu01_resume_search` exists\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.vectorsearch import EndpointStatusState, EndpointType\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, EndpointStateReady\n",
    "from databricks.sdk.errors import ResourceDoesNotExist, NotFound, PermissionDenied\n",
    "import os\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Create UC Catalog if it does not exist, otherwise, raise an exception\n",
    "try:\n",
    "    _ = w.catalogs.get(UC_CATALOG)\n",
    "    print(f\"PASS: UC catalog `{UC_CATALOG}` exists\")\n",
    "except NotFound as e:\n",
    "    print(f\"`{UC_CATALOG}` does not exist, trying to create...\")\n",
    "    try:\n",
    "        _ = w.catalogs.create(name=UC_CATALOG)\n",
    "    except PermissionDenied as e:\n",
    "        print(f\"FAIL: `{UC_CATALOG}` does not exist, and no permissions to create.  Please provide an existing UC Catalog.\")\n",
    "        raise ValueError(f\"Unity Catalog `{UC_CATALOG}` does not exist.\")\n",
    "        \n",
    "# Create UC Schema if it does not exist, otherwise, raise an exception\n",
    "try:\n",
    "    _ = w.schemas.get(full_name=f\"{UC_CATALOG}.{UC_SCHEMA}\")\n",
    "    print(f\"PASS: UC schema `{UC_CATALOG}.{UC_SCHEMA}` exists\")\n",
    "except NotFound as e:\n",
    "    print(f\"`{UC_CATALOG}.{UC_SCHEMA}` does not exist, trying to create...\")\n",
    "    try:\n",
    "        _ = w.schemas.create(name=UC_SCHEMA, catalog_name=UC_CATALOG)\n",
    "        print(f\"PASS: UC schema `{UC_CATALOG}.{UC_SCHEMA}` created\")\n",
    "    except PermissionDenied as e:\n",
    "        print(f\"FAIL: `{UC_CATALOG}.{UC_SCHEMA}` does not exist, and no permissions to create.  Please provide an existing UC Schema.\")\n",
    "        raise ValueError(\"Unity Catalog Schema `{UC_CATALOG}.{UC_SCHEMA}` does not exist.\")\n",
    "\n",
    "# Create the Vector Search endpoint if it does not exist\n",
    "vector_search_endpoints = w.vector_search_endpoints.list_endpoints()\n",
    "if sum([VECTOR_SEARCH_ENDPOINT == ve.name for ve in vector_search_endpoints]) == 0:\n",
    "    print(f\"Please wait, creating Vector Search endpoint `{VECTOR_SEARCH_ENDPOINT}`.  This can take up to 20 minutes...\")\n",
    "    w.vector_search_endpoints.create_endpoint_and_wait(VECTOR_SEARCH_ENDPOINT, endpoint_type=EndpointType.STANDARD)\n",
    "\n",
    "# Make sure vector search endpoint is online and ready.\n",
    "w.vector_search_endpoints.wait_get_endpoint_vector_search_endpoint_online(VECTOR_SEARCH_ENDPOINT)\n",
    "\n",
    "print(f\"PASS: Vector Search endpoint `{VECTOR_SEARCH_ENDPOINT}` exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b61a4b-847a-49f6-a51b-2e0948cbb453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHUNKS_DELTA_TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e0a9ba-30dc-4ecf-9377-7c7bde61439c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Delta Table at: https://adb-640321604414221.1.azuredatabricks.net/explore/data/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resumes_chunked\n"
     ]
    }
   ],
   "source": [
    "# Now you can proceed with saving chunked_docs_df to a Delta table for further processing\n",
    "documents_with_id.write.format(\"delta\").mode(\"overwrite\").saveAsTable(CHUNKS_DELTA_TABLE)\n",
    "\n",
    "# Enable change-data capture for the Delta table\n",
    "spark.sql(f\"ALTER TABLE {CHUNKS_DELTA_TABLE} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\n",
    "print(f\"View Delta Table at: https://{workspace_url}/explore/data/{UC_CATALOG}/{UC_SCHEMA}/{CHUNKS_DELTA_TABLE.split('.')[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e11348f9-b563-4cf3-8a30-18ee45357b94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "THIS is the end that has written to the chunk table now we need to run the code to create the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b2c1a01-ba85-4dc4-9847-7ca967acc19b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fa4691f-0faa-4f7e-a4b2-202dbd07f733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1/ Create the Vector Search Index\n",
    "First, we copy the sample data to a Delta Table and sync to a Vector Search index. Here, we use the gte-large-en-v1.5 embedding model hosted on Databricks Foundational Model APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e54f799a-a924-4f97-9491-c3045c5271ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing index named 'dbc_adv_anlaytics_dev.yh_agent_resume.resumes_chunked_index_p4' was found or an error occurred: Response content b'{\"error_code\":\"RESOURCE_DOES_NOT_EXIST\",\"message\":\"Unity Catalog entity dbc_adv_anlaytics_dev.yh_agent_resume.resumes_chunked_index_p4 does not exist.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"e932d44d-7f9d-4f50-bacf-b159331a7bbc\",\"serving_data\":\"\"}]}', status_code 404\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index_name=vector_index_name\n",
    "index_name\n",
    "\n",
    "# Attempt to delete the existing index if it exists\n",
    "try:\n",
    "    vsc.delete_index(index_name=index_name)\n",
    "    print(f\"Existing index '{index_name}' deleted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"No existing index named '{index_name}' was found or an error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "129f6107-671f-4195-9e00-2b57f9fc174f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding docs & creating Vector Search Index, this will take ~5 - 10 minutes.\nView Index Status at: https://adb-640321604414221.1.azuredatabricks.net/explore/data/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resumes_chunked_index\nError creating Vector Search index 'dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked_index': Response content b'{\"error_code\":\"RESOURCE_ALREADY_EXISTS\",\"message\":\"UC entity dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked_index already exists. Please use a unique name in UC for the index name.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"67d787d7-c2da-4afc-b1f4-0dc954b104eb\",\"serving_data\":\"\"}]}', status_code 400\n"
     ]
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# Embed and sync chunks to a vector index\n",
    "print(\n",
    "    f\"Embedding docs & creating Vector Search Index, this will take ~5 - 10 minutes.\\nView Index Status at: https://{workspace_url}/explore/data/{UC_CATALOG}/{UC_SCHEMA}/{CHUNKS_VECTOR_INDEX.split('.')[-1]}\"\n",
    ")\n",
    "\n",
    "# Initialize the Vector Search client\n",
    "vsc = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "# Define the index name and other parameters\n",
    "index_name = CHUNKS_VECTOR_INDEX\n",
    "source_table_name = CHUNKS_DELTA_TABLE\n",
    "primary_key_column = \"chunk_id\"  # Adjust based on your table's primary key\n",
    "embedding_column = \"chunked_text\"  # Adjust based on your embedding column\n",
    "\n",
    "# Check if the source table exists\n",
    "if not spark.catalog.tableExists(source_table_name):\n",
    "    raise Exception(f\"Table '{source_table_name}' does not exist.\")\n",
    "\n",
    "# Proceed to create the Vector Search index without invoking serverless compute\n",
    "# This example creates a Delta Sync Index with embeddings computed by Databricks\n",
    "try:\n",
    "    index = vsc.create_delta_sync_index_and_wait(\n",
    "        endpoint_name=VECTOR_SEARCH_ENDPOINT,\n",
    "        index_name=index_name,\n",
    "        primary_key=primary_key_column,\n",
    "        source_table_name=source_table_name,\n",
    "        pipeline_type=\"TRIGGERED\",  # Using TRIGGERED to avoid continuous compute costs\n",
    "        embedding_source_column=embedding_column,\n",
    "        embedding_model_endpoint_name=\"databricks-bge-large-en\",  # Specify your model endpoint\n",
    "    )\n",
    "    print(f\"Vector Search index '{index_name}' created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Vector Search index '{index_name}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82f25c2f-48df-4c47-831a-83c2da9f74a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2/ Deploy to the review application\n",
    "\n",
    "Now that our Vector Search index is ready, let's prepare the RAG chain and deploy it to the review application backed by a scalable-production ready REST API on Model serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ce0ba2e-73c5-4984-9cb4-c9242bea6a31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2.1/ Configuring our Chain parameters\n",
    "Databricks makes it easy to parameterize your chain with MLflow Model Configurations. Later, you can tune application quality by adjusting these parameters, such as the system prompt or retrieval settings. Most applications will include many more parameters, but for this demo, we'll keep the configuration to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "138d6f89-98ce-4ef0-be51-8e4fc101b563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chain_config = {\n",
    "    \"llm_model_serving_endpoint_name\": \"poc-openai-completions-endpoint\",  # the foundation model we want to use\n",
    "    \"vector_search_endpoint_name\": VECTOR_SEARCH_ENDPOINT,  # Endoint for vector search\n",
    "    \"vector_search_index\": f\"{CHUNKS_VECTOR_INDEX}\",\n",
    "    \"llm_prompt_template\": \"\"\"You are a hiring manager for Data Scientist, Consultant of Blue Shield of California, which \n",
    "-Requires a bachelors degree in mathematics, statistics, computer science, or an equivalent quantitative scientific discipline\n",
    "-Requires at least 3 years of professional Data Science or ML experience; or a Ph.D. degree in operations research, applied statistics, data mining, machine learning, or other quantitative discipline\n",
    "-Must be able to demonstrate real-world experience to translate business problems into an ML problem and be able to communicate AI recommendations in a business context to a general non-technical audience\n",
    "-Proficiency in scalable data transformation techniques using SQL, SAS, Spark, or equivalent\n",
    "-Proficiency in open-source languages such as Python, R, and Julia\n",
    "-Hands-on experience with cloud environments such as Azure and Google Cloud and nice to have experience in DataBricks\n",
    "-Understanding of statistical methods and advanced modeling techniques (e.g., SVM, K-Means, Random Forest, Boosting, Bayesian inference, natural language processing)\n",
    "-Experience with machine learning and deep learning packages (scikit-learn, XGBoost, Tensorflow, or PyTorch)\n",
    "-Knowledge of evaluating solutions for fairness, bias, accuracy, drift, validity, fit, robustness, and explainability\n",
    "-MLOps experience including good design documentation, unit testing, integration testing, and version control (git)\n",
    "-Experience in experimentation design and A/B testing\n",
    "-Knowledge and experience in Generative AI techniques and applications, including natural language generation, image synthesis, and automated content creation\n",
    "-Experience with Generative AI frameworks and tools such as GPT, GANs, and VAEs\n",
    "-Ability to develop and deploy generative models for various applications\n",
    "-Understanding of ethical considerations and best practices in the development and deployment of Generative AI solutions\n",
    "-Ability to partner, collaborate with, and lead relevant stakeholders across diverse functions and experience levels \n",
    "\n",
    "\n",
    "Job Description\n",
    "The Advanced Analytics team works in partnership across the entire Blue Shield of CA enterprise to accelerate business outcomes through the application of AI/machine learning, statistical methodologies, or unstructured data analysis techniques to uncover insights, predict behaviors, and ultimately drive automation to create intelligence at scale. The Data Scientist, Consultant will report to the Director, Advanced Analytics. In this role you will solve problems which range from but are not limited to, text analytics of customer feedback, conversations, and clinical notes, predicting clinical disease progression, understanding the impact of population health programs, clustering member behaviors, creating propensity models, and geospatial analysis of populations to uncover social determinants of health.\n",
    "\n",
    "Responsibilities\n",
    "Collaborate with product owners and business stakeholders to identify opportunities for process optimization and decision-making improvements\n",
    "Perform data exploration and visualization using Python on DataBricks, Tableau, and Python Notebooks to understand the signal-to-noise ratio in datasets\n",
    "Partner with the data engineering team for rapid prototyping of training data sets using SQL, Apache Spark, and other tools\n",
    "Conduct feature engineering using appropriate techniques for the given data and business problem\n",
    "Develop robust model validation procedures and generate performance metrics for evaluation and monitoring\n",
    "Act as the subject matter expert in applied ML for claims, premiums, member risk scores, and other business contexts\n",
    "Drive the lifecycle of machine learning projects from ideation to deployment, ensuring timely delivery and maintaining documentation\n",
    "Perform literature reviews to inform statistical modeling approaches and best practices\n",
    "Translate business requirements into technical specifications and design scalable pipelines with data engineers\n",
    "Assist in transitioning from on-prem infrastructure to the cloud\n",
    "Convert text-based data into feature data sets for predictive analytics\n",
    "Develop materials to explain project findings\n",
    "Lead and contribute to Generative AI projects, including model development and deployment for applications like natural language generation and image synthesis\n",
    "Stay updated with advancements in Generative AI and incorporate relevant techniques into projects\n",
    "Collaborate with cross-functional teams to integrate Generative AI solutions into products and services\n",
    "Conduct experiments and research to explore new applications of Generative AI in healthcare\n",
    "Provide mentorship on Generative AI techniques and best practices\n",
    "Perform other projects or duties as assigned\n",
    "\n",
    "Use the following resumes answer the question. \\n\\nContext: {context}\"\"\", # LLM Prompt template\n",
    "}\n",
    "\n",
    "# Here, we define an input example in the schema required by Agent Framework\n",
    "input_example = {\"messages\": [ {\"role\": \"user\", \"content\": \"Who would be a good data scientist?\"}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a0c84c0-3fe9-489c-817d-7f6b1d002180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2.1/ Log the application & view trace\n",
    "We first register the chain as an MLflow model and inspect the MLflow Trace to understand what is happening inside the chain.\n",
    "\n",
    "MLflow trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68756fd6-c935-4330-8fe4-73bb27174e8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "QUICK_START_REPO_SAVE_FOLDER = \"genai-resume\"\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "lc_model=os.path.join(\n",
    "            os.getcwd(),\n",
    "            f\"rag_langchain_runner\",\n",
    "        ),  # Chain code file from the quick start repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afa7a660-40e1-42c2-bb32-73b5edcdd92d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/rag_langchain_runner',)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ce7a54-97db-4f38-a466-a99a594de4b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/09 04:03:31 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n\uD83D\uDD17 View Logged Model at: https://adb-640321604414221.1.azuredatabricks.net/ml/experiments/eaeb0ef61f564c63a0c453968c21069a/models/m-ae7082f6457a4fa4896e95aab0f89387?o=640321604414221\n2025/10/09 04:03:34 WARNING mlflow.models.utils: The model file uses 'dbutils' commands which are not supported. To ensure your code functions correctly, make sure that it does not rely on these dbutils commands for correctness.\n/tmp/tmpdfqkp7t1/model.py:64: LangChainDeprecationWarning: The class `DatabricksVectorSearch` was deprecated in LangChain 0.3.3 and will be removed in 1.0. An updated version of the class exists in the :class:`~databricks-langchain package and should be used instead. To use it run `pip install -U :class:`~databricks-langchain` and import as `from :class:`~databricks_langchain import DatabricksVectorSearch``.\n  vector_search_as_retriever = DatabricksVectorSearch(\n/tmp/tmpdfqkp7t1/model.py:79: FutureWarning: set_retriever_schema is deprecated and will be removed in a future version. Please migrate to use VectorSearchRetrieverTool in the 'databricks-ai-bridge' package, or match the default schema so your retriever spans can be detected without requiring explicit configuration. See https://mlflow.org/docs/latest/genai/data-model/traces/#retriever-spans for more information.\n  mlflow.models.set_retriever_schema(\n/tmp/tmpdfqkp7t1/model.py:153: LangChainDeprecationWarning: The class `ChatDatabricks` was deprecated in LangChain 0.3.3 and will be removed in 1.0. An updated version of the class exists in the :class:`~databricks-langchain package and should be used instead. To use it run `pip install -U :class:`~databricks-langchain` and import as `from :class:`~databricks_langchain import ChatDatabricks``.\n  model = ChatDatabricks(\n2025/10/09 04:03:35 WARNING mlflow.models.signature: Failed to infer the model signature from the input example. Reason: MlflowException('1 tasks failed. Errors: {0: \\'error: Exception(\\\\\\'Response content b\\\\\\\\\\\\\\'{\"error_code\":\"NOT_FOUND\",\"message\":\"Vector search endpoint f78b32e3-9a0f-4915-b045-21ad2edd3979 not found.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"d679f4a6-4a73-464c-9acd-755d7fd87ef6\",\"serving_data\":\"\"}]}\\\\\\\\\\\\\\', status_code 404\\\\\\') Traceback (most recent call last):\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/utils.py\", line 159, in issue_request\\\\n    response.raise_for_status()\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/requests/models.py\", line 1026, in raise_for_status\\\\n    raise HTTPError(http_error_msg, response=self)\\\\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://centralus-c2.azuredatabricks.net/api/2.0/vector-search/indexes/dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked_index/query\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 199, in call_api\\\\n    response = self.single_call_api(callback_handlers)\\\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 143, in single_call_api\\\\n    response = self._predict_single_input(\\\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 114, in _predict_single_input\\\\n    return self.lc_model.invoke(single_input, config=config, **kwargs)\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3246, in invoke\\\\n    input_ = context.run(step.invoke, input_, config)\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 4001, in invoke\\\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\\\n                   ^^^^^^^^^^^^^^^\\\\n  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 456, in result\\\\n    return self.__get_result()\\\\n           ^^^^^^^^^^^^^^^^^^^\\\\n  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\\\\n    raise self._exception\\\\n  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\\\\n    result = self.fn(*self.args, **self.kwargs)\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3985, in _invoke_step\\\\n    return context.run(\\\\n           ^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3246, in invoke\\\\n    input_ = context.run(step.invoke, input_, config)\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/retrievers.py\", line 263, in invoke\\\\n    result = self._get_relevant_documents(\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/vectorstores/base.py\", line 1067, in _get_relevant_documents\\\\n    docs = self.vectorstore.similarity_search(query, **kwargs_)\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_community/vectorstores/databricks_vector_search.py\", line 315, in similarity_search\\\\n    docs_with_score = self.similarity_search_with_score(\\\\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_community/vectorstores/databricks_vector_search.py\", line 355, in similarity_search_with_score\\\\n    search_resp = self.index.similarity_search(\\\\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/index.py\", line 358, in similarity_search\\\\n    response = RequestUtils.issue_request(\\\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/utils.py\", line 161, in issue_request\\\\n    raise Exception(\\\\nException: Response content b\\\\\\'{\"error_code\":\"NOT_FOUND\",\"message\":\"Vector search endpoint f78b32e3-9a0f-4915-b045-21ad2edd3979 not found.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"d679f4a6-4a73-464c-9acd-755d7fd87ef6\",\"serving_data\":\"\"}]}\\\\\\', status_code 404\\\\n\\\\n request payload: {\\\\\\'messages\\\\\\': [{\\\\\\'role\\\\\\': \\\\\\'user\\\\\\', \\\\\\'content\\\\\\': \\\\\\'Who would be a good data scientist?\\\\\\'}]}\\'}'). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/09 04:03:36 INFO mlflow: Attempting to auto-detect Databricks resource dependencies for the current langchain model. Dependency auto-detection is best-effort and may not capture all dependencies of your langchain model, resulting in authorization errors when serving or querying your model. We recommend that you explicitly pass `resources` to mlflow.langchain.log_model() to ensure authorization to dependent resources succeeds when the model is deployed.\n2025/10/09 04:04:06 WARNING mlflow.utils.requirements_utils: Failed to run predict on input_example, dependencies introduced in predict are not captured.\nMlflowException('1 tasks failed. Errors: {0: \\'error: Exception(\\\\\\'Response content b\\\\\\\\\\\\\\'{\"error_code\":\"NOT_FOUND\",\"message\":\"Vector search endpoint f78b32e3-9a0f-4915-b045-21ad2edd3979 not found.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"77c176dd-707e-4397-84ff-a196910619dc\",\"serving_data\":\"\"}]}\\\\\\\\\\\\\\', status_code 404\\\\\\') Traceback (most recent call last):\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/utils.py\", line 159, in issue_request\\\\n    response.raise_for_status()\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/requests/models.py\", line 1026, in raise_for_status\\\\n    raise HTTPError(http_error_msg, response=self)\\\\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://centralus-c2.azuredatabricks.net/api/2.0/vector-search/indexes/dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked_index/query\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 199, in call_api\\\\n    response = self.single_call_api(callback_handlers)\\\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 143, in single_call_api\\\\n    response = self._predict_single_input(\\\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 114, in _predict_single_input\\\\n    return self.lc_model.invoke(single_input, config=config, **kwargs)\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3246, in invoke\\\\n    input_ = context.run(step.invoke, input_, config)\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 4001, in invoke\\\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\\\n                   ^^^^^^^^^^^^^^^\\\\n  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 456, in result\\\\n    return self.__get_result()\\\\n           ^^^^^^^^^^^^^^^^^^^\\\\n  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\\\\n    raise self._exception\\\\n  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\\\\n    result = self.fn(*self.args, **self.kwargs)\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3985, in _invoke_step\\\\n    return context.run(\\\\n           ^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3246, in invoke\\\\n    input_ = context.run(step.invoke, input_, config)\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/retrievers.py\", line 263, in invoke\\\\n    result = self._get_relevant_documents(\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/vectorstores/base.py\", line 1067, in _get_relevant_documents\\\\n    docs = self.vectorstore.similarity_search(query, **kwargs_)\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_community/vectorstores/databricks_vector_search.py\", line 315, in similarity_search\\\\n    docs_with_score = self.similarity_search_with_score(\\\\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_community/vectorstores/databricks_vector_search.py\", line 355, in similarity_search_with_score\\\\n    search_resp = self.index.similarity_search(\\\\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/index.py\", line 358, in similarity_search\\\\n    response = RequestUtils.issue_request(\\\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/utils.py\", line 161, in issue_request\\\\n    raise Exception(\\\\nException: Response content b\\\\\\'{\"error_code\":\"NOT_FOUND\",\"message\":\"Vector search endpoint f78b32e3-9a0f-4915-b045-21ad2edd3979 not found.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"77c176dd-707e-4397-84ff-a196910619dc\",\"serving_data\":\"\"}]}\\\\\\', status_code 404\\\\n\\\\n request payload: {\\\\\\'messages\\\\\\': [{\\\\\\'role\\\\\\': \\\\\\'user\\\\\\', \\\\\\'content\\\\\\': \\\\\\'Who would be a good data scientist?\\\\\\'}]}\\'}')Traceback (most recent call last):\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/mlflow/utils/_capture_modules.py\", line 166, in load_model_and_predict\n    model.predict(input_example, params=params)\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/mlflow/langchain/model.py\", line 687, in predict\n    return self._predict_with_callbacks(data, params, callback_handlers=callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/mlflow/langchain/model.py\", line 731, in _predict_with_callbacks\n    results = process_api_requests(\n              ^^^^^^^^^^^^^^^^^^^^^\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 289, in process_api_requests\n    raise mlflow.MlflowException(\nmlflow.exceptions.MlflowException: 1 tasks failed. Errors: {0: 'error: Exception(\\'Response content b\\\\\\'{\"error_code\":\"NOT_FOUND\",\"message\":\"Vector search endpoint f78b32e3-9a0f-4915-b045-21ad2edd3979 not found.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"77c176dd-707e-4397-84ff-a196910619dc\",\"serving_data\":\"\"}]}\\\\\\', status_code 404\\') Traceback (most recent call last):\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/utils.py\", line 159, in issue_request\\n    response.raise_for_status()\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/requests/models.py\", line 1026, in raise_for_status\\n    raise HTTPError(http_error_msg, response=self)\\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://centralus-c2.azuredatabricks.net/api/2.0/vector-search/indexes/dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked_index/query\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 199, in call_api\\n    response = self.single_call_api(callback_handlers)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 143, in single_call_api\\n    response = self._predict_single_input(\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 114, in _predict_single_input\\n    return self.lc_model.invoke(single_input, config=config, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3246, in invoke\\n    input_ = context.run(step.invoke, input_, config)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 4001, in invoke\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n                   ^^^^^^^^^^^^^^^\\n  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 456, in result\\n    return self.__get_result()\\n           ^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\\n    raise self._exception\\n  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3985, in _invoke_step\\n    return context.run(\\n           ^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3246, in invoke\\n    input_ = context.run(step.invoke, input_, config)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/retrievers.py\", line 263, in invoke\\n    result = self._get_relevant_documents(\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/vectorstores/base.py\", line 1067, in _get_relevant_documents\\n    docs = self.vectorstore.similarity_search(query, **kwargs_)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_community/vectorstores/databricks_vector_search.py\", line 315, in similarity_search\\n    docs_with_score = self.similarity_search_with_score(\\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_community/vectorstores/databricks_vector_search.py\", line 355, in similarity_search_with_score\\n    search_resp = self.index.similarity_search(\\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/index.py\", line 358, in similarity_search\\n    response = RequestUtils.issue_request(\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/utils.py\", line 161, in issue_request\\n    raise Exception(\\nException: Response content b\\'{\"error_code\":\"NOT_FOUND\",\"message\":\"Vector search endpoint f78b32e3-9a0f-4915-b045-21ad2edd3979 not found.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"77c176dd-707e-4397-84ff-a196910619dc\",\"serving_data\":\"\"}]}\\', status_code 404\\n\\n request payload: {\\'messages\\': [{\\'role\\': \\'user\\', \\'content\\': \\'Who would be a good data scientist?\\'}]}'}\n\u001B[31m2025/10/09 04:04:09 WARNING mlflow.models.model: Model logged without a signature. Signatures are required for Databricks UC model registry as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/3.4.0/model/signatures.html#how-to-set-signatures-on-models for instructions on setting signature on models.\u001B[0m\n/local_disk0/repl_tmp_data/ReplId-199ab-e2bbb-c/tmpffny6wsk/model/model.py:79: FutureWarning: set_retriever_schema is deprecated and will be removed in a future version. Please migrate to use VectorSearchRetrieverTool in the 'databricks-ai-bridge' package, or match the default schema so your retriever spans can be detected without requiring explicit configuration. See https://mlflow.org/docs/latest/genai/data-model/traces/#retriever-spans for more information.\n  mlflow.models.set_retriever_schema(\n2025/10/09 04:04:09 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-ae7082f6457a4fa4896e95aab0f89387\n2025/10/09 04:04:09 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n2025/10/09 04:04:10 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"messages\": [\n    {\n      \"role\": \"user\",\n    .... Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: 1 tasks failed. Errors: {0: 'error: Exception(\\'Response content b\\\\\\'{\"error_code\":\"NOT_FOUND\",\"message\":\"Vector search endpoint f78b32e3-9a0f-4915-b045-21ad2edd3979 not found.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"53404a0a-3917-4c15-9d74-f3770d35a42a\",\"serving_data\":\"\"}]}\\\\\\', status_code 404\\') Traceback (most recent call last):\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/utils.py\", line 159, in issue_request\\n    response.raise_for_status()\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/requests/models.py\", line 1026, in raise_for_status\\n    raise HTTPError(http_error_msg, response=self)\\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://centralus-c2.azuredatabricks.net/api/2.0/vector-search/indexes/dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked_index/query\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 199, in call_api\\n    response = self.single_call_api(callback_handlers)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 143, in single_call_api\\n    response = self._predict_single_input(\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/mlflow/langchain/api_request_parallel_processor.py\", line 114, in _predict_single_input\\n    return self.lc_model.invoke(single_input, config=config, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3246, in invoke\\n    input_ = context.run(step.invoke, input_, config)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 4001, in invoke\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n                   ^^^^^^^^^^^^^^^\\n  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 456, in result\\n    return self.__get_result()\\n           ^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\\n    raise self._exception\\n  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3985, in _invoke_step\\n    return context.run(\\n           ^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3246, in invoke\\n    input_ = context.run(step.invoke, input_, config)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/retrievers.py\", line 263, in invoke\\n    result = self._get_relevant_documents(\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/vectorstores/base.py\", line 1067, in _get_relevant_documents\\n    docs = self.vectorstore.similarity_search(query, **kwargs_)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_community/vectorstores/databricks_vector_search.py\", line 315, in similarity_search\\n    docs_with_score = self.similarity_search_with_score(\\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_community/vectorstores/databricks_vector_search.py\", line 355, in similarity_search_with_score\\n    search_resp = self.index.similarity_search(\\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/index.py\", line 358, in similarity_search\\n    response = RequestUtils.issue_request(\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/utils.py\", line 161, in issue_request\\n    raise Exception(\\nException: Response content b\\'{\"error_code\":\"NOT_FOUND\",\"message\":\"Vector search endpoint f78b32e3-9a0f-4915-b045-21ad2edd3979 not found.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"53404a0a-3917-4c15-9d74-f3770d35a42a\",\"serving_data\":\"\"}]}\\', status_code 404\\n\\n request payload: {\\'messages\\': [{\\'role\\': \\'user\\', \\'content\\': \\'Who would be a good data scientist?\\'}]}'}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7aeec56e7f34aa0b2c75ad4a941421a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/repl_tmp_data/ReplId-199ab-e2bbb-c/tmpupa_m3jc/model.py:79: FutureWarning: set_retriever_schema is deprecated and will be removed in a future version. Please migrate to use VectorSearchRetrieverTool in the 'databricks-ai-bridge' package, or match the default schema so your retriever spans can be detected without requiring explicit configuration. See https://mlflow.org/docs/latest/genai/data-model/traces/#retriever-spans for more information.\n  mlflow.models.set_retriever_schema(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-8c9f1677c2d779aca36881ce13df1a34\"",
      "text/plain": [
       "Trace(trace_id=tr-8c9f1677c2d779aca36881ce13df1a34)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/utils.py:159\u001B[0m, in \u001B[0;36mRequestUtils.issue_request\u001B[0;34m(url, method, token, params, json, verify, auth, data, headers)\u001B[0m\n",
       "\u001B[1;32m    158\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 159\u001B[0m     response\u001B[38;5;241m.\u001B[39mraise_for_status()\n",
       "\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/requests/models.py:1026\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1025\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n",
       "\u001B[0;32m-> 1026\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
       "\n",
       "\u001B[0;31mHTTPError\u001B[0m: 404 Client Error: Not Found for url: https://centralus-c2.azuredatabricks.net/api/2.0/vector-search/indexes/dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked_index/query\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7597083230090017>, line 14\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Test the chain locally to see the MLflow Trace\u001B[39;00m\n",
       "\u001B[1;32m     13\u001B[0m chain \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mlangchain\u001B[38;5;241m.\u001B[39mload_model(logged_chain_info\u001B[38;5;241m.\u001B[39mmodel_uri)\n",
       "\u001B[0;32m---> 14\u001B[0m chain\u001B[38;5;241m.\u001B[39minvoke(input_example)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py:3246\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   3244\u001B[0m                 input_ \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, input_, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   3245\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 3246\u001B[0m                 input_ \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, input_, config)\n",
       "\u001B[1;32m   3247\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n",
       "\u001B[1;32m   3248\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py:4001\u001B[0m, in \u001B[0;36mRunnableParallel.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   3996\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_executor_for_config(config) \u001B[38;5;28;01mas\u001B[39;00m executor:\n",
       "\u001B[1;32m   3997\u001B[0m         futures \u001B[38;5;241m=\u001B[39m [\n",
       "\u001B[1;32m   3998\u001B[0m             executor\u001B[38;5;241m.\u001B[39msubmit(_invoke_step, step, \u001B[38;5;28minput\u001B[39m, config, key)\n",
       "\u001B[1;32m   3999\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key, step \u001B[38;5;129;01min\u001B[39;00m steps\u001B[38;5;241m.\u001B[39mitems()\n",
       "\u001B[1;32m   4000\u001B[0m         ]\n",
       "\u001B[0;32m-> 4001\u001B[0m         output \u001B[38;5;241m=\u001B[39m {key: future\u001B[38;5;241m.\u001B[39mresult() \u001B[38;5;28;01mfor\u001B[39;00m key, future \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(steps, futures)}\n",
       "\u001B[1;32m   4002\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n",
       "\u001B[1;32m   4003\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:456\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n",
       "\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n",
       "\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n",
       "\u001B[0;32m--> 456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n",
       "\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    458\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:401\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n",
       "\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n",
       "\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m    403\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n",
       "\u001B[1;32m    404\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.12/concurrent/futures/thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
       "\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs)\n",
       "\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
       "\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py:3985\u001B[0m, in \u001B[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001B[0;34m(step, input_, config, key)\u001B[0m\n",
       "\u001B[1;32m   3979\u001B[0m child_config \u001B[38;5;241m=\u001B[39m patch_config(\n",
       "\u001B[1;32m   3980\u001B[0m     config,\n",
       "\u001B[1;32m   3981\u001B[0m     \u001B[38;5;66;03m# mark each step as a child run\u001B[39;00m\n",
       "\u001B[1;32m   3982\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39mrun_manager\u001B[38;5;241m.\u001B[39mget_child(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmap:key:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m   3983\u001B[0m )\n",
       "\u001B[1;32m   3984\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m set_config_context(child_config) \u001B[38;5;28;01mas\u001B[39;00m context:\n",
       "\u001B[0;32m-> 3985\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m context\u001B[38;5;241m.\u001B[39mrun(\n",
       "\u001B[1;32m   3986\u001B[0m         step\u001B[38;5;241m.\u001B[39minvoke,\n",
       "\u001B[1;32m   3987\u001B[0m         input_,\n",
       "\u001B[1;32m   3988\u001B[0m         child_config,\n",
       "\u001B[1;32m   3989\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py:3246\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   3244\u001B[0m                 input_ \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, input_, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   3245\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 3246\u001B[0m                 input_ \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, input_, config)\n",
       "\u001B[1;32m   3247\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n",
       "\u001B[1;32m   3248\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/retrievers.py:263\u001B[0m, in \u001B[0;36mBaseRetriever.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    261\u001B[0m kwargs_ \u001B[38;5;241m=\u001B[39m kwargs \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expects_other_args \u001B[38;5;28;01melse\u001B[39;00m {}\n",
       "\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new_arg_supported:\n",
       "\u001B[0;32m--> 263\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_relevant_documents(\n",
       "\u001B[1;32m    264\u001B[0m         \u001B[38;5;28minput\u001B[39m, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs_\n",
       "\u001B[1;32m    265\u001B[0m     )\n",
       "\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    267\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_relevant_documents(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs_)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:1067\u001B[0m, in \u001B[0;36mVectorStoreRetriever._get_relevant_documents\u001B[0;34m(self, query, run_manager, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1065\u001B[0m kwargs_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_kwargs \u001B[38;5;241m|\u001B[39m kwargs\n",
       "\u001B[1;32m   1066\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[0;32m-> 1067\u001B[0m     docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvectorstore\u001B[38;5;241m.\u001B[39msimilarity_search(query, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs_)\n",
       "\u001B[1;32m   1068\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity_score_threshold\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   1069\u001B[0m     docs_and_similarities \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m   1070\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvectorstore\u001B[38;5;241m.\u001B[39msimilarity_search_with_relevance_scores(\n",
       "\u001B[1;32m   1071\u001B[0m             query, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs_\n",
       "\u001B[1;32m   1072\u001B[0m         )\n",
       "\u001B[1;32m   1073\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_community/vectorstores/databricks_vector_search.py:315\u001B[0m, in \u001B[0;36mDatabricksVectorSearch.similarity_search\u001B[0;34m(self, query, k, filter, query_type, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    295\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msimilarity_search\u001B[39m(\n",
       "\u001B[1;32m    296\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m    297\u001B[0m     query: \u001B[38;5;28mstr\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    302\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n",
       "\u001B[1;32m    303\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Document]:\n",
       "\u001B[1;32m    304\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return docs most similar to query.\u001B[39;00m\n",
       "\u001B[1;32m    305\u001B[0m \n",
       "\u001B[1;32m    306\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    313\u001B[0m \u001B[38;5;124;03m        List of Documents most similar to the embedding.\u001B[39;00m\n",
       "\u001B[1;32m    314\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 315\u001B[0m     docs_with_score \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msimilarity_search_with_score(\n",
       "\u001B[1;32m    316\u001B[0m         query\u001B[38;5;241m=\u001B[39mquery,\n",
       "\u001B[1;32m    317\u001B[0m         k\u001B[38;5;241m=\u001B[39mk,\n",
       "\u001B[1;32m    318\u001B[0m         \u001B[38;5;28mfilter\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfilter\u001B[39m,\n",
       "\u001B[1;32m    319\u001B[0m         query_type\u001B[38;5;241m=\u001B[39mquery_type,\n",
       "\u001B[1;32m    320\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n",
       "\u001B[1;32m    321\u001B[0m     )\n",
       "\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [doc \u001B[38;5;28;01mfor\u001B[39;00m doc, _ \u001B[38;5;129;01min\u001B[39;00m docs_with_score]\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_community/vectorstores/databricks_vector_search.py:355\u001B[0m, in \u001B[0;36mDatabricksVectorSearch.similarity_search_with_score\u001B[0;34m(self, query, k, filter, query_type, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    353\u001B[0m         query_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    354\u001B[0m     query_vector \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings\u001B[38;5;241m.\u001B[39membed_query(query)\n",
       "\u001B[0;32m--> 355\u001B[0m search_resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39msimilarity_search(\n",
       "\u001B[1;32m    356\u001B[0m     columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns,\n",
       "\u001B[1;32m    357\u001B[0m     query_text\u001B[38;5;241m=\u001B[39mquery_text,\n",
       "\u001B[1;32m    358\u001B[0m     query_vector\u001B[38;5;241m=\u001B[39mquery_vector,\n",
       "\u001B[1;32m    359\u001B[0m     filters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfilter\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m _alias_filters(kwargs),\n",
       "\u001B[1;32m    360\u001B[0m     num_results\u001B[38;5;241m=\u001B[39mk,\n",
       "\u001B[1;32m    361\u001B[0m     query_type\u001B[38;5;241m=\u001B[39mquery_type,\n",
       "\u001B[1;32m    362\u001B[0m )\n",
       "\u001B[1;32m    363\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parse_search_response(search_resp)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/index.py:358\u001B[0m, in \u001B[0;36mVectorSearchIndex.similarity_search\u001B[0;34m(self, columns, query_text, query_vector, filters, num_results, debug_level, score_threshold, query_type, columns_to_rerank, disable_notice, reranker)\u001B[0m\n",
       "\u001B[1;32m    355\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    356\u001B[0m     query_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex_url\n",
       "\u001B[0;32m--> 358\u001B[0m response \u001B[38;5;241m=\u001B[39m RequestUtils\u001B[38;5;241m.\u001B[39missue_request(\n",
       "\u001B[1;32m    359\u001B[0m     url\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/query\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    360\u001B[0m     token\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_token_for_request(),\n",
       "\u001B[1;32m    361\u001B[0m     method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGET\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    362\u001B[0m     json\u001B[38;5;241m=\u001B[39mjson_data,\n",
       "\u001B[1;32m    363\u001B[0m )\n",
       "\u001B[1;32m    365\u001B[0m out_put \u001B[38;5;241m=\u001B[39m response\n",
       "\u001B[1;32m    366\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m response[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext_page_token\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/utils.py:161\u001B[0m, in \u001B[0;36mRequestUtils.issue_request\u001B[0;34m(url, method, token, params, json, verify, auth, data, headers)\u001B[0m\n",
       "\u001B[1;32m    159\u001B[0m     response\u001B[38;5;241m.\u001B[39mraise_for_status()\n",
       "\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[0;32m--> 161\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n",
       "\u001B[1;32m    162\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResponse content \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mcontent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, status_code \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    163\u001B[0m     )\n",
       "\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39mjson()\n",
       "\n",
       "\u001B[0;31mException\u001B[0m: Response content b'{\"error_code\":\"NOT_FOUND\",\"message\":\"Vector search endpoint f78b32e3-9a0f-4915-b045-21ad2edd3979 not found.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"2ad0fd9c-2e54-4ccf-bda0-73692e3a16bf\",\"serving_data\":\"\"}]}', status_code 404"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Exception",
        "evalue": "Response content b'{\"error_code\":\"NOT_FOUND\",\"message\":\"Vector search endpoint f78b32e3-9a0f-4915-b045-21ad2edd3979 not found.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"2ad0fd9c-2e54-4ccf-bda0-73692e3a16bf\",\"serving_data\":\"\"}]}', status_code 404"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Exception</span>: Response content b'{\"error_code\":\"NOT_FOUND\",\"message\":\"Vector search endpoint f78b32e3-9a0f-4915-b045-21ad2edd3979 not found.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"2ad0fd9c-2e54-4ccf-bda0-73692e3a16bf\",\"serving_data\":\"\"}]}', status_code 404\n[Trace ID: 00-e49d6cb1001c3ff637d69765b471b84f-f7bd406d5b2477d4-00]"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/utils.py:159\u001B[0m, in \u001B[0;36mRequestUtils.issue_request\u001B[0;34m(url, method, token, params, json, verify, auth, data, headers)\u001B[0m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 159\u001B[0m     response\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/requests/models.py:1026\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1025\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1026\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
        "\u001B[0;31mHTTPError\u001B[0m: 404 Client Error: Not Found for url: https://centralus-c2.azuredatabricks.net/api/2.0/vector-search/indexes/dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked_index/query",
        "\nDuring handling of the above exception, another exception occurred:\n",
        "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-7597083230090017>, line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Test the chain locally to see the MLflow Trace\u001B[39;00m\n\u001B[1;32m     13\u001B[0m chain \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mlangchain\u001B[38;5;241m.\u001B[39mload_model(logged_chain_info\u001B[38;5;241m.\u001B[39mmodel_uri)\n\u001B[0;32m---> 14\u001B[0m chain\u001B[38;5;241m.\u001B[39minvoke(input_example)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py:3246\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   3244\u001B[0m                 input_ \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, input_, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   3245\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3246\u001B[0m                 input_ \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, input_, config)\n\u001B[1;32m   3247\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   3248\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py:4001\u001B[0m, in \u001B[0;36mRunnableParallel.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   3996\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_executor_for_config(config) \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[1;32m   3997\u001B[0m         futures \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   3998\u001B[0m             executor\u001B[38;5;241m.\u001B[39msubmit(_invoke_step, step, \u001B[38;5;28minput\u001B[39m, config, key)\n\u001B[1;32m   3999\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key, step \u001B[38;5;129;01min\u001B[39;00m steps\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m   4000\u001B[0m         ]\n\u001B[0;32m-> 4001\u001B[0m         output \u001B[38;5;241m=\u001B[39m {key: future\u001B[38;5;241m.\u001B[39mresult() \u001B[38;5;28;01mfor\u001B[39;00m key, future \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(steps, futures)}\n\u001B[1;32m   4002\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   4003\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
        "File \u001B[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:456\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m--> 456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    458\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
        "File \u001B[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:401\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[1;32m    404\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "File \u001B[0;32m/usr/lib/python3.12/concurrent/futures/thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs)\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py:3985\u001B[0m, in \u001B[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001B[0;34m(step, input_, config, key)\u001B[0m\n\u001B[1;32m   3979\u001B[0m child_config \u001B[38;5;241m=\u001B[39m patch_config(\n\u001B[1;32m   3980\u001B[0m     config,\n\u001B[1;32m   3981\u001B[0m     \u001B[38;5;66;03m# mark each step as a child run\u001B[39;00m\n\u001B[1;32m   3982\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39mrun_manager\u001B[38;5;241m.\u001B[39mget_child(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmap:key:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   3983\u001B[0m )\n\u001B[1;32m   3984\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m set_config_context(child_config) \u001B[38;5;28;01mas\u001B[39;00m context:\n\u001B[0;32m-> 3985\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m context\u001B[38;5;241m.\u001B[39mrun(\n\u001B[1;32m   3986\u001B[0m         step\u001B[38;5;241m.\u001B[39minvoke,\n\u001B[1;32m   3987\u001B[0m         input_,\n\u001B[1;32m   3988\u001B[0m         child_config,\n\u001B[1;32m   3989\u001B[0m     )\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/runnables/base.py:3246\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   3244\u001B[0m                 input_ \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, input_, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   3245\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3246\u001B[0m                 input_ \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, input_, config)\n\u001B[1;32m   3247\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   3248\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/retrievers.py:263\u001B[0m, in \u001B[0;36mBaseRetriever.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    261\u001B[0m kwargs_ \u001B[38;5;241m=\u001B[39m kwargs \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expects_other_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new_arg_supported:\n\u001B[0;32m--> 263\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_relevant_documents(\n\u001B[1;32m    264\u001B[0m         \u001B[38;5;28minput\u001B[39m, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs_\n\u001B[1;32m    265\u001B[0m     )\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    267\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_relevant_documents(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs_)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:1067\u001B[0m, in \u001B[0;36mVectorStoreRetriever._get_relevant_documents\u001B[0;34m(self, query, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m   1065\u001B[0m kwargs_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_kwargs \u001B[38;5;241m|\u001B[39m kwargs\n\u001B[1;32m   1066\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 1067\u001B[0m     docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvectorstore\u001B[38;5;241m.\u001B[39msimilarity_search(query, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs_)\n\u001B[1;32m   1068\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msearch_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimilarity_score_threshold\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1069\u001B[0m     docs_and_similarities \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1070\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvectorstore\u001B[38;5;241m.\u001B[39msimilarity_search_with_relevance_scores(\n\u001B[1;32m   1071\u001B[0m             query, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs_\n\u001B[1;32m   1072\u001B[0m         )\n\u001B[1;32m   1073\u001B[0m     )\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_community/vectorstores/databricks_vector_search.py:315\u001B[0m, in \u001B[0;36mDatabricksVectorSearch.similarity_search\u001B[0;34m(self, query, k, filter, query_type, **kwargs)\u001B[0m\n\u001B[1;32m    295\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msimilarity_search\u001B[39m(\n\u001B[1;32m    296\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    297\u001B[0m     query: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    302\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    303\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Document]:\n\u001B[1;32m    304\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return docs most similar to query.\u001B[39;00m\n\u001B[1;32m    305\u001B[0m \n\u001B[1;32m    306\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;124;03m        List of Documents most similar to the embedding.\u001B[39;00m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 315\u001B[0m     docs_with_score \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msimilarity_search_with_score(\n\u001B[1;32m    316\u001B[0m         query\u001B[38;5;241m=\u001B[39mquery,\n\u001B[1;32m    317\u001B[0m         k\u001B[38;5;241m=\u001B[39mk,\n\u001B[1;32m    318\u001B[0m         \u001B[38;5;28mfilter\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfilter\u001B[39m,\n\u001B[1;32m    319\u001B[0m         query_type\u001B[38;5;241m=\u001B[39mquery_type,\n\u001B[1;32m    320\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    321\u001B[0m     )\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [doc \u001B[38;5;28;01mfor\u001B[39;00m doc, _ \u001B[38;5;129;01min\u001B[39;00m docs_with_score]\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/langchain_community/vectorstores/databricks_vector_search.py:355\u001B[0m, in \u001B[0;36mDatabricksVectorSearch.similarity_search_with_score\u001B[0;34m(self, query, k, filter, query_type, **kwargs)\u001B[0m\n\u001B[1;32m    353\u001B[0m         query_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    354\u001B[0m     query_vector \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings\u001B[38;5;241m.\u001B[39membed_query(query)\n\u001B[0;32m--> 355\u001B[0m search_resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39msimilarity_search(\n\u001B[1;32m    356\u001B[0m     columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns,\n\u001B[1;32m    357\u001B[0m     query_text\u001B[38;5;241m=\u001B[39mquery_text,\n\u001B[1;32m    358\u001B[0m     query_vector\u001B[38;5;241m=\u001B[39mquery_vector,\n\u001B[1;32m    359\u001B[0m     filters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfilter\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m _alias_filters(kwargs),\n\u001B[1;32m    360\u001B[0m     num_results\u001B[38;5;241m=\u001B[39mk,\n\u001B[1;32m    361\u001B[0m     query_type\u001B[38;5;241m=\u001B[39mquery_type,\n\u001B[1;32m    362\u001B[0m )\n\u001B[1;32m    363\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parse_search_response(search_resp)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/index.py:358\u001B[0m, in \u001B[0;36mVectorSearchIndex.similarity_search\u001B[0;34m(self, columns, query_text, query_vector, filters, num_results, debug_level, score_threshold, query_type, columns_to_rerank, disable_notice, reranker)\u001B[0m\n\u001B[1;32m    355\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    356\u001B[0m     query_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex_url\n\u001B[0;32m--> 358\u001B[0m response \u001B[38;5;241m=\u001B[39m RequestUtils\u001B[38;5;241m.\u001B[39missue_request(\n\u001B[1;32m    359\u001B[0m     url\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/query\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    360\u001B[0m     token\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_token_for_request(),\n\u001B[1;32m    361\u001B[0m     method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGET\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    362\u001B[0m     json\u001B[38;5;241m=\u001B[39mjson_data,\n\u001B[1;32m    363\u001B[0m )\n\u001B[1;32m    365\u001B[0m out_put \u001B[38;5;241m=\u001B[39m response\n\u001B[1;32m    366\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m response[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext_page_token\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-88c91d3a-838e-430e-aea5-d5232d32cf7a/lib/python3.12/site-packages/databricks/vector_search/utils.py:161\u001B[0m, in \u001B[0;36mRequestUtils.issue_request\u001B[0;34m(url, method, token, params, json, verify, auth, data, headers)\u001B[0m\n\u001B[1;32m    159\u001B[0m     response\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 161\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[1;32m    162\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResponse content \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mcontent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, status_code \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    163\u001B[0m     )\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39mjson()\n",
        "\u001B[0;31mException\u001B[0m: Response content b'{\"error_code\":\"NOT_FOUND\",\"message\":\"Vector search endpoint f78b32e3-9a0f-4915-b045-21ad2edd3979 not found.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"2ad0fd9c-2e54-4ccf-bda0-73692e3a16bf\",\"serving_data\":\"\"}]}', status_code 404"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Log the model to MLflow\n",
    "with mlflow.start_run(run_name=\"databricks-docs-bot\"):\n",
    "    logged_chain_info = mlflow.langchain.log_model(\n",
    "        lc_model=lc_model[0],  # Chain code file from the quick start repo\n",
    "        model_config=chain_config,  # Chain configuration set above\n",
    "        artifact_path=\"chain\",  # Required by MLflow\n",
    "        input_example=input_example,  # Save the chain's input schema.  MLflow will execute the chain before logging & capture it's output schema.\n",
    "    )\n",
    "\n",
    "# Test the chain locally to see the MLflow Trace\n",
    "chain = mlflow.langchain.load_model(logged_chain_info.model_uri)\n",
    "chain.invoke(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b9ebb2-465e-4204-afa6-b657d09134ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "py4j.Py4JException: Error while obtaining a new communication channel\n",
       "\tat py4j.CallbackClient.getConnectionLock(CallbackClient.java:257)\n",
       "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:377)\n",
       "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
       "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
       "\tat jdk.proxy23/jdk.proxy23.$Proxy217.updateWsfsWorkingDir(Unknown Source)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.$anonfun$setWsfsWorkingDir$3(PythonDriverLocalBase.scala:444)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)\n",
       "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)\n",
       "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n",
       "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n",
       "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1193)\n",
       "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1666)\n",
       "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1633)\n",
       "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\n",
       "Caused by: java.net.ConnectException: Connection refused\n",
       "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
       "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
       "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
       "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
       "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
       "\tat java.base/java.net.Socket.connect(Socket.java:639)\n",
       "\tat java.base/java.net.Socket.connect(Socket.java:588)\n",
       "\tat java.base/java.net.Socket.<init>(Socket.java:512)\n",
       "\tat java.base/java.net.Socket.<init>(Socket.java:324)\n",
       "\tat java.base/javax.net.DefaultSocketFactory.createSocket(SocketFactory.java:277)\n",
       "\tat py4j.PythonClient.startClientSocket(PythonClient.java:192)\n",
       "\tat py4j.PythonClient.getConnection(PythonClient.java:213)\n",
       "\tat py4j.CallbackClient.getConnectionLock(CallbackClient.java:250)\n",
       "\t... 14 more"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Py4JException: Error while obtaining a new communication channel"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "py4j.Py4JException: Error while obtaining a new communication channel",
        "\tat py4j.CallbackClient.getConnectionLock(CallbackClient.java:257)",
        "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:377)",
        "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)",
        "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)",
        "\tat jdk.proxy23/jdk.proxy23.$Proxy217.updateWsfsWorkingDir(Unknown Source)",
        "\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.$anonfun$setWsfsWorkingDir$3(PythonDriverLocalBase.scala:444)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)",
        "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)",
        "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)",
        "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)",
        "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)",
        "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1193)",
        "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1666)",
        "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1633)",
        "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)",
        "Caused by: java.net.ConnectException: Connection refused",
        "\tat java.base/sun.nio.ch.Net.connect0(Native Method)",
        "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)",
        "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)",
        "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)",
        "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)",
        "\tat java.base/java.net.Socket.connect(Socket.java:639)",
        "\tat java.base/java.net.Socket.connect(Socket.java:588)",
        "\tat java.base/java.net.Socket.<init>(Socket.java:512)",
        "\tat java.base/java.net.Socket.<init>(Socket.java:324)",
        "\tat java.base/javax.net.DefaultSocketFactory.createSocket(SocketFactory.java:277)",
        "\tat py4j.PythonClient.startClientSocket(PythonClient.java:192)",
        "\tat py4j.PythonClient.getConnection(PythonClient.java:213)",
        "\tat py4j.CallbackClient.getConnectionLock(CallbackClient.java:250)",
        "\t... 14 more"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "chain_config = {\n",
    "    \"llm_model_serving_endpoint_name\": \"poc-openai-completions-endpoint\",  # the foundation model we want to use\n",
    "    \"vector_search_endpoint_name\": VECTOR_SEARCH_ENDPOINT,  # Endoint for vector search\n",
    "    \"vector_search_index\": f\"{CHUNKS_VECTOR_INDEX}\",\n",
    "    \"llm_prompt_template\": \"\"\"you are a manager hiring a Data Scientist. Use the following pieces of retrieved context to answer the question. Some pieces of context may be irrelevant, in which case you should not use them to form the answer.\\n\\nContext: {context}\"\"\", # LLM Prompt template\n",
    "}\n",
    "\n",
    "# Here, we define an input example in the schema required by Agent Framework\n",
    "input_example = {\"messages\": [ {\"role\": \"user\", \"content\": \"who is the best candidate for data scientist role for blue shield of ca?\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d8ff9b-2c69-409c-87f6-9237771acffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "CHUNKS_DELTA_TABLE": "resumes_chunked_p4",
        "Catalog_Name": "dbc_adv_anlaytics_dev",
        "Embeddings_Model": "databricks-bge-large-en",
        "Persisted_UC_Table_Location": "dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked",
        "Source_Documents": "/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/Resumes_10.8",
        "UC_CATALOG": "dbc_adv_anlaytics_dev",
        "UC_SCHEMA": "yh_agent_resume",
        "UC_Volume_Path": "/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume",
        "Vector_Index": "dbc_adv_anlaytics_dev.yh_agent_resume.resumes_chunked_index_p4",
        "Vector_Search_Endpoint": "yhu01_resume_search",
        "Vse_Schema_Name": "yh_agent_resume"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the chain locally to see the MLflow Trace\n",
    "chain = mlflow.langchain.load_model(logged_chain_info.model_uri)\n",
    "chain.invoke(input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "809ecb5e-f853-4afc-b665-4aa09422c5ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2.1/ Deploy the application\n",
    "Now, we:\n",
    "\n",
    "Register the application in Unity Catalog\n",
    "Use Agent Framework to deploy to the Quality Lab review application\n",
    "Along side the review ap, a scalable, production-ready Model Serving endpoint is also deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b84a9bf9-f933-4a89-b746-23ead8c66802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "CHUNKS_DELTA_TABLE": "resumes_chunked_p4",
        "Catalog_Name": "dbc_adv_anlaytics_dev",
        "Embeddings_Model": "databricks-bge-large-en",
        "Persisted_UC_Table_Location": "dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked",
        "Source_Documents": "/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/Resumes_10.8",
        "UC_CATALOG": "dbc_adv_anlaytics_dev",
        "UC_SCHEMA": "yh_agent_resume",
        "UC_Volume_Path": "/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume",
        "Vector_Index": "dbc_adv_anlaytics_dev.yh_agent_resume.resumes_chunked_index_p4",
        "Vector_Search_Endpoint": "yhu01_resume_search",
        "Vse_Schema_Name": "yh_agent_resume"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Use the current user name to create any necesary resources\n",
    "w = WorkspaceClient()\n",
    "user_name = w.current_user.me().user_name.split(\"@\")[0].replace(\".\", \"\")\n",
    "\n",
    "# UC Catalog & Schema where outputs tables/indexs are saved\n",
    "# If this catalog/schema does not exist, you need create catalog/schema permissions.\n",
    "UC_CATALOG = f'dbc_adv_anlaytics_dev'\n",
    "UC_SCHEMA = f'yh_agent_resume'\n",
    "\n",
    "# UC Model name where the POC chain is logged\n",
    "UC_MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.chat_bot_updated_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "# Vector Search endpoint where index is loaded\n",
    "# If this does not exist, it will be created\n",
    "VECTOR_SEARCH_ENDPOINT = f'{user_name}_resume_search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b65b51d5-8d2e-46ee-a1f7-d270fd0d572d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "CHUNKS_DELTA_TABLE": "resumes_chunked_p4",
        "Catalog_Name": "dbc_adv_anlaytics_dev",
        "Embeddings_Model": "databricks-bge-large-en",
        "Persisted_UC_Table_Location": "dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked",
        "Source_Documents": "/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/Resumes_10.8",
        "UC_CATALOG": "dbc_adv_anlaytics_dev",
        "UC_SCHEMA": "yh_agent_resume",
        "UC_Volume_Path": "/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume",
        "Vector_Index": "dbc_adv_anlaytics_dev.yh_agent_resume.resumes_chunked_index_p4",
        "Vector_Search_Endpoint": "yhu01_resume_search",
        "Vse_Schema_Name": "yh_agent_resume"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "UC_MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d954116-2369-4aa5-a37e-6f9f1492f852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "CHUNKS_DELTA_TABLE": "resumes_chunked_p4",
        "Catalog_Name": "dbc_adv_anlaytics_dev",
        "Embeddings_Model": "databricks-bge-large-en",
        "Persisted_UC_Table_Location": "dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked",
        "Source_Documents": "/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/Resumes_10.8",
        "UC_CATALOG": "dbc_adv_anlaytics_dev",
        "UC_SCHEMA": "yh_agent_resume",
        "UC_Volume_Path": "/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume",
        "Vector_Index": "dbc_adv_anlaytics_dev.yh_agent_resume.resumes_chunked_index_p4",
        "Vector_Search_Endpoint": "yhu01_resume_search",
        "Vse_Schema_Name": "yh_agent_resume"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from databricks import agents\n",
    "import time\n",
    "from databricks.sdk.service.serving import EndpointStateReady, EndpointStateConfigUpdate\n",
    "\n",
    "# Use Unity Catalog to log the chain\n",
    "import mlflow\n",
    "\n",
    "# Register the chain to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=UC_MODEL_NAME)\n",
    "\n",
    "# Deploy to enable the Review APP and create an API endpoint\n",
    "deployment_info = agents.deploy(model_name=UC_MODEL_NAME, model_version=uc_registered_model_info.version)\n",
    "\n",
    "# Wait for the Review App to be ready\n",
    "print(\"\\nWaiting for endpoint to deploy.  This can take 10 - 20 minutes.\", end=\"\")\n",
    "while w.serving_endpoints.get(deployment_info.endpoint_name).state.ready == EndpointStateReady.NOT_READY or w.serving_endpoints.get(deployment_info.endpoint_name).state.config_update == EndpointStateConfigUpdate.IN_PROGRESS:\n",
    "    print(\".\", end=\"\")\n",
    "    time.sleep(30)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7597083230089993,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BSC_Resume_Reviewer_P4_10.08",
   "widgets": {
    "CHUNKS_DELTA_TABLE": {
     "currentValue": "resumes_chunked_p4",
     "nuid": "2b371288-3ccd-4e1c-b5ad-f1c404134e6b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "resumes_chunked_p4",
      "label": null,
      "name": "CHUNKS_DELTA_TABLE",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "resumes_chunked_p4",
      "label": null,
      "name": "CHUNKS_DELTA_TABLE",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Catalog_Name": {
     "currentValue": "dbc_adv_anlaytics_dev",
     "nuid": "69075ec1-cb9c-43f6-8115-ec73a660c45e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbc_adv_anlaytics_dev",
      "label": null,
      "name": "Catalog_Name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbc_adv_anlaytics_dev",
      "label": null,
      "name": "Catalog_Name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Embeddings_Model": {
     "currentValue": "databricks-bge-large-en",
     "nuid": "1acca840-0316-432b-99e2-930be9fbb985",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-bge-large-en",
      "label": null,
      "name": "Embeddings_Model",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks-bge-large-en",
      "label": null,
      "name": "Embeddings_Model",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Persisted_UC_Table_Location": {
     "currentValue": "dbc_adv_anlaytics_dev.yh_agent_resume.p4_resumes_chunked",
     "nuid": "11df1c01-59eb-44c6-99a5-20027c8d59c6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbc_adv_anlaytics_dev.yh_agent_resume.resumes_chunked_p4",
      "label": null,
      "name": "Persisted_UC_Table_Location",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbc_adv_anlaytics_dev.yh_agent_resume.resumes_chunked_p4",
      "label": null,
      "name": "Persisted_UC_Table_Location",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Source_Documents": {
     "currentValue": "/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/Resumes_10.8",
     "nuid": "19b4182a-e11a-4a53-9be5-1c8bc18715dd",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/resumes_9.22",
      "label": null,
      "name": "Source_Documents",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Workspace/Users/yhu01@blueshieldca.com/adb_rag_resume/resumes_9.22",
      "label": null,
      "name": "Source_Documents",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "UC_CATALOG": {
     "currentValue": "dbc_adv_anlaytics_dev",
     "nuid": "9dbd8c02-aeea-41d8-ab0b-eead85b214bf",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbc_adv_anlaytics_dev",
      "label": null,
      "name": "UC_CATALOG",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbc_adv_anlaytics_dev",
      "label": null,
      "name": "UC_CATALOG",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "UC_SCHEMA": {
     "currentValue": "yh_agent_resume",
     "nuid": "8ecaabcc-ff78-493d-b277-fbf732e5a20d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "yh_agent_resume",
      "label": null,
      "name": "UC_SCHEMA",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "yh_agent_resume",
      "label": null,
      "name": "UC_SCHEMA",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "UC_Volume_Path": {
     "currentValue": "/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume",
     "nuid": "44bfb4ab-d674-4a43-8460-839726b76eee",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume",
      "label": null,
      "name": "UC_Volume_Path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Volumes/dbc_adv_anlaytics_dev/yh_agent_resume/p4_resume_volume",
      "label": null,
      "name": "UC_Volume_Path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Vector_Index": {
     "currentValue": "dbc_adv_anlaytics_dev.yh_agent_resume.resumes_chunked_index_p4",
     "nuid": "37bb0462-9f87-4797-b43c-b1372b84ab3a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbc_adv_anlaytics_dev.yh_agent_resume.resumes_chunked_index_p4",
      "label": null,
      "name": "Vector_Index",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbc_adv_anlaytics_dev.yh_agent_resume.resumes_chunked_index_p4",
      "label": null,
      "name": "Vector_Index",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Vector_Search_Endpoint": {
     "currentValue": "yhu01_resume_search",
     "nuid": "73d301fd-77a1-4507-a693-c687c2d204df",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "yhu01_resume_search",
      "label": null,
      "name": "Vector_Search_Endpoint",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "yhu01_resume_search",
      "label": null,
      "name": "Vector_Search_Endpoint",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Vse_Schema_Name": {
     "currentValue": "yh_agent_resume",
     "nuid": "874a0a1b-060e-477f-bc79-5532e5775b78",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "yh_agent_resume",
      "label": null,
      "name": "Vse_Schema_Name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "yh_agent_resume",
      "label": null,
      "name": "Vse_Schema_Name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}