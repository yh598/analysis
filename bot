SELECT
    current_dt,
    MEME_CK,
    cspd_cat,
    mepe_eff_dt,
    mepe_term_dt,
    grgr_ck,
    MEPE_ELIG_IND,
    SSBB_CK,
    MEME_SFX,
    HASH(MEME_FIRST_NAME) AS MEME_FIRST_NAME,
    HASH(MEME_MID_INIT) AS MEME_MID_INIT,
    HASH(MEME_LAST_NAME) AS MEME_LAST_NAME,
    HASH(MEME_SFX) AS MEME_SFX,
    REGEXP_REPLACE(TO_VARCHAR(MEME_BIRTH_DT), '[0-9]', 'X') AS MEME_BIRTH_DT,
    AGE,
    HASH(MEME_MARITAL_STATUS) AS MEME_MARITAL_STATUS,
    HASH(SSBB_ID) AS SSBB_ID,
    GRGR_ID,
    GRGR_NAME,
    ECM_IND,
    RISK_SCORE,
    RISK_FACTORS,
    MATERNAL_TYPE
FROM stage_adb.pregnancy.medi_cal_maternal_risk
LIMIT 15;

import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import ast

# ================================
# 1. LOAD ORIGINAL FILE
# ================================

input_path = "do_params_dmd_loc.parquet"
df = pd.read_parquet(input_path)

orig_cols = df.columns.tolist()
orig_schema = df.dtypes.to_dict()


# ================================
# 2. ENSURE LIST FORMAT
# ================================

def ensure_list(x):
    if isinstance(x, str):
        try:
            return ast.literal_eval(x)
        except:
            return x
    return x

df["fulfill_nodes_priority"] = df["fulfill_nodes_priority"].apply(ensure_list)


# ================================
# 3. RULE 1: Duplicate 1032_DL → 1017_DL
# ================================

rows_1032 = df[df["dmd_loc_cd"] == "1032_DL"].copy()
rows_1032["dmd_loc_cd"] = "1017_DL"
df = pd.concat([df, rows_1032], ignore_index=True)


# ================================
# 4. RULE 2: Update fulfill_nodes_priority
# ================================

def update_priority(row):
    arr = row["fulfill_nodes_priority"]

    # standardize all to string
    arr = [str(x) for x in arr]

    # remove existing '1017'
    arr = [x for x in arr if x != "1017"]

    # NEW LOGIC:
    # If 1017_DL → insert at FRONT
    if row["dmd_loc_cd"] == "1017_DL":
        arr = ["1017"] + arr
    else:
        # If not 1017_DL → append 1017 to END
        arr = arr + ["1017"]

    return arr

df["fulfill_nodes_priority"] = df.apply(update_priority, axis=1)


# ================================
# 5. RULE 3: Update ss_node_by_seg
# ================================

def update_seg(row):
    seg = row["ss_node_by_seg"]
    if isinstance(seg, dict) and row["dmd_loc_cd"] == "1017_DL":
        seg = dict(seg)
        seg["B"] = "1017"
        seg["L"] = "1032"
        seg["T"] = "1032"
    return seg

df["ss_node_by_seg"] = df.apply(update_seg, axis=1)


# ================================
# 6. RESTORE ORIGINAL COLUMN ORDER & SCHEMA
# ================================

df = df[orig_cols]

for col, dtype in orig_schema.items():
    try:
        df[col] = df[col].astype(dtype)
    except:
        pass


# ================================
# 7. SAVE OUTPUT
# ================================

output_path = "output_files/do_params_dmd_loc.parquet"
table = pa.Table.from_pandas(df, preserve_index=False)
pq.write_table(table, output_path)

print("Saved updated do_params_dmd_loc.parquet with new logic.")


# ================================
# 8. OPTIONAL QUICK VERIFY
# ================================

df_verify = pd.read_parquet(output_path)

print("\nCheck 1017_DL rows:")
print(df_verify[df_verify["dmd_loc_cd"]=="1017_DL"][["dmd_loc_cd","fulfill_nodes_priority"]].head())

print("\nCheck NON-1017 rows to confirm 1017 appended:")
print(df_verify[df_verify["dmd_loc_cd"]!="1017_DL"][["dmd_loc_cd","fulfill_nodes_priority"]].head())



import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import ast

# Load original DF (once!)
input_path = "do_params_dmd_loc.parquet"
df = pd.read_parquet(input_path)

orig_cols = df.columns.tolist()
orig_schema = df.dtypes.to_dict()

def ensure_list(x):
    if isinstance(x, str):
        try:
            return ast.literal_eval(x)
        except:
            return x
    return x

df["fulfill_nodes_priority"] = df["fulfill_nodes_priority"].apply(ensure_list)

# RULE 1: duplicate 1032_DL → 1017_DL
rows_1032 = df[df["dmd_loc_cd"] == "1032_DL"].copy()
rows_1032["dmd_loc_cd"] = "1017_DL"
df = pd.concat([df, rows_1032], ignore_index=True)

# RULE 2: update fulfill_nodes_priority
def update_priority(row):
    arr = row["fulfill_nodes_priority"]
    if isinstance(arr, list):

        # remove ANY existing "1017"
        arr = [str(x) for x in arr if str(x) != "1017"]

        # insert 1017 at front if 1017_DL
        if row["dmd_loc_cd"] == "1017_DL":
            arr = ["1017"] + arr

    return arr

df["fulfill_nodes_priority"] = df.apply(update_priority, axis=1)

# RULE 3: update ss_node_by_seg
def update_seg(row):
    seg = row["ss_node_by_seg"]
    if isinstance(seg, dict) and row["dmd_loc_cd"] == "1017_DL":
        seg = dict(seg)
        seg["B"] = "1017"
        seg["L"] = "1032"
        seg["T"] = "1032"
    return seg

df["ss_node_by_seg"] = df.apply(update_seg, axis=1)

# Restore column order + schema
df = df[orig_cols]

for col, dtype in orig_schema.items():
    try:
        df[col] = df[col].astype(dtype)
    except:
        pass

# Save output
output_path = "output_files/do_params_dmd_loc.parquet"
table = pa.Table.from_pandas(df, preserve_index=False)
pq.write_table(table, output_path)

print("DONE. Saved:", output_path)



import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import ast

# ---------------------------------------
# LOAD ORIGINAL FILE + SAVE ORIGINAL SCHEMA
# ---------------------------------------
input_path = "do_params_dmd_loc.parquet"
df = pd.read_parquet(input_path)

orig_cols = df.columns.tolist()
orig_schema = df.dtypes.to_dict()

# ---------------------------------------
# Convert string arrays -> real lists
# ---------------------------------------
def ensure_list(x):
    if isinstance(x, str):
        try:
            return ast.literal_eval(x)
        except:
            return x
    return x

df["fulfill_nodes_priority"] = df["fulfill_nodes_priority"].apply(ensure_list)

# ---------------------------------------
# RULE 1: Duplicate DL_1032 → DL_1017 (temp)
# ---------------------------------------
rows_1032 = df[df["dmd_loc_cd"] == "DL_1032"].copy()
rows_1032["dmd_loc_cd"] = "DL_1017"   # temporary name before final rename
df = pd.concat([df, rows_1032], ignore_index=True)


# ---------------------------------------
# RULE 2: Modify fulfill_nodes_priority
# ---------------------------------------
def update_priority(row):
    arr = row["fulfill_nodes_priority"]

    if isinstance(arr, list):
        # remove any existing 1017
        arr = [x for x in arr if x != 1017]

        # insert depending on location
        if row["dmd_loc_cd"] == "DL_1017":
            return [1017] + arr
        else:
            return arr + [1017]

    return arr

df["fulfill_nodes_priority"] = df.apply(update_priority, axis=1)


# ---------------------------------------
# RULE 3: Update ss_node_by_seg
# ---------------------------------------
def update_seg(row):
    seg = row["ss_node_by_seg"]
    loc = row["dmd_loc_cd"]

    if isinstance(seg, dict) and loc == "DL_1017":
        s = dict(seg)
        s["B"] = "1017"
        s["L"] = "1032"
        s["T"] = "1032"
        return s

    return seg

df["ss_node_by_seg"] = df.apply(update_seg, axis=1)


# ---------------------------------------
# RULE 4: Rename dmd_loc_cd from DL_1017 → 1017_DL
# ---------------------------------------
df["dmd_loc_cd"] = df["dmd_loc_cd"].replace({"DL_1017": "1017_DL"})


# ---------------------------------------
# RESTORE ORIGINAL COLUMN ORDER + DTYPES
# ---------------------------------------
df = df[orig_cols]

for col, dtype in orig_schema.items():
    try:
        df[col] = df[col].astype(dtype)
    except Exception:
        # for object, list, dict keep original format
        pass


# ---------------------------------------
# SAVE OUTPUT
# ---------------------------------------
output_path = "output_files/do_params_dmd_loc.parquet"
table = pa.Table.from_pandas(df, preserve_index=False)
pq.write_table(table, output_path)

print("Saved updated do_params_dmd_loc with original schema and formats.")
print("Output:", output_path)

import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

# ---------------------------------------
# LOAD ORIGINAL + STORE ORIGINAL SCHEMA
# ---------------------------------------
input_path = "do_params_dmd_loc.parquet"
df = pd.read_parquet(input_path)

# Capture original column order and dtypes
orig_cols = df.columns.tolist()
orig_schema = df.dtypes.to_dict()

# ---------------------------------------
# RULE 1: Duplicate DL_1032 -> DL_1017
# ---------------------------------------
rows_1032 = df[df["dmd_loc_cd"] == "DL_1032"].copy()
rows_1032["dmd_loc_cd"] = "DL_1017"

df = pd.concat([df, rows_1032], ignore_index=True)

# ---------------------------------------
# RULE 2: Update fulfill_nodes_priority safely
# ---------------------------------------
def update_priority(row):
    arr = row["fulfill_nodes_priority"]

    # Guarantee list format stays the same
    if isinstance(arr, list):
        arr = [x for x in arr if x != 1017]  # remove duplicates if any
        if row["dmd_loc_cd"] == "DL_1017":
            return [1017] + arr
        else:
            return arr + [1017]
    return arr

df["fulfill_nodes_priority"] = df.apply(update_priority, axis=1)

# ---------------------------------------
# RULE 3: Update ss_node_by_seg safely
# ---------------------------------------
def update_seg(row):
    seg = row["ss_node_by_seg"]
    loc = row["dmd_loc_cd"]
    if isinstance(seg, dict) and loc == "DL_1017":
        s = dict(seg)  # make a copy
        s["B"] = "1017"
        s["L"] = "1032"
        s["T"] = "1032"
        return s
    return seg

df["ss_node_by_seg"] = df.apply(update_seg, axis=1)

# ---------------------------------------
# ENFORCE ORIGINAL COLUMN ORDER + DTYPES
# ---------------------------------------

# Restore column order
df = df[orig_cols]

# Cast back to original dtypes (especially important for int, datetime, object)
for col, dtype in orig_schema.items():
    try:
        df[col] = df[col].astype(dtype)
    except Exception:
        # For list/dict/object columns, ignore dtype errors
        pass

# ---------------------------------------
# SAVE USING THE ORIGINAL PARQUET SCHEMA
# ---------------------------------------

output_path = "output_files/do_params_dmd_loc.parquet"
table = pa.Table.from_pandas(df, preserve_index=False)
pq.write_table(table, output_path)

print("Saved with original schema and column formats:")
print(output_path)


import os
import pandas as pd

# ---------------------------------
# CONFIG
# ---------------------------------
INPUT_DIR = "./"                   # folder with parquet files
OUTPUT_DIR = "./output_files"      # folder to save updated parquet files

OLD_NODE = "1032"                  # treat node codes as strings
NEW_NODE = "1017"

# Ensure output directory exists
os.makedirs(OUTPUT_DIR, exist_ok=True)


# ---------------------------------
# FUNCTION TO PROCESS A FILE
# ---------------------------------
def update_parquet(file_path):
    print(f"Processing: {file_path}")

    # Load parquet file
    df = pd.read_parquet(file_path)

    # Track whether the file was updated
    updated = False

    # Identify columns that contain 'node' in their name
    node_cols = [c for c in df.columns if "node" in c.lower()]
    print(f"Found node columns: {node_cols}")

    for col in node_cols:
        col_str = df[col].astype(str)

        # Check if OLD_NODE exists
        if col_str.isin([OLD_NODE]).any():
            updated = True

            # Duplicate rows where col == OLD_NODE
            df_copy = df[col_str == OLD_NODE].copy()
            df_copy[col] = NEW_NODE

            # Append to original dataframe
            df = pd.concat([df, df_copy], ignore_index=True)

            print(f"Updated column: {col}, added {len(df_copy)} rows")

    # Save the updated parquet
    out_name = os.path.basename(file_path)
    out_path = os.path.join(OUTPUT_DIR, out_name)
    df.to_parquet(out_path, index=False)

    if updated:
        print(f"Saved updated file: {out_path}")
    else:
        print(f"No updates needed. Saved original file: {out_path}")


# ---------------------------------
# PROCESS ALL PARQUET FILES
# ---------------------------------
print("Starting DO-daily parameter updates...")

for fname in os.listdir(INPUT_DIR):
    if fname.endswith(".parquet"):
        update_parquet(os.path.join(INPUT_DIR, fname))

print("All files processed.")

import os
import pandas as pd

INPUT_DIR = "./"                      # where parquet files are
OUTPUT_DIR = "./output_files"         # where updated output goes
OLD_NODE = 1032
NEW_NODE = 1017

# Create output folder if not exists
os.makedirs(OUTPUT_DIR, exist_ok=True)

def update_parquet(file_path):
    print(f"Processing: {file_path}")

    # Load file
    df = pd.read_parquet(file_path)

    # Track whether we found any node columns
    updated = False

    # Find columns that contain node codes
    node_cols = [c for c in df.columns if "node" in c.lower()]

    for col in node_cols:
        # Only process numeric columns
        if df[col].dtype in ["int32", "int64", "float64"]:
            if df[col].isin([OLD_NODE]).any():
                updated = True

                # Copy rows where node == 1032
                df_copy = df[df[col] == OLD_NODE].copy()
                df_copy[col] = NEW_NODE      # replace with 1017

                # Append to original dataframe
                df = pd.concat([df, df_copy], ignore_index=True)

                print(f"  - Updated column: {col}")

    # Save output parquet
    out_name = os.path.basename(file_path)
    out_path = os.path.join(OUTPUT_DIR, out_name)
    df.to_parquet(out_path, index=False)

    print(f"Saved updated file to: {out_path}\n")


# --------------------------
# Process all parquet files
# --------------------------
for fname in os.listdir(INPUT_DIR):
    if fname.endswith(".parquet"):
        update_parquet(os.path.join(INPUT_DIR, fname))

print("✔ All DO-daily parameter files updated successfully.")

import pandas as pd

# ---------------------------
# 1. Load all 3 parquet files
# ---------------------------
df_loc = pd.read_parquet("do_params_dmd_loc.parquet")
df_lane = pd.read_parquet("do_params_lane.parquet")
df_rev = pd.read_parquet("do_params_node_rev.parquet")


# =====================================================
# RULE 1 — do_params_dmd_loc : add 1017 information
# =====================================================
# You mentioned 1017 information is used to fill:
# - fulfill_nodes_priority
# - ss_node_by_seg
# If these columns exist, fill them with default values.

if "fulfill_nodes_priority" in df_loc.columns:
    df_loc["fulfill_nodes_priority"] = df_loc["fulfill_nodes_priority"].fillna("1017")

if "ss_node_by_seg" in df_loc.columns:
    df_loc["ss_node_by_seg"] = df_loc["ss_node_by_seg"].fillna("1017")


# =====================================================
# RULE 2 — do_params_lane : duplicate rows where rw_node_cd == 1032
# =====================================================

df_lane_copy = df_lane[df_lane["rw_node_cd"] == 1032].copy()
df_lane_copy["rw_node_cd"] = 1017        # overwrite with new node
df_lane_final = pd.concat([df_lane, df_lane_copy], ignore_index=True)


# =====================================================
# RULE 3 — do_params_node_rev : same logic as lane
# =====================================================

df_rev_copy = df_rev[df_rev["rev_node_cd"] == 1032].copy()
df_rev_copy["rev_node_cd"] = 1017
df_rev_final = pd.concat([df_rev, df_rev_copy], ignore_index=True)


# =====================================================
# Save output
# =====================================================

df_loc.to_parquet("output_do_params_dmd_loc.parquet", index=False)
df_lane_final.to_parquet("output_do_params_lane.parquet", index=False)
df_rev_final.to_parquet("output_do_params_node_rev.parquet", index=False)

df_loc.to_csv("output_do_params_dmd_loc.csv", index=False)
df_lane_final.to_csv("output_do_params_lane.csv", index=False)
df_rev_final.to_csv("output_do_params_node_rev.csv", index=False)

print("Processing complete.")
